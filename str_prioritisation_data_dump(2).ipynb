{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Notebook for extracting STR dump for STR PRIORITISATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy\n",
    "import pyodbc\n",
    "from urllib.parse import quote\n",
    "import pandas as pd\n",
    "import datetime\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from sqlalchemy.exc import OperationalError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first file containing Batchid and Rptsernum\n",
    "\n",
    "df1 = pd.read_excel('HIGH_VALUE_TRANSACTIONS_17-JUL-2023.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first file containing Batchid and Rptsernum\n",
    "\n",
    "df2 = pd.read_excel('TF-17-JUL-2023.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combining the two dataframes to get all STRs related batchid and rptsernum\n",
    "\n",
    "complete_df = pd.concat([df1,df2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df['brpt'] = \"'\"+complete_df['BATCHID'].astype('str')+'|'+complete_df['RPTSERNUM'].astype('str')+\"'\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df = complete_df.drop_duplicates(subset='brpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "complete_df.to_excel('batchids_str.xlsx', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# starting the sql engine\n",
    "\n",
    "engine=sqlalchemy.create_engine('mssql+pyodbc://SAL_USR:%s@172.16.20.36:1433/HDM_DB?driver=ODBC+Driver+17+for+SQL+Server' % quote('Sal@read1'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to run the query\n",
    "\n",
    "\n",
    "def query_run(query,engine_,path_to_file, file_name,mode = 'a'):\n",
    "    '''\n",
    "    **query**: ('str') Enter the query \n",
    "    **engine_** : ('str') the function creates a connection with postgres server 172.16.22.15:5432/postgres as default,\n",
    "              other engines can be specified here as well\n",
    "    **mode**: ('str') mode is append by default, can be changed to simply write as well\n",
    "    **path_to_file**: ('str') path where the file to be saved\n",
    "    **filename**: ('str') name of the file, defaults get saved as csv\n",
    "    '''\n",
    "    if mode == 'a':\n",
    "        try:\n",
    "            print('--Engine Running--')\n",
    "            engine_ = engine_\n",
    "            conn = engine_.connect()\n",
    "            results = pd.read_sql(query,conn)\n",
    "            conn.close()\n",
    "            engine_.dispose()\n",
    "            print('--Engine Closing--')\n",
    "            print('Results_appending')\n",
    "            file_name = f'{file_name}.csv'\n",
    "            path = os.path.join(path_to_file,file_name)\n",
    "            results.to_csv(path, index = False, mode = mode, header= not os.path.exists(path))\n",
    "        except OperationalError as oe:\n",
    "            time.sleep(3000)\n",
    "            print('Reconnecting with Server')\n",
    "            print('--Engine Running--')\n",
    "            engine_ = engine_\n",
    "            conn = engine_.connect()\n",
    "            results = pd.read_sql(query,conn)\n",
    "            conn.close()\n",
    "            engine_.dispose()\n",
    "            print('--Engine Closing--')\n",
    "            print('Results_appending')\n",
    "            filename = f'{file_name}.csv'\n",
    "            path = os.path.join(path_to_file,file_name)\n",
    "            results.to_csv(path, index = False, mode = mode, header= not os.path.exists(path))\n",
    "    else:\n",
    "        print('--Engine Running--')\n",
    "        engine_ = engine_\n",
    "        conn = engine_.connect()\n",
    "        results = pd.read_sql(query,engine_)\n",
    "        conn.close()\n",
    "        engine_.dispose()\n",
    "        print('--Engine Closing--')\n",
    "        print('Results_appending')\n",
    "        file_name = f'{file_name}.csv'\n",
    "        path = os.path.join(path_to_file,file_name)\n",
    "        results.to_csv(path, index = False, mode = mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunks(lst, n):\n",
    "    \"\"\"Yield successive n-sized chunks from lst.\"\"\"\n",
    "    for i in range(0, len(lst), n):\n",
    "        yield lst[i:i + n]\n",
    "\n",
    "def remove_list_brackets(list_):\n",
    "    str_no_list = str(list_)\n",
    "    str_no_list = str_no_list.replace('[','(').replace(']',')')\n",
    "    return str_no_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                           | 0/39 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Engine Running--\n"
     ]
    }
   ],
   "source": [
    "for chunk in tqdm(list(chunks(complete_df['brpt'].unique().tolist(),2000))):\n",
    "    chunk = remove_list_brackets(chunk)\n",
    "    query = f\"SELECT * FROM [HDM_DB].[FINCORE_BACKUP_CC].[ARFBRC] WHERE CONCAT(batchid,'|',rptsernum) in {chunk}\"\n",
    "    query = query.replace('\"',\"\")\n",
    "    engine_ = engine\n",
    "    path_to_file = \"D:\\Str_prioritisation\\data_dump\"\n",
    "    file_name = \"arfbrc\"\n",
    "    query_run(query,engine_,path_to_file,file_name,mode = 'a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
