{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d88e2433",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from flask import Flask, request, render_template, session, send_from_directory, redirect, url_for, send_file, flash\n",
    "from werkzeug.utils import secure_filename\n",
    "import re\n",
    "import os\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "import openpyxl\n",
    "import numpy as np\n",
    "import nltk\n",
    "from datetime import datetime\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load(\"en_core_web_lg\")\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "from datetime import timedelta\n",
    "import psycopg2\n",
    "from ast import literal_eval\n",
    "import warnings\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.probability import FreqDist\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import sqlalchemy\n",
    "from sqlalchemy import create_engine\n",
    "from tqdm import tqdm\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83102a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app \"__main__\" (lazy loading)\n",
      " * Environment: production\n",
      "\u001b[31m   WARNING: This is a development server. Do not use it in a production deployment.\u001b[0m\n",
      "\u001b[2m   Use a production WSGI server instead.\u001b[0m\n",
      " * Debug mode: on\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " * Running on http://172.20.1.81:5000/ (Press CTRL+C to quit)\n",
      "172.20.1.81 - - [04/Jul/2023 16:58:59] \"GET / HTTP/1.1\" 200 -\n",
      "172.20.1.81 - - [04/Jul/2023 16:58:59] \"GET /static/css/bootstrap.min.css HTTP/1.1\" 404 -\n",
      "172.20.1.81 - - [04/Jul/2023 16:59:20] \"GET /favicon.ico HTTP/1.1\" 404 -\n",
      "172.20.1.81 - - [04/Jul/2023 16:59:44] \"POST /upload HTTP/1.1\" 500 -\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3621, in get_loc\n",
      "    return self._engine.get_loc(casted_key)\n",
      "  File \"pandas\\_libs\\index.pyx\", line 136, in pandas._libs.index.IndexEngine.get_loc\n",
      "    \n",
      "  File \"pandas\\_libs\\index.pyx\", line 163, in pandas._libs.index.IndexEngine.get_loc\n",
      "    \n",
      "  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 5198, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "    \n",
      "  File \"pandas\\_libs\\hashtable_class_helper.pxi\", line 5206, in pandas._libs.hashtable.PyObjectHashTable.get_item\n",
      "    \n",
      "KeyError: 'batchid'\n",
      "\n",
      "The above exception was the direct cause of the following exception:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 2464, in __call__\n",
      "    return self.wsgi_app(environ, start_response)\n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 2450, in wsgi_app\n",
      "    response = self.handle_exception(e)\n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1867, in handle_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 2447, in wsgi_app\n",
      "    response = self.full_dispatch_request()\n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1952, in full_dispatch_request\n",
      "    rv = self.handle_user_exception(e)\n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1821, in handle_user_exception\n",
      "    reraise(exc_type, exc_value, tb)\n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\flask\\_compat.py\", line 39, in reraise\n",
      "    raise value\n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1950, in full_dispatch_request\n",
      "    rv = self.dispatch_request()\n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\flask\\app.py\", line 1936, in dispatch_request\n",
      "    return self.view_functions[rule.endpoint](**req.view_args)\n",
      "  File \"C:\\Users\\SAL005\\AppData\\Local\\Temp\\ipykernel_3436\\3701365975.py\", line 50, in upload\n",
      "    \n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py\", line 3505, in __getitem__\n",
      "    indexer = self.columns.get_loc(key)\n",
      "  File \"C:\\Users\\SAL005\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py\", line 3623, in get_loc\n",
      "    raise KeyError(key) from err\n",
      "KeyError: 'batchid'\n",
      "172.20.1.81 - - [04/Jul/2023 16:59:44] \"GET /upload?__debugger__=yes&cmd=resource&f=debugger.js HTTP/1.1\" 200 -\n",
      "172.20.1.81 - - [04/Jul/2023 16:59:45] \"GET /upload?__debugger__=yes&cmd=resource&f=style.css HTTP/1.1\" 200 -\n",
      "172.20.1.81 - - [04/Jul/2023 16:59:45] \"GET /upload?__debugger__=yes&cmd=resource&f=console.png HTTP/1.1\" 200 -\n",
      "172.20.1.81 - - [04/Jul/2023 16:59:45] \"GET /upload?__debugger__=yes&cmd=resource&f=ubuntu.ttf HTTP/1.1\" 200 -\n",
      "172.20.1.81 - - [04/Jul/2023 16:59:45] \"GET /upload?__debugger__=yes&cmd=resource&f=console.png HTTP/1.1\" 200 -\n",
      "172.20.1.81 - - [04/Jul/2023 17:01:28] \"GET / HTTP/1.1\" 200 -\n",
      "172.20.1.81 - - [04/Jul/2023 17:01:28] \"GET /static/css/bootstrap.min.css HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Postgres database connected succesfully\n",
      "Extracting Report Data from DB\n",
      "Time now:  2023-07-04 17:01:56.501064\n",
      "Executing Query...\n",
      "Time Taken:  relativedelta(seconds=+12, microseconds=+365453)\n",
      "Extracting Entities from GOS+DOI\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 3378/3378 [04:21<00:00, 12.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting PAN\n",
      "Processing Batchid based Concat\n",
      "Time now:  2023-07-04 17:06:36.706901\n",
      "Executing Query...\n",
      "Time Taken:  relativedelta(seconds=+6, microseconds=+65369)\n",
      "Time now:  2023-07-04 17:06:42.800195\n",
      "Executing Query...\n",
      "Time Taken:  relativedelta(seconds=+2, microseconds=+924448)\n",
      "Time now:  2023-07-04 17:06:45.742595\n",
      "Executing Query...\n",
      "Time Taken:  relativedelta(seconds=+2, microseconds=+932388)\n",
      "Time now:  2023-07-04 17:06:48.689974\n",
      "Executing Query...\n",
      "Time Taken:  relativedelta(seconds=+2, microseconds=+96004)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "172.20.1.81 - - [04/Jul/2023 17:07:48] \"POST /upload HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "def rm(path):\n",
    "    os.remove(path)\n",
    "\n",
    "\n",
    "def background_remove(path: object) -> object:\n",
    "    task = Process(target=rm(path))\n",
    "    task.start()\n",
    "\n",
    "\n",
    "def allowed_file(filename):\n",
    "    return '.' in filename and \\\n",
    "           filename.rsplit('.', 1)[1] in ALLOWED_EXTENSIONS\n",
    "\n",
    "\n",
    "def clean(i):\n",
    "    i=re.sub('\\d+',' ',i)\n",
    "    i=re.sub(r'[\\.\\-_\\)@$\\(:+/]',' ',i)\n",
    "    i=re.sub(r'[^A-Za-z]',' ',i)\n",
    "    i=re.sub('\\s+',' ',i)\n",
    "    i=re.sub('\\t',' ',i)\n",
    "    i=re.sub('\\n',' ',i)\n",
    "    return i\n",
    "\n",
    "\n",
    "UPLOAD_FOLDER = \"C:\\\\Users\\\\SAL005\\\\Desktop\\\\TOPIC_MODELLING_HOSTED\\\\uploads\"\n",
    "ALLOWED_EXTENSIONS = 'xlsx'\n",
    "DOWNLOAD_FOLDER = \"C:\\\\Users\\\\SAL005\\\\Desktop\\\\TOPIC_MODELLING_HOSTED\\\\data_folder\"\n",
    "\n",
    "app = Flask(__name__, template_folder='C:\\\\Users\\\\SAL005\\\\Desktop\\\\TOPIC_MODELLING_HOSTED\\\\templates',\n",
    "           static_folder='C:\\\\Users\\\\SAL005\\\\Desktop\\\\TOPIC_MODELLING_HOSTED\\\\static')\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "app.config['DOWNLOAD_FOLDER'] = DOWNLOAD_FOLDER\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template('index_2.html')\n",
    "\n",
    "@app.route('/upload', methods=['GET','POST'])\n",
    "def upload():\n",
    "    if request.method == 'POST':\n",
    "        file = request.files['file']\n",
    "        if file and allowed_file(file.filename):\n",
    "            filename = secure_filename(file.filename)\n",
    "            map_name = str(filename).split('.')[0]\n",
    "            file.save(os.path.join(app.config['UPLOAD_FOLDER'], filename))\n",
    "            df_start = pd.read_excel(os.path.join(UPLOAD_FOLDER, filename))\n",
    "            df_start.columns = [x.lower() for x in df_start.columns]\n",
    "            df_start['brn']=df_start['batchid'].astype(str)+'|'+df_start['rptsernum'].astype(str)\n",
    "            tmp=[\"'\"+i+\"'\" for i in df_start['brn'].to_list()]\n",
    "            brns=\",\".join(tmp)\n",
    "            \n",
    "            try:\n",
    "                pengine = create_engine('postgresql+psycopg2://postgres:postgres@172.16.22.15:5432/postgres')\n",
    "                conn = psycopg2.connect(database = 'postgres', user = 'postgres', password = 'postgres',host = \"172.16.22.15\",port= 5432)\n",
    "                conn.autocommit = True\n",
    "                cursor = conn.cursor()\n",
    "                db_conn=pengine.connect()\n",
    "                print(\"\\n Postgres database connected succesfully\")\n",
    "            except:\n",
    "                print(\"Unable to create the Postgres DB connection\")\n",
    "                \n",
    "            def sql_exec(sql,dengine):\n",
    "                startime=datetime.now()\n",
    "                print(\"Time now: \",startime)\n",
    "                print(\"Executing Query...\")\n",
    "                conn=pengine.connect()\n",
    "                df=pd.read_sql_query(sql,pengine)\n",
    "                endtime=datetime.now()\n",
    "                db_conn.close()\n",
    "                timetaken=relativedelta(endtime,startime)\n",
    "                print('Time Taken: ',timetaken)\n",
    "                return df\n",
    "            \n",
    "            print('Extracting Report Data from DB')\n",
    "            extracted_data=pd.DataFrame()\n",
    "            sql=\"with cte1 as (       (SELECT batchid, b.fiureid, coalesce(r.entityname,b.entityname) as entityname, coalesce(r.recategory, b.recategory) as recategory            FROM risk_priority.batch_res_final b \tleft join public.re_master r on r.fiureid = b.fiureid           WHERE b.reporttype = 'STR'::text \tand b.batchtype<>'D')         UNION           (SELECT batchid, b.fiureid, coalesce(r.entityname,b.entityname) as entityname, coalesce(r.recategory, b.recategory) as recategory            FROM str_prioritization2.batch_22_23 b \tleft join public.re_master r on r.fiureid = b.fiureid           WHERE b.reporttype = 'STR'::text \tand b.batchtype<>'D') ) , cte2 as ( select cte1.baTCHID,rptsernum,mainpersonname,groundsofsusp,detailsofinvestigations,sourceofalert from  risk_priority.report_res_final_april20_march23 r  inner join cte1 on r.batchid=cte1.batchid where r.batchid::text||'|'||r.rptsernum::text in ({}) ) select * from  cte2 where batchid not in ( select originalbatchid from str_prioritization2.batch_22_23 union select originalbatchid from risk_priority.batch_res_final ) and (batchid,rptsernum) not in ( SELECT DISTINCT isolated_str_1804_2212.batchid, isolated_str_1804_2212.reportserialnum AS rptsernum FROM risk_priority.isolated_str_1804_2212)\".format(brns)\n",
    "            results=sql_exec(sql,pengine)\n",
    "            extracted_data=extracted_data.append(results)\n",
    "            extracted_data['GD']=extracted_data['groundsofsusp'].astype(str)+extracted_data['detailsofinvestigations'].astype(str)\n",
    "            gosDF=extracted_data.copy()\n",
    "            df=extracted_data.copy()\n",
    "            df_copy=extracted_data.copy()\n",
    "            df['GD']=df['groundsofsusp'].astype(str)+df['detailsofinvestigations'].astype(str)\n",
    "            df['GD'] = df['GD'].replace(\"\\n\",\"\")\n",
    "            df['GD']=df['GD'].map(lambda x: x.upper())\n",
    "            df['GD']=df['GD'].str.replace('PVT','PRIVATE')\n",
    "            df['GD']=df['GD'].str.replace('LTD','LIMITED')\n",
    "            df['GD']=df['GD'].str.replace('LLP','LIMITED LIABILITY PARTNERSHIP')\n",
    "            df_copy=extracted_data.copy()\n",
    "            df['GD']= df['GD'].apply(clean)\n",
    "            \n",
    "            print('Extracting Entities from GOS+DOI')\n",
    "            org_names=[]\n",
    "            for data in tqdm(df['GD'].values[0:]):\n",
    "                doc1=nlp(data)\n",
    "                entities = [ent for ent in doc1.ents if ent.label_ == \"ORG\"]\n",
    "                org_names.append(entities)\n",
    "            org_names_updated=[]\n",
    "            for k in org_names:\n",
    "                tmp=[]\n",
    "                for l in k:\n",
    "                    if len(str(l).split())>1:\n",
    "                        tmp.append(l)\n",
    "                org_names_updated.append(tmp)\n",
    "            \n",
    "            org_names_final=[]\n",
    "            for p in org_names_updated:\n",
    "                temps=[]\n",
    "                for m in p:\n",
    "                    m=str(m).strip()\n",
    "                    if m not in temps and re.search('PRIVATE|LIMITED|LIMITED LIABILITY PARTNERSHIP',m,flags=re.IGNORECASE) and not (re.search('BANK',m,flags=re.IGNORECASE)):\n",
    "                        temps.append(m)\n",
    "                org_names_final.append(temps)\n",
    "            df['org_names']=org_names_final\n",
    "            l2 = list(map(str, chain.from_iterable(org_names_final)))\n",
    "            \n",
    "            k=Counter(l2)\n",
    "            k1=dict(k)\n",
    "            df2 = pd.DataFrame({'Entities Name' : k1.keys() , 'Count of entities' : k1.values()})\n",
    "            df2=df2.sort_values(by='Entities Name',ascending=True)\n",
    "            df2=df2.reset_index()\n",
    "            df2=df2.drop(['index'],axis=1)\n",
    "            \n",
    "            def clean_pan(i):\n",
    "                i=re.sub(r'[\\.\\-_\\)@$\\(:+/]',' ',i)\n",
    "                i=re.sub('\\s+',' ',i)\n",
    "                i=re.sub('\\t',' ',i)\n",
    "                i=re.sub('\\n',' ',i)\n",
    "                return i\n",
    "            \n",
    "            df_copy['GD']=df_copy['GD'].map(lambda x: x.upper())\n",
    "            df_copy['GD']=df_copy['GD'].replace('.','')\n",
    "            df_copy['GD']= df_copy['GD'].apply(clean_pan)\n",
    "            regex=\"[A-Z]{5}[0-9]{4}[A-Z]{1}\"\n",
    "            p=re.compile(regex)\n",
    "            print('Extracting PAN')\n",
    "            pan_names=[]\n",
    "            for i in (df_copy['GD'].values[0:]):\n",
    "                temps=[]\n",
    "                for q in i.split():\n",
    "                    if (re.search(p,q) and len(q)==10):\n",
    "                        temps.append(q)\n",
    "                pan_names.append(temps)\n",
    "            l1=[]\n",
    "            df_copy['PAN NUMBER']=pan_names\n",
    "            l1 = list(map(str, chain.from_iterable(pan_names)))\n",
    "            l1=list(set(l1))\n",
    "            print('Processing Batchid based Concat')\n",
    "            sql=\"with cte1 as (select distinct batchid,rptsernum, occupation as occupation_names, personname as person_names,pan from risk_priority.arfinp_1804_2212 where batchid::text||'|'||rptsernum::text in ({}) union select distinct batchid,rptsernum, natureofbusiness as occupation_names, personname as person_names, pan from risk_priority.arflpe_1804_2212 where batchid::text||'|'||rptsernum::text in ({}) union select distinct batchid,rptsernum,  occupation as occupation_names,customername as person_names, pan from risk_priority.trftrn_1804_2003 where batchid::text||'|'||rptsernum::text in ({})) select distinct batchid,rptsernum,STRING_AGG(distinct occupation_names,',') as occupation_names,count(distinct occupation_names) as occupation_names_count,STRING_AGG(distinct person_names,',') as person_names,count(distinct person_names) as person_names_count,STRING_AGG(distinct pan,',') as pan,count(distinct pan) as pan_count from cte1 group by batchid,rptsernum\".format(brns,brns,brns)\n",
    "            batchid_based_concat=pd.DataFrame()\n",
    "            results=sql_exec(sql,pengine)\n",
    "            batchid_based_concat=batchid_based_concat.append(results)\n",
    "            \n",
    "            l12=[]\n",
    "            l22=[]\n",
    "            res=[]\n",
    "            res12=[]\n",
    "            l12=batchid_based_concat['pan']\n",
    "            l12=l12.drop_duplicates()\n",
    "            for i in l12:\n",
    "                l22.append(i)\n",
    "            l22=str(l22)\n",
    "            res = literal_eval(f'[{l22}]')\n",
    "            res1=res[0]\n",
    "            res1 = [i for i in res1 if i is not None]\n",
    "            res1 = [i for i in res1 if i is not '']\n",
    "            res_p=[]\n",
    "            for i in res1:\n",
    "                tmp=i.split(',')\n",
    "                for j in tmp:\n",
    "                    res_p.append(j)\n",
    "            res_p=list(set(res_p))\n",
    "            res_p = [i for i in res_p if i is not '']\n",
    "            final=list(set(l1 + res_p))\n",
    "            final=list(set(final))\n",
    "            final1= (\", \".join( repr(e) for e in final ) )\n",
    "            \n",
    "            sql=\"with cte1 as( select distinct pan, occupation as occupation_names, personname as person_names, batchid::text||'|'||rptsernum::text as batch_concat from risk_priority.arfinp_1804_2212 where (pan) in ({}) and batchid::text||'|'||rptsernum::text in ({}) union select distinct pan, natureofbusiness as occupation_names, personname as person_names, batchid::text||'|'||rptsernum::text as batch_concat from risk_priority.arflpe_1804_2212 where (pan) in ({}) and batchid::text||'|'||rptsernum::text in ({}) union select distinct pan, occupation as occupation_names ,customername as person_names,batchid::text||'|'||rptsernum::text as batch_concat from risk_priority.trftrn_1804_2003 where (pan) in ({}) and batchid::text||'|'||rptsernum::text in ({})) select pan,STRING_AGG(distinct batch_concat,',') batch_concat,count(distinct batch_concat) as batch_count,STRING_AGG(distinct occupation_names,',') occupation_concat,count(distinct occupation_names) as occupation_count,STRING_AGG(distinct person_names,',') person_name_concat,count(distinct person_names) as person_count from cte1 group by pan order by batch_count desc\".format(final1,brns,final1,brns,final1,brns)\n",
    "            pan_based_concat=pd.DataFrame()\n",
    "            results=sql_exec(sql,pengine)\n",
    "            pan_based_concat=pan_based_concat.append(results)\n",
    "            \n",
    "            sql=\"with cte1 as ( select distinct batchid,rptsernum, occupation as occupation_names, personname as person_names,pan from risk_priority.inp_5_years where batchid::text||'|'||rptsernum::text in ({}) union select distinct batchid,rptsernum, natureofbusiness as occupation_names, personname as person_names, pan from risk_priority.lpe_5_years where batchid::text||'|'||rptsernum::text in ({}) union select distinct batchid,rptsernum,  occupation as occupation_names,customername as person_names, pan from risk_priority.trftrn_1804_2003 where batchid::text||'|'||rptsernum::text in ({}) ) ,cet2 as( select distinct batchid,rptsernum,STRING_AGG(distinct pan,',') as pan,count(distinct pan) as pan_count,STRING_AGG(distinct occupation_names,',') as occupation_names,count(distinct occupation_names) as occupation_names_count, STRING_AGG(distinct person_names,',') as person_names,count(distinct person_names) as person_names_count from cte1 group by batchid,rptsernum) , cte3 as ( select pan from ( select row_number() over( partition by pan order by pan_count desc) rn,c.* from cet2 c)z where rn>1 and  pan is not null and length (pan)>11 order by pan, rn desc ) select * from ( select row_number() over( partition by pan order by pan_count desc) rn,c.* from cet2 c)zz where pan in (select pan from cte3) order by pan, rn\".format(brns,brns,brns)\n",
    "            pan_based_cluster=pd.DataFrame()\n",
    "            results=sql_exec(sql,pengine)\n",
    "            pan_based_cluster=pan_based_cluster.append(results)\n",
    "            \n",
    "            sql=\"with cte1 as(select upper(trim(occupation)) as occupation_names from risk_priority.inp_5_years where batchid::text||'|'||rptsernum::text in ({}) union select upper(trim(natureofbusiness)) as occupation_names from risk_priority.lpe_5_years where batchid::text||'|'||rptsernum::text in ({}) union select upper(trim(occupation)) as occupation_names from risk_priority.trftrn_1804_2003 where batchid::text||'|'||rptsernum::text in ({})) select (occupation_names) occupation_concat,count(occupation_names) as occupation_count from cte1 GROUP BY occupation_concat ORDER BY occupation_count DESC\".format(brns,brns,brns)\n",
    "            occupation_based_cluster=pd.DataFrame()\n",
    "            results=sql_exec(sql,pengine)\n",
    "            occupation_based_cluster=occupation_based_cluster.append(results)\n",
    "            occupation_based_cluster=occupation_based_cluster.sort_values(by='occupation_count',ascending=False)\n",
    "            occupation_based_cluster=occupation_based_cluster.reset_index()\n",
    "            occupation_based_cluster=occupation_based_cluster.drop(['index'],axis=1)\n",
    "            \n",
    "            gosDF['GD'] = gosDF['GD'].apply(clean)\n",
    "            gosDF['GD']=gosDF['GD'].str.lower()\n",
    "            stop_words=stopwords.words('english')\n",
    "            stop_words.extend(['customer','account','detail','details','since','available','till','utilized','towards','inward','outward','number','name','hold','several','along','apart','others','other','also','refer','total','e','mail','id','seem','seems','wherein','found','wrt','given','belongs','belong','r','lakh','transaction','bank','credit','transfer','c','debit','fund','instance','high','profile','ltd','technology','imp','branch','declared','majorly','ifsc','mobile','aggregating','side','current','pan','date','income','received','mainly','nil','per','value','limited','pvt','balance','annual','saving','observed','year','risk','turnover','address','currency','private','money','hdfc','internet','individual','india','str','fy','status','business','payment','approx','due','opening','account','amount','filed','various','suspicion','company','icici','pattern','com','period','active','financial','debited','mr','followed','kumar','multiple','neblio','line','credited','source','amounting','birth','crore','quantum','low','party','maintaining','linked','registered','related','yes','etc','entity','revealed','basis','review','customer','b','road','record','card','amounted','opened','day','ground','aforesaid','diligence','code','noted','alert','platform','time','gmail','via','http','dealing','transferred','deposit','volume','maintains','categorized','lac','carried','scrutiny','previous','appears','self','raised','p','service','nagar','end','ac','seen','singh','employed','part','filing','search','immediate','medium','yesb','made','like','atm','ybl','show','salaried','use','clearing','unusual','k','report','www','bearing','new','obtained','public','triggered','contact','f','month','updated','categorization','used','updation','axis','major','view','beneficiary','model','domain','example','purpose','got','solution','however','based','near','icic','internal','information','includes','mmid','informed','earlier','last','statement','online','furnished','activity','aadhar','enhanced','reviewing','holder','following','case','involved','newly','similarly','g','n','done','sbi','range','relationship','reported','sheet','owner','could','state','innovation','way','reason','huge','strs','rule','cr','subsequent','provides','official','beneficial','suspicious','hence','aml','h','deployed','awlencan','copy','held','floor','category','sbin','carry','excess','noticing','authorized','appear','possibility','totalling','kotak','enquiry','indeterminable','mahindra','u','recorded','uttar','categorised','ascertained','notice','overall','first','raise','regular','mentioned','sudden','currently','intention','sale','investment','nature','gross','yet','rest','small','reveals','periodic','idfc','similar','closed','three','respectively','remaining','person','upto','sector','matching','within','make','j','adverse','l','edd','trader','signatory','location','freeze','maintained','possibly','identification','light','located','director','additional','availed','favouring','idfb','banking','one','client','station','wise','passport','facility','said','web','idbi','jain','gstin','type','span','non','cust','receiving','co','merchant','utib','age','dist','form','vide','mailing','design','sharma','submitted','serf','colony','develops','old','authority','dob','tech','city','mob','plot','mar','full','add','tune','point','utilization','compliance','stated','provide','june','determined','unrelated','main','rajesh','market','ucic','v','different','communication','fixed','iec','reactive','short','aggregated','kkbk','shop','deal','home','narration','finance','gupta','data','complex','indicates','indian','confirmed','salary','third','th','mode','noticed','frequent','identified','history','therefore','house','summary','firm','inr','feedback','signing','law','opp','dated','inconsistent','apr','withdrawn','ibkl','m','paid','large','block','kolkata','technolo','jun','recommended','identical','operation','totaling','mohammed','user','q','acc','possible','le','po','far','bandhan','commensurate','open','oct','batch'])\n",
    "            new_stop_words=['win', 'phone', 'royal', 'corresponds', 'chance', 'using', 'caller', 'average', 'external', 'forward', 'july', 'unique', 'red', 'flag', 'application', 'luck', 'correspond', 'distinct', 'play', 'treating', 'truecaller', 'primary', 'exchange', 'sent', 'link', 'facilitating', 'sending', 'daily', 'ernakulam', 'would', 'accepting', 'youtube', 'unable', 'pay', 'facilitates', 'app', 'reporting', 'themself', 'sampling', 'prediction', 'help', 'another', 'sport', 'digital', 'across', 'compared', 'google', 'transacting', 'kerala', 'query', 'accept', 'result', 'country', 'reflects', 'clear', 'try', 'pradesh', 'trying', 'relate', 'fiu', 'highly', 'evidence', 'evident', 'unlikely', 'going', 'facilitaitng', 'mumbai', 'kyc', 'teen', 'kemps', 'yaar', 'sender', 'follows', 'priya', 'matka', 'sakunthala', 'ranveer', 'cash', 'added', 'much', 'indicating', 'match', 'justify', 'looking', 'pmcares', 'look', 'almost', 'gone', 'ref', 'march', 'ram', 'prakash', 'deepak', 'winning', 'group', 'net', 'find', 'br', 'created', 'goyal', 'clearly', 'facebook', 'kind', 'shivam', 'raman', 'doesnt', 'satta', 'kingh', 'apkpure', 'romibhai', 'sonubhaiapp', 'rsgames', 'august', 'able', 'western', 'watch', 'virendra', 'bjwe', 'kmpa', 'paytm', 'delhi', 'maharashtra', 'karnataka', 'st', 'enterprise', 'telangana', 'bihar', 'indicate', 'sample', 'haryana', 'majority', 'return', 'set', 'spent', 'vpa', 'sampled', 'aug', 'madhya', 'ganesh', 'pe', 'kadalundipuzha', 'export', 'rakesh', 'rajan', 'cricket', 'golden', 'six', 'baba', 'seven', 'bazaar', 'five', 'vijayawada', 'malappuram', 'david', 'durgapur', 'himachal', 'sharad', 'dindigul', 'bos', 'thiruvananthapuram', 'kochi', 'test', 'milan', 'balveer', 'kasera', 'maza', 'palem', 'cheeni', 'paadam', 'manimooly', 'chelakkad', 'veliancode', 'nenmara', 'thuvvur', 'hoshiarpur', 'ritu', 'matlka', 'milanbazar', 'kheteshwar', 'con', 'koomp', 'oppo', 'rextan', 'managalore', 'singareddy', 'dhilip', 'dheeraj', 'lotusonlineid', 'upi', 'star', 'sk', 'andhra', 'bengaluru', 'externally', 'lotus', 'dheeru', 'searching', 'lotusgaminglive', 'goa', 'secunderabad', 'anantapur', 'sikkim', 'transact', 'flutter', 'chodavaram', 'neft', 'policy', 'skstar', 'privacy', 'billdesk', 'euronet', 'moneymultiple', 'rtgs', 'document', 'tranche', 'standard', 'chartered', 'completed', 'cashfree', 'bangalore', 'chennai', 'limit', 'rummy', 'remitters', 'analysis', 'aadhaar', 'email', 'withdrawal', 'tamil', 'nadu', 'repayment', 'occasion', 'junglee', 'may', 'conducted', 'locate', 'assigned', 'april', 'portal', 'pune', 'west', 'issued', 'site', 'rationale', 'ici', 'spends', 'routed', 'economic', 'landline', 'monitoring', 'loan', 'hyderabad', 'xxxxxxxx', 'fetched', 'razorpay', 'investigation', 'office', 'rd', 'gujarat', 'remittance', 'rajasthan', 'street', 'pytm', 'xbet', 'considering', 'gateway', 'round', 'tower', 'dream', 'wallet', 'masked', 'software', 'student', 'reddy', 'personal', 'around', 'count', 'combined', 'mmt', 'cross', 'appx', 'instrument', 'conclusion', 'nd', 'top', 'x', 'noida', 'pmla', 'outstanding', 'authorised', 'flat', 'park', 'incorporation', 'east', 'purchase', 'gurgaon', 'known', 'percent', 'indusind', 'jan', 'highest', 'mcc', 'constituting', 'trading', 'common', 'lowest', 'upon', 'resident', 'national', 'post', 'building', 'cumulative', 'activation', 'sorted', 'payu', 'xxxxx', 'came', 'driving', 'bengal', 'punjab', 'ludo', 'provided', 'yesbank', 'indicated', 'krishna', 'sai', 'occupation', 'consists', 'interconnected', 'general', 'inactive', 'website', 'dec', 'single', 'usage', 'permanent', 'summation', 'thane', 'license', 'prasad', 'indb', 'request', 'school', 'two', 'monthly', 'feb', 'xxxxxx', 'oksbi', 'consolidated', 'arcade', 'relation', 'transctions', 'deposited', 'cheque', 'transacted', 'yadav', 'culture', 'jehanabad', 'ip', 'mostly', 'working', 'store', 'complied', 'disbursed', 'jaipur', 'phase', 'okicici', 'namely', 'rto', 'note', 'playing', 'video', 'recurring', 'network', 'constitution', 'ahmedabad', 'international', 'marg', 'database', 'establishment', 'suresh', 'increase', 'door', 'federal', 'retail', 'ctr', 'amit', 'comprising', 'okhdfcbank', 'pasfar', 'ptc', 'next', 'raj', 'consist', 'week', 'patel', 'wunderbaked', 'fort', 'situated', 'january', 'authentication', 'uidai', 'system', 'layout', 'avenue', 'dreamplug', 'message', 'ranging', 'unit', 'consultancy', 'proprietor', 'supreme', 'pin', 'process', 'vijay', 'file', 'ntr', 'cbwt', 'cred', 'mall', 'reviewed', 'remaning', 'ascertain', 'billing', 'area', 'boarded', 'tranasction', 'poker', 'unregistered', 'quadrific', 'inception', 'canara', 'behind', 'male', 'chakma', 'rzpx', 'okaxis', 'south', 'north', 'infotech', 'provider', 'citibank', 'actual', 'velocity', 'village', 'justified', 'vihar', 'global', 'list', 'investigating', 'device', 'virtual', 'fiduciary', 'gambling', 'gst', 'voter', 'circle', 'baroda', 'four', 'overseas', 'favoring', 'police', 'accordingly', 'spend', 'ka', 'reimbursement', 'nov', 'engaged', 'garden', 'devi', 'numerous', 'anna', 'weekly', 'rating', 'pool', 'domestic', 'sri', 'sunil', 'khan', 'incoming', 'xxxx', 'additionally', 'suspect', 'highlighted', 'check', 'meena', 'bill', 'housewife', 'fall', 'free', 'contactable']\n",
    "            stop_words.extend(new_stop_words)\n",
    "            def remove_stop_words(i):\n",
    "                tokens=word_tokenize(i)\n",
    "                return \" \".join([token for token in tokens if token not in stop_words])\n",
    "            gosDF['GD']=gosDF['GD'].apply(remove_stop_words)\n",
    "            lematizer=WordNetLemmatizer()\n",
    "            def lematization(i):\n",
    "                tokens=word_tokenize(i)\n",
    "                return \" \".join([lematizer.lemmatize(token) for token in tokens if token not in stop_words])\n",
    "            gosDF['GD']=gosDF['GD'].apply(lematization)\n",
    "            gosDF['GD']=gosDF['GD'].apply(remove_stop_words)\n",
    "            def tokens_from_corpus(i):\n",
    "                return word_tokenize(i)\n",
    "            list_gos=gosDF['GD'].to_list()\n",
    "            final_tokens=[]\n",
    "            for i in list_gos:\n",
    "                final_tokens.extend(tokens_from_corpus(i))\n",
    "            final_tokens=['{0}'.format(elem) for elem in final_tokens]\n",
    "            fdist=FreqDist(final_tokens)\n",
    "            bgs=nltk.bigrams(final_tokens)\n",
    "            fdist_bg=FreqDist(bgs)\n",
    "            freq_words=[i[0] for i in fdist.most_common(1000)]\n",
    "            freq_bg_words=[i[0] for i in fdist_bg.most_common(1000)]\n",
    "            freq_bg=[i[1] for i in fdist_bg.most_common(1000)]\n",
    "            gosDF['GD']=gosDF['GD'].apply(clean)\n",
    "            gosDF['GD']=gosDF['GD'].map(lambda x: str(x).lower())\n",
    "            unigram=pd.DataFrame()\n",
    "            j=[]\n",
    "            k=[]\n",
    "            for i in freq_words:\n",
    "                count=0\n",
    "                count=len(gosDF[gosDF['GD'].str.contains(' '+i+' ')])\n",
    "                j.append(i)\n",
    "                k.append(count)\n",
    "            unigram['entities']=j\n",
    "            unigram['count']=k\n",
    "            unigram=unigram.sort_values(by='count',ascending=False)\n",
    "            unigram=unigram.reset_index()\n",
    "            unigram=unigram.drop(['index'],axis=1)\n",
    "            bigram=pd.DataFrame()\n",
    "            j=[]\n",
    "            k=[]\n",
    "            l=[]\n",
    "            for x,y in list(freq_bg_words):\n",
    "                count=0\n",
    "                count=len(gosDF[gosDF['GD'].str.contains(x+' '+y)])\n",
    "                j.append(x)\n",
    "                l.append(y)\n",
    "                k.append(count)\n",
    "            bigram['word_1']=j\n",
    "            bigram['word_2']=l\n",
    "            bigram['count']=k\n",
    "            bigram=bigram.sort_values(by='count',ascending=False)\n",
    "            bigram=bigram.reset_index()\n",
    "            bigram=bigram.drop(['index'],axis=1)\n",
    "            df_negative_words = pd.DataFrame(stop_words, columns=['Negative Words for Word Cloud'])\n",
    "            \n",
    "            \n",
    "            \n",
    "            path=\"C:\\\\Users\\\\SAL005\\\\Desktop\\\\TOPIC_MODELLING_HOSTED\\\\data_folder\\\\{}.xlsx\".format(filename.split('.')[0])\n",
    "            \n",
    "            writer=pd.ExcelWriter(path,engine='xlsxwriter')\n",
    "            df_start.to_excel(writer,sheet_name='Batchid Rptsernum provided',index=False)\n",
    "            extracted_data.to_excel(writer,sheet_name='Extracted Report Data',index=False)\n",
    "            df.to_excel(writer,sheet_name='Extracted Entities',index=False)\n",
    "            df2.to_excel(writer,sheet_name='Extracted Entities Count',index=False)\n",
    "            df_copy.to_excel(writer,sheet_name='Extracted PAN',index=False)\n",
    "            batchid_based_concat.to_excel(writer,sheet_name='BATCHID based concat',index=False)\n",
    "            pan_based_concat.to_excel(writer,sheet_name='PAN based concat',index=False)\n",
    "            pan_based_cluster.to_excel(writer,sheet_name='PAN Cluster',index=False)\n",
    "            occupation_based_cluster.to_excel(writer,sheet_name='Top Occupation Count',index=False)\n",
    "            df_negative_words.to_excel(writer,sheet_name='Negative Words',index=False)\n",
    "            unigram.to_excel(writer,sheet_name='Word Cloud Unigram',index=False)\n",
    "            bigram.to_excel(writer,sheet_name='Word Cloud Bigram',index=False)\n",
    "            writer.close()\n",
    "            \n",
    "            \n",
    "            return send_file(path,attachment_filename=f\"{filename.split('.')[0]}.xlsx\", mimetype='xlsx', as_attachment=True)\n",
    "            \n",
    "            \n",
    "    return render_template('index_2.html')\n",
    "            \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True, host='172.20.1.81', use_reloader = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caae3714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
