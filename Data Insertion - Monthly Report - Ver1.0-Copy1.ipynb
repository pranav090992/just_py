{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sqlalchemy\n",
    "import os\n",
    "import glob\n",
    "import xlrd\n",
    "import xlsxwriter\n",
    "import openpyxl\n",
    "import psycopg2\n",
    "import urllib\n",
    "import openpyxl\n",
    "import csv\n",
    "\n",
    "from datetime import datetime\n",
    "from openpyxl.styles import Font, Color, Alignment, Border, Side, NamedStyle, PatternFill\n",
    "from openpyxl.worksheet.dimensions import ColumnDimension\n",
    "from datetime import datetime\n",
    "from openpyxl import  load_workbook\n",
    "from openpyxl.utils.cell import get_column_letter\n",
    "from urllib.parse import quote\n",
    "from psycopg2.extensions import AsIs\n",
    "from sqlalchemy import create_engine\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from openpyxl.styles import Font, Color, Alignment, Border, Side, NamedStyle, PatternFill\n",
    "from openpyxl.worksheet.dimensions import ColumnDimension\n",
    "from datetime import datetime\n",
    "from openpyxl import  load_workbook\n",
    "from openpyxl.utils.cell import get_column_letter\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution started at : 2023-06-16 14:37:00.110138\n"
     ]
    }
   ],
   "source": [
    "initial = datetime.now()\n",
    "start_time = datetime.now()\n",
    "print('Execution started at :',start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option(\"display.max_columns\",None)\n",
    "pd.set_option(\"display.max_rows\",None)\n",
    "pd.set_option(\"display.max_colwidth\",None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'C:\\Users\\SAL008\\Desktop\\Ravi\\Monthly Report'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Supporting documents "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **RE MASTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fiureid</th>\n",
       "      <th>entityname</th>\n",
       "      <th>recategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BADCB00001</td>\n",
       "      <td>AGRA ZILLA SAHAKARI BANK LIMITED</td>\n",
       "      <td>BADCB</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BADCB00002</td>\n",
       "      <td>THE AHMEDABAD DISTRICT  CO-OPERATIVE BANK LIMITED</td>\n",
       "      <td>BADCB</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      fiureid                                         entityname recategory\n",
       "0  BADCB00001                   AGRA ZILLA SAHAKARI BANK LIMITED      BADCB\n",
       "1  BADCB00002  THE AHMEDABAD DISTRICT  CO-OPERATIVE BANK LIMITED      BADCB"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_master = pd.read_csv(path + '\\\\Supporting_docs\\\\re_master.csv')\n",
    "re_master.columns = re_master.columns.str.lower()\n",
    "re_master.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Map    <i>re_category_description</i>   using    <i>re_category_master</i>**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **RE CATEGORY MASTER**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recategory</th>\n",
       "      <th>categorydesc</th>\n",
       "      <th>reprefix</th>\n",
       "      <th>creationdate</th>\n",
       "      <th>createdby</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>INCRE</td>\n",
       "      <td>Credit Rating Agencies</td>\n",
       "      <td>INCRE</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>FINGATE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>INVCD</td>\n",
       "      <td>Domestic Venture Capital Funds</td>\n",
       "      <td>INVCD</td>\n",
       "      <td>2011-02-19</td>\n",
       "      <td>FINGATE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  recategory  \\\n",
       "0      INCRE   \n",
       "1      INVCD   \n",
       "\n",
       "                                                                                           categorydesc  \\\n",
       "0  Credit Rating Agencies                                                                                 \n",
       "1  Domestic Venture Capital Funds                                                                         \n",
       "\n",
       "  reprefix creationdate createdby  \n",
       "0    INCRE   2011-02-19   FINGATE  \n",
       "1    INVCD   2011-02-19   FINGATE  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re_category_master = pd.read_csv(path + '\\\\Supporting_docs\\\\recategorymaster.csv')\n",
    "re_category_master.columns = re_category_master.columns.str.lower()\n",
    "re_category_master.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Function to Map RE_CATEGORY DESCRIPTION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_category_desc(i):\n",
    "    if not re_category_master.loc[re_category_master['recategory']==i]['categorydesc'].empty:\n",
    "        return re_category_master.loc[re_category_master['recategory']==i]['categorydesc'].unique()[0]\n",
    "    else:\n",
    "        return i.split(maxsplit = 1)[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function for CamelCase\n",
    "from re import sub\n",
    "\n",
    "def Camel_Case(st):\n",
    "    st = st[:-(len(st)-1)].upper()+st[1:len(st)].lower()\n",
    "    return st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re_master['category_description'] = re_master['recategory'].apply(return_category_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#re_list = pd.read_csv(f'C:\\Users\\SAL008\\Desktop\\Ravi\\Monthly Report\\RE_26_OCT_2022.csv',encoding = 'latin1')\n",
    "#REs_july.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Update the Analysis Month - EVERY MONTH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis month - sep22\n",
      "Analysis month in Camel Case - Sep22\n",
      "Analysis month in reporting format - Sep 2022\n"
     ]
    }
   ],
   "source": [
    "month = 'sep22'\n",
    "print('Analysis month -',month)\n",
    "month_cc = Camel_Case(month)\n",
    "print('Analysis month in Camel Case -',month_cc)\n",
    "#month_report_format = format(month_cc[0:3] +' '+str(datetime.now().year))\n",
    "month_report_format = format(month_cc[0:3] +' '+'20'+month[3:5])\n",
    "print('Analysis month in reporting format -',month_report_format)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **NOTE**: Create a folder with the above mentioned 'Analysis month in reporting format' i.e for sep22 it must be Sep 2022. Within this folder create a Data folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EVERY MONTH CHANGES - Assigning the timeframe by setting the minimum and maximum batchid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start BATCHID: 2209010000\n",
      "End BATCHID: 2209309999\n"
     ]
    }
   ],
   "source": [
    "min_batchid = 2209010000\n",
    "print('Start BATCHID:',min_batchid)\n",
    "\n",
    "max_batchid = 2209309999\n",
    "print('End BATCHID:',max_batchid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create Postgres Engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Postgres database connected succesfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pengine = create_engine('postgresql+psycopg2://postgres:postgres@172.16.22.15:5432/postgres')\n",
    "\n",
    "    conn = psycopg2.connect(database = 'postgres', user = 'postgres', password = 'postgres',host = \"172.16.22.15\",port= 5432)\n",
    "    conn.autocommit = True\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "except:\n",
    "    print(\"Unable to create the Postgres DB connection\")\n",
    "\n",
    "db_conn=pengine.connect()\n",
    "print(\"\\n Postgres database connected succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df=pd.read_excel(r'C:\\Users\\SAL005\\Desktop\\nto J&k punjab str\\output_excluding_isolated.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "329"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.to_sql(f'npo_data',pengine,schema='gos_cluster',if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ctr.to_sql(f'{month}_batch_{reporttype}',pengine,schema='monthly_ctr',if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **CTR - Schema Creation if doesn't exists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report type is: ctr\n"
     ]
    }
   ],
   "source": [
    "reporttype = 'ctr'\n",
    "print('Report type is:',reporttype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_ctr_query = \"CREATE SCHEMA IF NOT EXISTS %s AUTHORIZATION %s;\"\n",
    "monthly_ctr = (AsIs(f'monthly_{reporttype}'),AsIs('postgres'))\n",
    "cursor.execute(monthly_ctr_query,monthly_ctr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **file_path is the path from where we have to insert the data into the postgresDB which we have received from RAKESH**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SAL008\\\\Desktop\\\\Ravi\\\\Monthly Report\\\\Oct 2022\\\\Data\\\\SAL_CTR_OCT2022'"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file_path = r'C:\\Users\\SAL008\\Desktop\\Ravi\\Monthly Report\\Sep 2022\\Data\\SAL_CTR_SEP2022'\n",
    "file_path = path +'\\\\'+month_report_format+ f'\\Data\\SAL_{reporttype.upper()}_{month[:3].upper()}2022'\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create the folder e.g **SAL_CTR_NOV2022** (make sure to keep the same format) inside the above path, then Create another folder name **Data**, similarly create **Output** and **Submission** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataread Start time\n",
    "data_read_start_time= datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for BATCH file: 2210033444\n",
      "Maximum batch id BATCH file: 2210297591\n",
      "\n",
      " Batch file inserted into oct22_batch_ctr with 3298 rows\n"
     ]
    }
   ],
   "source": [
    "batch = pd.read_csv(file_path + '\\BATCH.csv')\n",
    "batch_ctr = batch[(batch['BATCHID'] >=min_batchid) & (batch['BATCHID'] <= max_batchid)]\n",
    "print('Minimum batch id for BATCH file:',batch_ctr['BATCHID'].min())\n",
    "print('Maximum batch id BATCH file:',batch_ctr['BATCHID'].max())\n",
    "batch_ctr.shape\n",
    "#Lower the Columns\n",
    "batch_ctr.columns = [col.lower() for col in batch_ctr.columns]\n",
    "\n",
    "#Insert data to postgres\n",
    "batch_ctr.to_sql(f'{month}_batch_{reporttype}',pengine,schema='monthly_ctr',if_exists='replace',index=False)\n",
    "print(f\"\\n Batch file inserted into {month}_batch_{reporttype} with {batch_ctr.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **report**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\Users\\\\SAL008\\\\Desktop\\\\Ravi\\\\Monthly Report\\\\Oct 2022\\\\Data\\\\SAL_CTR_OCT2022\\\\ARFREPORT.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-7d844c6700b9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\ARFREPORT.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mreport_ctr_arf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreport\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCHID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m\u001b[0mmin_batchid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCHID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mmax_batchid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Minimum batch id in REPORT file:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreport_ctr_arf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCHID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Maximum batch id in REPORT file:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreport_ctr_arf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCHID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mreport_ctr_arf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\Users\\\\SAL008\\\\Desktop\\\\Ravi\\\\Monthly Report\\\\Oct 2022\\\\Data\\\\SAL_CTR_OCT2022\\\\ARFREPORT.csv'"
     ]
    }
   ],
   "source": [
    "report = pd.read_csv(file_path + '\\ARFREPORT.csv')\n",
    "report_ctr_arf = report[(report['BATCHID'] >=min_batchid) & (report['BATCHID'] <= max_batchid)]\n",
    "print('Minimum batch id in REPORT file:',report_ctr_arf['BATCHID'].min())\n",
    "print('Maximum batch id in REPORT file:',report_ctr_arf['BATCHID'].max())\n",
    "report_ctr_arf.shape\n",
    "\n",
    "#Lower the Columns\n",
    "report_ctr_arf.columns = [col.lower() for col in report_ctr_arf.columns]\n",
    "\n",
    "#Insert data to postgres\n",
    "report_ctr_arf.to_sql(f'{month}_reportarf_{reporttype}',pengine,schema='monthly_ctr',if_exists='replace',index=False)\n",
    "print(f\"\\n arf report file inserted into {month}_report_{reporttype} with {batch_ctr.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id in REPORT file: 2211029098\n",
      "Maximum batch id in REPORT file: 2211306982\n",
      "\n",
      " Batch file inserted into nov22_report_ctr with 3726 rows\n"
     ]
    }
   ],
   "source": [
    "report = pd.read_csv(file_path + '\\TRFREPORT.csv')\n",
    "report_ctr_trf = report[(report['BATCHID'] >=min_batchid) & (report['BATCHID'] <= max_batchid)]\n",
    "print('Minimum batch id in REPORT file:',report_ctr_trf['BATCHID'].min())\n",
    "print('Maximum batch id in REPORT file:',report_ctr_trf['BATCHID'].max())\n",
    "report_ctr_trf.shape\n",
    "\n",
    "#Lower the Columns\n",
    "report_ctr_trf.columns = [col.lower() for col in report_ctr_trf.columns]\n",
    "\n",
    "#Insert data to postgres\n",
    "report_ctr_trf.to_sql(f'{month}_reporttrf_{reporttype}',pengine,schema='monthly_ctr',if_exists='replace',index=False)\n",
    "print(f\"\\n trf report file inserted into {month}_report_{reporttype} with {batch_ctr.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create combined table of arfreport and trfreport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f'CREATE TABLE IF NOT EXISTS monthly_{reporttype}.{month}_report_{reporttype}\\\n",
    "                    ( rptsernum bigint,     originalrptsernum bigint,     mainpersonname text COLLATE pg_catalog.\"default\",     sourceofalert text COLLATE pg_catalog.\"default\",     suspduetoproceedsofcrime text COLLATE pg_catalog.\"default\",     suspduetocomplextrans text COLLATE pg_catalog.\"default\",     suspduetonoecorationale text COLLATE pg_catalog.\"default\",     suspoffinancingofterrorism text COLLATE pg_catalog.\"default\",     attemptedtransaction text COLLATE pg_catalog.\"default\",     groundsofsusp double precision,     detailsofinvestigations double precision,     leainformed text COLLATE pg_catalog.\"default\",     leadetails text COLLATE pg_catalog.\"default\",     priorityrating text COLLATE pg_catalog.\"default\",     reportcoverage text COLLATE pg_catalog.\"default\",     additionaldocuments text COLLATE pg_catalog.\"default\",     batchid bigint,     deletedflag text COLLATE pg_catalog.\"default\",     creationdate text COLLATE pg_catalog.\"default\",     createdby text COLLATE pg_catalog.\"default\",     xmlindexid bigint,     batchxmlindexid bigint )  \\\n",
    "                    TABLESPACE pg_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefinedTable",
     "evalue": "relation \"monthly_ctr.nov22_report_ctr_trf_ctr\" does not exist\nLINE 1: ... accountnum, personname, pan                 from monthly_ct...\n                                                             ^\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUndefinedTable\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-754ef6b5b706>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m cursor.execute(f'insert into monthly_{reporttype}.{month}_report_{reporttype}\\\n\u001b[0m\u001b[0;32m      2\u001b[0m                 \u001b[0mselect\u001b[0m \u001b[0mdistinct\u001b[0m \u001b[0mbatchid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrptsernum\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0maccountnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpersonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpan\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m                 \u001b[1;32mfrom\u001b[0m \u001b[0mmonthly_\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mreporttype\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mmonth\u001b[0m\u001b[1;33m}\u001b[0m\u001b[0m_report_ctr_trf_\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mreporttype\u001b[0m\u001b[1;33m}\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m                 \u001b[0munion\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                 \u001b[0mselect\u001b[0m \u001b[0mdistinct\u001b[0m \u001b[0mbatchid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrptsernum\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0maccountnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpersonname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpan\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUndefinedTable\u001b[0m: relation \"monthly_ctr.nov22_report_ctr_trf_ctr\" does not exist\nLINE 1: ... accountnum, personname, pan                 from monthly_ct...\n                                                             ^\n"
     ]
    }
   ],
   "source": [
    "cursor.execute(f'insert into monthly_{reporttype}.{month}_report_{reporttype}\\\n",
    "                select distinct batchid, rptsernum , accountnum, personname, pan \\\n",
    "                from {month}_reportarf_{reporttype} \\\n",
    "                union \\\n",
    "                select distinct batchid, rptsernum , accountnum, personname, pan \\\n",
    "                from {month}_reporttrf_{reporttype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **arfbrc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for ARFBRC file: 2210033444\n",
      "Maximum batch id for ARFBRC file: 2210297591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1500668, 23)"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arfbrc = pd.read_csv(file_path + '\\ARFBRC.csv')\n",
    "arfbrc_ctr = arfbrc[(arfbrc['BATCHID'] >=min_batchid) & (arfbrc['BATCHID'] <= max_batchid)]\n",
    "print('Minimum batch id for ARFBRC file:',arfbrc_ctr['BATCHID'].min())\n",
    "print('Maximum batch id for ARFBRC file:',arfbrc_ctr['BATCHID'].max())\n",
    "arfbrc_ctr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **arfacc**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for ARFACC file: 2210033444\n",
      "Maximum batch id for ARFACC file: 2210297591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1500399, 21)"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arfacc = pd.read_csv(file_path + '\\ARFACC.csv')\n",
    "arfacc_ctr = arfacc[(arfacc['BATCHID'] >=min_batchid) & (arfacc['BATCHID'] <= max_batchid)]\n",
    "print('Minimum batch id for ARFACC file:',arfacc_ctr['BATCHID'].min())\n",
    "print('Maximum batch id for ARFACC file:',arfacc_ctr['BATCHID'].max())\n",
    "arfacc_ctr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **arfinp**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for ARFINP file: 2210033444\n",
      "Maximum batch id for ARFINP file: 2210297591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1847005, 40)"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arfinp = pd.read_csv(file_path + '\\ARFINP.csv')\n",
    "arfinp_ctr = arfinp[(arfinp['BATCHID'] >=min_batchid) & (arfinp['BATCHID'] <= max_batchid)]\n",
    "print('Minimum batch id for ARFINP file:',arfinp_ctr['BATCHID'].min())\n",
    "print('Maximum batch id for ARFINP file:',arfinp_ctr['BATCHID'].max())\n",
    "arfinp_ctr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **arflpe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for ARFLPE file: 2210033444\n",
      "Maximum batch id for ARFLPE file: 2210297591\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1148814, 36)"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arflpe = pd.read_csv(file_path + '\\ARFLPE.csv')\n",
    "arflpe_ctr = arflpe[(arflpe['BATCHID'] >=min_batchid) & (arflpe['BATCHID'] <= max_batchid)]\n",
    "print('Minimum batch id for ARFLPE file:',arflpe_ctr['BATCHID'].min())\n",
    "print('Maximum batch id for ARFLPE file:',arflpe_ctr['BATCHID'].max())\n",
    "arflpe_ctr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRF BATCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for ctr trfbatch file: 2210034048\n",
      "Maximum batch id for ctr trfbatch file: 2210277404\n",
      "trfbatch file for ctr has 133 rows and 33 columns\n"
     ]
    }
   ],
   "source": [
    "trfbatch = pd.read_csv(file_path + '\\TRF_BATCH.csv')\n",
    "trfbatch_ctr = trfbatch[(trfbatch['BATCHID'] >=min_batchid) & (trfbatch['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} trfbatch file:',trfbatch_ctr['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} trfbatch file:',trfbatch_ctr['BATCHID'].max())\n",
    "print(f'trfbatch file for {reporttype} has {trfbatch_ctr.shape[0]} rows and {trfbatch_ctr.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRF REPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum report id for ctr trfreport file: 2210034048\n",
      "Maximum report id for ctr trfreport file: 2210277404\n",
      "trfreport file for ctr has 2519 rows and 22 columns\n"
     ]
    }
   ],
   "source": [
    "trfreport = pd.read_csv(file_path + '\\TRF_REPORT.csv')\n",
    "trfreport_ctr = trfreport[(trfreport['BATCHID'] >=min_batchid) & (trfreport['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum report id for {reporttype} trfreport file:',trfreport_ctr['BATCHID'].min())\n",
    "print(f'Maximum report id for {reporttype} trfreport file:',trfreport_ctr['BATCHID'].max())\n",
    "print(f'trfreport file for {reporttype} has {trfreport_ctr.shape[0]} rows and {trfreport_ctr.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRFBRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum report id for ctr trfreport file: 2210034048\n",
      "Maximum report id for ctr trfreport file: 2210277404\n",
      "trfreport file for ctr has 3576 rows and 23 columns\n"
     ]
    }
   ],
   "source": [
    "trfbrc = pd.read_csv(file_path + '\\TRF_TRFBRC.csv')\n",
    "trfbrc_ctr = trfbrc[(trfbrc['BATCHID'] >=min_batchid) & (trfbrc['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum report id for {reporttype} trfreport file:',trfbrc_ctr['BATCHID'].min())\n",
    "print(f'Maximum report id for {reporttype} trfreport file:',trfbrc_ctr['BATCHID'].max())\n",
    "print(f'trfreport file for {reporttype} has {trfbrc_ctr.shape[0]} rows and {trfbrc_ctr.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRFTRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum report id for ctr trfreport file: 2210034048\n",
      "Maximum report id for ctr trfreport file: 2210277404\n",
      "trfreport file for ctr has 52941 rows and 55 columns\n"
     ]
    }
   ],
   "source": [
    "trftrn = pd.read_csv(file_path + '\\TRF_TRFTRN.csv')\n",
    "trftrn_ctr = trftrn[(trftrn['BATCHID'] >=min_batchid) & (trftrn['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum report id for {reporttype} trfreport file:',trftrn_ctr['BATCHID'].min())\n",
    "print(f'Maximum report id for {reporttype} trfreport file:',trftrn_ctr['BATCHID'].max())\n",
    "print(f'trfreport file for {reporttype} has {trftrn_ctr.shape[0]} rows and {trftrn_ctr.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in reading the data : 0.93 minutes\n"
     ]
    }
   ],
   "source": [
    "data_read_end_time= datetime.now()\n",
    "time_taken_in_data_read = data_read_end_time -data_read_start_time\n",
    "print('Time taken in reading the data :',round((time_taken_in_data_read.total_seconds()/60),2),'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lower the column headers to minimize thye error while importing the table into Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CTR\n",
    "#REs_Aug.columns=[col.lower() for col in REs_Aug.columns]\n",
    "#batch_ctr.columns = [col.lower() for col in batch_ctr.columns]\n",
    "#report_ctr.columns = [col.lower() for col in report_ctr.columns]\n",
    "arfbrc_ctr.columns = [col.lower() for col in arfbrc_ctr.columns]\n",
    "arfacc_ctr.columns = [col.lower() for col in arfacc_ctr.columns]\n",
    "arfinp_ctr.columns = [col.lower() for col in arfinp_ctr.columns]\n",
    "arflpe_ctr.columns = [col.lower() for col in arflpe_ctr.columns]\n",
    "#trfbatch_ctr.columns = [col.lower() for col in trfbatch_ctr.columns]\n",
    "#trfreport_ctr.columns = [col.lower() for col in trfreport_ctr.columns]\n",
    "trfbrc_ctr.columns = [col.lower() for col in trfbrc_ctr.columns]\n",
    "trftrn_ctr.columns = [col.lower() for col in trftrn_ctr.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Insert Data into Created Schema**\n",
    "        * Time Complexity for Data insertion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data insertion started at- 2023-01-19 17:05:35.600193\n"
     ]
    }
   ],
   "source": [
    "data_insertion_start_time= datetime.now()\n",
    "print('Data insertion started at-',data_insertion_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch file inserted into oct22_batch_ctr with dimensions(3298, 33)\n",
      "Report file inserted into oct22_report_ctr with dimensions (148250, 22)\n",
      "Branch file inserted into oct22_arfbrc_ctr with dimensions (1500668, 23)\n",
      "arfacc file inserted into oct22_arfacc_ctr with dimensions(1500399, 21)\n",
      "arfinp file inserted into oct22_arfinp_ctr with dimensions(1847005, 40)\n",
      "arflpe file inserted into oct22_arflpe_ctr with dimensions(1148814, 36)\n"
     ]
    }
   ],
   "source": [
    "# REs_Aug.to_sql(f'{month}_re_list',pengine,schema='monthly_ctr',if_exists='replace',index=False)\n",
    "# print(f\"batch file inserted into {month}_re_list with dimensions{ REs_Aug.shape}\")\n",
    "\n",
    "# batch_ctr.to_sql(f'{month}_arfbatch_{reporttype}',pengine,schema='monthly_ctr',if_exists='replace',index=False)\n",
    "# print(f\"batch file inserted into {month}_batch_{reporttype} with dimensions{batch_ctr.shape}\")\n",
    "\n",
    "# report_ctr.to_sql(f'{month}_arfreport_{reporttype}',pengine,schema='monthly_ctr',if_exists='replace',index=False)\n",
    "# print(f\"Report file inserted into {month}_report_{reporttype} with dimensions {report_ctr.shape}\")\n",
    "\n",
    "arfbrc_ctr.to_sql(f'{month}_arfbrc_{reporttype}',pengine,schema='monthly_ctr',if_exists='replace',index=False)\n",
    "print(f\"Branch file inserted into {month}_arfbrc_{reporttype} with dimensions {arfbrc_ctr.shape}\")\n",
    "\n",
    "arfacc_ctr.to_sql(f'{month}_arfacc_{reporttype}',pengine,schema='monthly_ctr',if_exists='replace',index=False)\n",
    "print(f\"arfacc file inserted into {month}_arfacc_{reporttype} with dimensions{arfacc_ctr.shape}\")\n",
    "\n",
    "arfinp_ctr.to_sql(f'{month}_arfinp_{reporttype}',pengine,schema='monthly_ctr',if_exists='replace',index=False)\n",
    "print(f\"arfinp file inserted into {month}_arfinp_{reporttype} with dimensions{arfinp_ctr.shape}\")\n",
    "\n",
    "arflpe_ctr.to_sql(f'{month}_arflpe_{reporttype}',pengine,schema='monthly_ctr',if_exists='replace',index=False)\n",
    "print(f\"arflpe file inserted into {month}_arflpe_{reporttype} with dimensions{arflpe_ctr.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRF files data insertion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trfbatch file inserted into oct22_trfbatch_ctr with 133 rows\n",
      "trfreport file inserted into oct22_trfreport_ctr with 2519 rows\n"
     ]
    }
   ],
   "source": [
    "#Lower the Columns\n",
    "trfbatch_ctr.columns = [col.lower() for col in trfbatch_ctr.columns]\n",
    "#Insert data to postgres\n",
    "trfbatch_ctr.to_sql(f'{month}_trfbatch_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"trfbatch file inserted into {month}_trfbatch_{reporttype} with {trfbatch_ctr.shape[0]} rows\")\n",
    "\n",
    "#Lower the Columns\n",
    "trfreport_ctr.columns = [col.lower() for col in trfreport_ctr.columns]\n",
    "#Insert data to postgres\n",
    "trfreport_ctr.to_sql(f'{month}_trfreport_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"trfreport file inserted into {month}_trfreport_{reporttype} with {trfreport_ctr.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trfbatch file inserted into oct22_trfbrc_ctr with 3576 rows\n",
      "trfreport file inserted into oct22_trftrn_ctr with 52941 rows\n"
     ]
    }
   ],
   "source": [
    "#Lower the Columns\n",
    "trfbrc_ctr.columns = [col.lower() for col in trfbrc_ctr.columns]\n",
    "#Insert data to postgres\n",
    "trfbrc_ctr.to_sql(f'{month}_trfbrc_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"trfbatch file inserted into {month}_trfbrc_{reporttype} with {trfbrc_ctr.shape[0]} rows\")\n",
    "\n",
    "#Lower the Columns\n",
    "trftrn_ctr.columns = [col.lower() for col in trftrn_ctr.columns]\n",
    "#Insert data to postgres\n",
    "trftrn_ctr.to_sql(f'{month}_trftrn_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"trfreport file inserted into {month}_trftrn_{reporttype} with {trftrn_ctr.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Create combined table of inp & lpe then insert data into arfinp_lpe table** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f'CREATE TABLE IF NOT EXISTS monthly_{reporttype}.{month}_arfinp_lpe_{reporttype}\\\n",
    "                    ( batchid bigint, rptsernum bigint, accountnum text COLLATE pg_catalog.\"default\", \\\n",
    "                    personname text COLLATE pg_catalog.\"default\", pan text COLLATE pg_catalog.\"default\" )  \\\n",
    "                    TABLESPACE pg_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f'insert into monthly_{reporttype}.{month}_arfinp_lpe_{reporttype}\\\n",
    "                select distinct batchid, rptsernum , accountnum, personname, pan \\\n",
    "                from monthly_{reporttype}.{month}_arfinp_{reporttype} \\\n",
    "                union \\\n",
    "                select distinct batchid, rptsernum , accountnum, personname, pan \\\n",
    "                from monthly_{reporttype}.{month}_arflpe_{reporttype}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined table of arfinp and arflpe is created and data is inserted into monthly_ctr.oct22_arfinp_lpe_ctr\n"
     ]
    }
   ],
   "source": [
    "print(f\"Combined table of arfinp and arflpe is created and data is inserted into monthly_{reporttype}.{month}_arfinp_lpe_{reporttype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **arftrn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [],
   "source": [
    "arftrn = pd.read_csv(file_path + '\\ARFTRN.csv',chunksize= 10000,iterator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f'CREATE TABLE IF NOT EXISTS monthly_{reporttype}.{month}_arftrn_{reporttype} ( batchid bigint,     rptsernum bigint,     accountnum text COLLATE pg_catalog.\"default\",     dateoftransaction text COLLATE pg_catalog.\"default\",     transactionid text COLLATE pg_catalog.\"default\",     transactionmode text COLLATE pg_catalog.\"default\",     debitcredit text COLLATE pg_catalog.\"default\",     amount double precision,     currency text COLLATE pg_catalog.\"default\",     producttype text COLLATE pg_catalog.\"default\",     identifier text COLLATE pg_catalog.\"default\",     transactiontype text COLLATE pg_catalog.\"default\",     units text COLLATE pg_catalog.\"default\",     rate text COLLATE pg_catalog.\"default\",     dispositionsoffunds text COLLATE pg_catalog.\"default\",     relatedaccountnum text COLLATE pg_catalog.\"default\",     relatedinstname text COLLATE pg_catalog.\"default\",     relatedinstrefnum text COLLATE pg_catalog.\"default\",     remarks text COLLATE pg_catalog.\"default\",     deletedflag text COLLATE pg_catalog.\"default\",     creationdate text COLLATE pg_catalog.\"default\",     createdby text COLLATE pg_catalog.\"default\",     xmlindexid text COLLATE pg_catalog.\"default\",     accountxmlindexid text COLLATE pg_catalog.\"default\",     reportxmlindexid text COLLATE pg_catalog.\"default\", batchxmlindexid text, branchrefnum text COLLATE pg_catalog.\"default\")  TABLESPACE pg_default')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "for df in arftrn:\n",
    "    #print(df.shape)\n",
    "    df = df[(df['BATCHID'] >=min_batchid) & (df['BATCHID'] <= max_batchid)]\n",
    "    #print(df.head())\n",
    "    df.columns=[col.lower() for col in df.columns]\n",
    "    df.to_sql(f'{month}_arftrn_{reporttype}',pengine,schema='monthly_ctr',if_exists='append',index=False)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in data insertion: 27.05 minutes\n",
      "Time taken in data reading and insertion: 27.98 minutes\n"
     ]
    }
   ],
   "source": [
    "# * Time Consumed in reading the Data \n",
    "data_insertion_end_time= datetime.now()\n",
    "time_taken_in_file_creation = data_insertion_end_time - data_insertion_start_time\n",
    "t3 = data_insertion_end_time - data_read_start_time\n",
    "print('Time taken in data insertion:',round((time_taken_in_file_creation.total_seconds()/60),2),'minutes')\n",
    "print('Time taken in data reading and insertion:',round((t3.total_seconds()/60),2),'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* ***Run till here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **CTR resolved pan**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Note:** Copy all the pans (Credit/Debit CTR, Credit NTR and Credit/Debit EFT) from the appendix tab from last month submitted monthly report and paste them in monthly update data folder in the respective excluded_pans_{} csv file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SAL008\\\\Desktop\\\\Ravi\\\\Monthly Report'"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_pans_CTR = pd.read_csv(path + '\\\\monthly update data\\\\excluded_pans_{}.csv'.format(reporttype))\n",
    "excluded_pans_CTR.to_sql(f'{month}_excluded_pans_{reporttype}',pengine,schema='monthly_ctr',if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Repeated PANS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "repeated_pans_total = pd.read_csv(path +'\\\\monthly update data\\\\repeated_pans.csv')\n",
    "repeated_pans_total.to_sql(f'pan_month_of_report_{month}',pengine,schema='monthly_report',if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Note:** Copy all the pans (Credit/Debit CTR, Credit NTR and Credit/Debit EFT) from the repeated pans tab from last month submitted monthly report and paste them in the repeated_pans_{} csv file "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Assign the month for which PANS are getting repeated**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creation of Materialized views"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month of Analysis is : 0,\n",
      "Repeated PANS month is : -1,\n",
      "pan_month_of_report is :-1\n"
     ]
    }
   ],
   "source": [
    "batchid_month = int(str(min_batchid)[3])\n",
    "repeated_month_pan = batchid_month -1 # Analysis -1 month \n",
    "pan_month_of_report = repeated_month_pan\n",
    "print(f'Month of Analysis is : {batchid_month},\\nRepeated PANS month is : {repeated_month_pan},\\npan_month_of_report is :{pan_month_of_report}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "ProgrammingError",
     "evalue": "(psycopg2.errors.UndefinedTable) relation \"monthly_ctr.oct22_arfinp_lpe_ctr\" does not exist\nLINE 1: ...larity(a.personname, b.holdername) AS score  FROM monthly_ct...\n                                                             ^\n\n[SQL: CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_ctr.oct22_ctr_resolved_pans TABLESPACE pg_default AS  SELECT x.facc, x.fiureid,x.accountnum,x.pan FROM ( SELECT y.facc, y.fiureid, y.accountnum, y.pan, rank() OVER (PARTITION BY y.facc ORDER BY y.ctr_count DESC) AS rr FROM ( SELECT DISTINCT z.fiureid || z.accountnum AS facc,  z.fiureid,z.accountnum,  z.pan,     count(DISTINCT z.batchid::character varying::text || z.rptsernum::character varying::text) AS ctr_count FROM ( SELECT c.fiureid,  a.pan, a.batchid,  a.rptsernum, a.accountnum,a.personname,b.holdername,  similarity(a.personname, b.holdername) AS score  FROM monthly_ctr.oct22_arfinp_lpe_ctr a JOIN monthly_ctr.oct22_arfacc_ctr b ON a.batchid = b.batchid AND a.rptsernum = b.rptsernum AND a.accountnum = b.accountnum AND similarity(a.personname, b.holdername) >= 0.6::double precision JOIN monthly_ctr.oct22_batch_ctr c ON a.batchid = c.batchid                          WHERE a.pan <> ALL (ARRAY['XXXXX0000X'::text, 'XXXXX9999X'::text, 'ABCD1234F'::text, 'XXXXX1234X'::text, 'XXXXX'::text, 'X'::text, 'XXXXX1111X'::text])) z                          GROUP BY (z.fiureid || z.accountnum), z.fiureid, z.accountnum, z.pan                          ORDER BY (count(DISTINCT z.batchid::character varying::text || z.rptsernum::character varying::text)) DESC) y) x                          WHERE x.rr = 1                        WITH DATA]\n(Background on this error at: http://sqlalche.me/e/13/f405)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mUndefinedTable\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[0;32m   1275\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1276\u001b[1;33m                     self.dialect.do_execute(\n\u001b[0m\u001b[0;32m   1277\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mUndefinedTable\u001b[0m: relation \"monthly_ctr.oct22_arfinp_lpe_ctr\" does not exist\nLINE 1: ...larity(a.personname, b.holdername) AS score  FROM monthly_ct...\n                                                             ^\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mProgrammingError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-191-33185941e742>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#Resolved Pans\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m resolved_pans = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_resolved_pans TABLESPACE pg_default AS  SELECT x.facc, x.fiureid,x.accountnum,x.pan FROM ( SELECT y.facc, y.fiureid, y.accountnum, y.pan, rank() OVER (PARTITION BY y.facc ORDER BY y.{reporttype}_count DESC) AS rr FROM ( SELECT DISTINCT z.fiureid || z.accountnum AS facc,  z.fiureid,z.accountnum,  z.pan,     count(DISTINCT z.batchid::character varying::text || z.rptsernum::character varying::text) AS {reporttype}_count FROM ( SELECT c.fiureid,  a.pan, a.batchid,  a.rptsernum, a.accountnum,a.personname,b.holdername,  similarity(a.personname, b.holdername) AS score  FROM monthly_{reporttype}.{month}_arfinp_lpe_{reporttype} a JOIN monthly_{reporttype}.{month}_arfacc_{reporttype} b ON a.batchid = b.batchid AND a.rptsernum = b.rptsernum AND a.accountnum = b.accountnum AND similarity(a.personname, b.holdername) >= 0.6::double precision JOIN monthly_{reporttype}.{month}_batch_{reporttype} c ON a.batchid = c.batchid\\\n\u001b[0m\u001b[0;32m      3\u001b[0m                           WHERE a.pan <> ALL (ARRAY['XXXXX0000X'::text, 'XXXXX9999X'::text, 'ABCD1234F'::text, 'XXXXX1234X'::text, 'XXXXX'::text, 'X'::text, 'XXXXX1111X'::text])) z\\\n\u001b[0;32m      4\u001b[0m                           \u001b[0mGROUP\u001b[0m \u001b[0mBY\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiureid\u001b[0m \u001b[1;33m|\u001b[0m\u001b[1;33m|\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccountnum\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfiureid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maccountnum\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mz\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpan\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m                           ORDER BY (count(DISTINCT z.batchid::character varying::text || z.rptsernum::character varying::text)) DESC) y) x\\\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, object_, *multiparams, **params)\u001b[0m\n\u001b[0;32m   1001\u001b[0m         \"\"\"\n\u001b[0;32m   1002\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1003\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1004\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1005\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobject_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute_on_connection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_text\u001b[1;34m(self, statement, multiparams, params)\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[0mdialect\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdialect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1171\u001b[0m         \u001b[0mparameters\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_distill_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmultiparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1172\u001b[1;33m         ret = self._execute_context(\n\u001b[0m\u001b[0;32m   1173\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1174\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_ctx_cls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_statement\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1316\u001b[1;33m             self._handle_dbapi_exception(\n\u001b[0m\u001b[0;32m   1317\u001b[0m                 \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[0;32m   1508\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewraise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mshould_wrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1510\u001b[1;33m                 util.raise_(\n\u001b[0m\u001b[0;32m   1511\u001b[0m                     \u001b[0msqlalchemy_exception\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1512\u001b[0m                 )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;31m# credit to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[0;32m   1274\u001b[0m                             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1275\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1276\u001b[1;33m                     self.dialect.do_execute(\n\u001b[0m\u001b[0;32m   1277\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m                     )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo_execute_no_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mProgrammingError\u001b[0m: (psycopg2.errors.UndefinedTable) relation \"monthly_ctr.oct22_arfinp_lpe_ctr\" does not exist\nLINE 1: ...larity(a.personname, b.holdername) AS score  FROM monthly_ct...\n                                                             ^\n\n[SQL: CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_ctr.oct22_ctr_resolved_pans TABLESPACE pg_default AS  SELECT x.facc, x.fiureid,x.accountnum,x.pan FROM ( SELECT y.facc, y.fiureid, y.accountnum, y.pan, rank() OVER (PARTITION BY y.facc ORDER BY y.ctr_count DESC) AS rr FROM ( SELECT DISTINCT z.fiureid || z.accountnum AS facc,  z.fiureid,z.accountnum,  z.pan,     count(DISTINCT z.batchid::character varying::text || z.rptsernum::character varying::text) AS ctr_count FROM ( SELECT c.fiureid,  a.pan, a.batchid,  a.rptsernum, a.accountnum,a.personname,b.holdername,  similarity(a.personname, b.holdername) AS score  FROM monthly_ctr.oct22_arfinp_lpe_ctr a JOIN monthly_ctr.oct22_arfacc_ctr b ON a.batchid = b.batchid AND a.rptsernum = b.rptsernum AND a.accountnum = b.accountnum AND similarity(a.personname, b.holdername) >= 0.6::double precision JOIN monthly_ctr.oct22_batch_ctr c ON a.batchid = c.batchid                          WHERE a.pan <> ALL (ARRAY['XXXXX0000X'::text, 'XXXXX9999X'::text, 'ABCD1234F'::text, 'XXXXX1234X'::text, 'XXXXX'::text, 'X'::text, 'XXXXX1111X'::text])) z                          GROUP BY (z.fiureid || z.accountnum), z.fiureid, z.accountnum, z.pan                          ORDER BY (count(DISTINCT z.batchid::character varying::text || z.rptsernum::character varying::text)) DESC) y) x                          WHERE x.rr = 1                        WITH DATA]\n(Background on this error at: http://sqlalche.me/e/13/f405)"
     ]
    }
   ],
   "source": [
    "#Resolved Pans\n",
    "resolved_pans = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_resolved_pans TABLESPACE pg_default AS  SELECT x.facc, x.fiureid,x.accountnum,x.pan FROM ( SELECT y.facc, y.fiureid, y.accountnum, y.pan, rank() OVER (PARTITION BY y.facc ORDER BY y.{reporttype}_count DESC) AS rr FROM ( SELECT DISTINCT z.fiureid || z.accountnum AS facc,  z.fiureid,z.accountnum,  z.pan,     count(DISTINCT z.batchid::character varying::text || z.rptsernum::character varying::text) AS {reporttype}_count FROM ( SELECT c.fiureid,  a.pan, a.batchid,  a.rptsernum, a.accountnum,a.personname,b.holdername,  similarity(a.personname, b.holdername) AS score  FROM monthly_{reporttype}.{month}_arfinp_lpe_{reporttype} a JOIN monthly_{reporttype}.{month}_arfacc_{reporttype} b ON a.batchid = b.batchid AND a.rptsernum = b.rptsernum AND a.accountnum = b.accountnum AND similarity(a.personname, b.holdername) >= 0.6::double precision JOIN monthly_{reporttype}.{month}_batch_{reporttype} c ON a.batchid = c.batchid\\\n",
    "                          WHERE a.pan <> ALL (ARRAY['XXXXX0000X'::text, 'XXXXX9999X'::text, 'ABCD1234F'::text, 'XXXXX1234X'::text, 'XXXXX'::text, 'X'::text, 'XXXXX1111X'::text])) z\\\n",
    "                          GROUP BY (z.fiureid || z.accountnum), z.fiureid, z.accountnum, z.pan\\\n",
    "                          ORDER BY (count(DISTINCT z.batchid::character varying::text || z.rptsernum::character varying::text)) DESC) y) x\\\n",
    "                          WHERE x.rr = 1\\\n",
    "                        WITH DATA\")\n",
    "print(f\"Materialized view {month}_{reporttype}_resolved_pans created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holder_total_account\n",
    "holder_total_pan = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan TABLESPACE pg_default AS SELECT z.facc, z.fiureid,z.accountnum,d.pan,z.holdername, z.debit_amount, z.credit_amount, z.cumulativecrturnover, z.cumulativedrturnover, z.cumulativecashdpstturnover, z.cumulativecashwdturnover   from( select a.fiureid || b.accountnum AS facc, a.fiureid, b.accountnum,  b.holdername, sum(case when debitcredit = 'C' then amount end) as credit_amount, sum(case when debitcredit = 'D' then amount end) as debit_amount, max(b.cumulativecrturnover) AS cumulativecrturnover,  \t  max(b.cumulativedrturnover) AS cumulativedrturnover, \t  max(b.cumulativecashdpstturnover) AS cumulativecashdpstturnover, \t  max(b.cumulativecashwdturnover) AS cumulativecashwdturnover from monthly_{reporttype}.{month}_batch_{reporttype} a join monthly_{reporttype}.{month}_arfacc_{reporttype} b on a.batchid = b.batchid join  monthly_{reporttype}.{month}_arftrn_{reporttype} c ON b.batchid = c.batchid AND b.rptsernum = c.rptsernum AND b.accountnum = c.accountnum where (c.batchid,c.rptsernum) in (select distinct batchid, rptsernum from monthly_{reporttype}.{month}_arfinp_lpe_{reporttype} ) group by (a.fiureid || b.accountnum), a.fiureid, b.accountnum,  b.holdername) z LEFT JOIN monthly_{reporttype}.{month}_{reporttype}_resolved_pans d ON d.facc = z.facc WITH DATA\") \n",
    "print(f\"\\n Materialized view {month}_{reporttype}_holder_totals_pan created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Holder_total_account\n",
    "# holder_total_pan = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan TABLESPACE pg_default AS SELECT z.facc, z.fiureid,z.accountnum,d.pan,z.holdername, z.debit_amount, z.credit_amount, z.cumulativecrturnover, z.cumulativedrturnover, z.cumulativecashdpstturnover, z.cumulativecashwdturnover   from( select a.fiureid || b.accountnum AS facc, a.fiureid, b.accountnum,  b.holdername, \t sum(amount) filter (where debitcredit = 'C') as credit_amount, \t sum(amount) filter (where debitcredit = 'D') as debit_amount, \t max(b.cumulativecrturnover) AS cumulativecrturnover,  \t  \t max(b.cumulativedrturnover) AS cumulativedrturnover, \t  \t max(b.cumulativecashdpstturnover) AS cumulativecashdpstturnover, \t  \t max(b.cumulativecashwdturnover) AS cumulativecashwdturnover \t from monthly_{reporttype}.{month}_batch_{reporttype} a join monthly_{reporttype}.{month}_arfacc_{reporttype} b \t on a.batchid = b.batchid join  monthly_{reporttype}.{month}_arftrn_{reporttype} c \t ON b.batchid = c.batchid \t AND b.rptsernum = c.rptsernum \t AND b.accountnum = c.accountnum \t where (c.batchid,c.rptsernum) in (select distinct batchid, rptsernum from monthly_{reporttype}.{month}_arfinp_lpe_{reporttype} ) \t group by (a.fiureid || b.accountnum), a.fiureid, b.accountnum,  b.holdername) z LEFT JOIN monthly_{reporttype}.{month}_{reporttype}_resolved_pans d ON d.facc = z.facc WITH DATA\") \n",
    "# print(f\"\\n Materialized view {month}_{reporttype}_holder_totals_pan created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Credit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Materialized Views for Credit:\\n')\n",
    "accounts_ctr_credit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_accounts_credit TABLESPACE pg_default AS  SELECT {month}_{reporttype}_holder_totals_pan.facc,{month}_{reporttype}_holder_totals_pan.fiureid,{month}_{reporttype}_holder_totals_pan.accountnum, {month}_{reporttype}_holder_totals_pan.holdername, {month}_{reporttype}_holder_totals_pan.debit_amount,{month}_{reporttype}_holder_totals_pan.credit_amount,({month}_{reporttype}_holder_totals_pan.credit_amount / 10000000::numeric) AS credit_amount_in_cr, {month}_{reporttype}_holder_totals_pan.pan   FROM monthly_ctr.{month}_{reporttype}_holder_totals_pan   ORDER BY {month}_{reporttype}_holder_totals_pan.credit_amount DESC  LIMIT 5000 WITH DATA\")#. format(month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype))\n",
    "print(f\"Materialized view created : {month}_{reporttype}_accounts_credit \")\n",
    "\n",
    "pan_credit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_pans_credit TABLESPACE pg_default AS   WITH cte1 AS (SELECT {month}_{reporttype}_holder_totals_pan.pan,{month}_{reporttype}_holder_totals_pan.holdername,count(1) AS cnt FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan GROUP BY {month}_{reporttype}_holder_totals_pan.holdername, {month}_{reporttype}_holder_totals_pan.pan), cte2 AS (SELECT cte1.pan,cte1.holdername,row_number() OVER (PARTITION BY cte1.pan ORDER BY cte1.cnt DESC) AS rnk FROM cte1), cte3 AS (SELECT {month}_{reporttype}_holder_totals_pan.pan,row_number() OVER (PARTITION BY {month}_{reporttype}_holder_totals_pan.pan ORDER BY (length({month}_{reporttype}_holder_totals_pan.holdername)) DESC) AS rr,{month}_{reporttype}_holder_totals_pan.holdername FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan)SELECT a.pan,c.holdername AS max_freq,b.holdername AS max_len,sum(a.credit_amount) AS credit_amount, CASE WHEN sum(a.cumulativecrturnover) <> 0::numeric THEN (sum(a.cumulativecashdpstturnover) / sum(a.cumulativecrturnover))*100  WHEN sum(a.cumulativecrturnover) = 0::numeric THEN '-1'::integer::numeric ELSE NULL::numeric END AS cash_dep_ratio, CASE WHEN sum(a.cumulativedrturnover) <> 0::numeric THEN (sum(a.cumulativecashwdturnover) / sum(a.cumulativedrturnover))*100  WHEN sum(a.cumulativedrturnover) = 0::numeric THEN '-1'::integer::numeric ELSE NULL::numeric END AS cash_wid_ratio, sum(a.credit_amount)/10000000 AS credit_amount_in_cr FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan a JOIN ( SELECT cte2.pan,cte2.holdername,cte2.rnk FROM cte2 WHERE cte2.rnk = 1) c ON c.pan = a.pan JOIN ( SELECT cte3.pan,cte3.holdername,cte3.rr FROM cte3 WHERE cte3.rr = 1) b ON b.pan = c.pan GROUP BY a.pan, c.holdername, b.holdername ORDER BY (sum(a.credit_amount)) DESC WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_pans_credit\")\n",
    "\n",
    "repeated_pans_credit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_repeated_pans_credit TABLESPACE pg_default AS WITH cte1 AS ( SELECT DISTINCT pan_month_of_report_{month}.pan,pan_month_of_report_{month}.report_type,count(DISTINCT pan_month_of_report_{month}.month) AS month FROM monthly_report.pan_month_of_report_{month} WHERE pan_month_of_report_{month}.report_type = 'credit_{reporttype}'::text GROUP BY pan_month_of_report_{month}.pan, pan_month_of_report_{month}.report_type HAVING count(DISTINCT pan_month_of_report_{month}.month) = {pan_month_of_report} ORDER BY (count(DISTINCT pan_month_of_report_{month}.month)) DESC), cte2 AS (SELECT {month}_pans_credit.pan,{month}_pans_credit.max_freq,{month}_pans_credit.max_len,{month}_pans_credit.credit_amount,{month}_pans_credit.cash_dep_ratio,{month}_pans_credit.cash_wid_ratio FROM monthly_{reporttype}.{month}_pans_credit) SELECT cte2.pan,cte2.max_freq,cte2.max_len,cte2.credit_amount,cte2.cash_dep_ratio,cte2.cash_wid_ratio FROM cte2 WHERE (cte2.pan IN ( SELECT DISTINCT cte1.pan FROM cte1)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_repeated_pans_credit\")\n",
    "\n",
    "credit_main_without_repeated_excluded_pan = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_credit_main_without_repeated_excluded_pan TABLESPACE pg_default AS WITH cte1 AS (SELECT {month}_excluded_pans_{reporttype}.pan, {month}_excluded_pans_{reporttype}.holdername FROM monthly_{reporttype}.{month}_excluded_pans_{reporttype}), cte2 AS (SELECT {month}_repeated_pans_credit.pan,{month}_repeated_pans_credit.max_freq FROM monthly_{reporttype}.{month}_repeated_pans_credit), cte3 AS (SELECT a.pan FROM cte1 a UNION SELECT b.pan FROM cte2 b) SELECT {month}_pans_credit.pan,{month}_pans_credit.max_freq,{month}_pans_credit.max_len,{month}_pans_credit.credit_amount,{month}_pans_credit.cash_dep_ratio,{month}_pans_credit.cash_wid_ratio FROM monthly_{reporttype}.{month}_pans_credit WHERE NOT ({month}_pans_credit.pan IN ( SELECT cte3.pan FROM cte3)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_credit_main_without_repeated_excluded_pan\")\n",
    "\n",
    "credit_excluded_pans_without_repeated_pan = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_credit_excluded_pans_without_repeated_pan TABLESPACE pg_default AS WITH cte1 AS ( SELECT {month}_pans_credit.pan,{month}_pans_credit.max_freq,{month}_pans_credit.max_len,{month}_pans_credit.credit_amount,{month}_pans_credit.cash_dep_ratio,{month}_pans_credit.cash_wid_ratio,{month}_pans_credit.credit_amount_in_cr FROM monthly_{reporttype}.{month}_pans_credit), cte2 AS ( \tSELECT cte1.pan, cte1.max_freq,            cte1.max_len,            cte1.credit_amount,            cte1.cash_dep_ratio,            cte1.cash_wid_ratio,            cte1.credit_amount_in_cr FROM cte1          WHERE (cte1.pan IN ( SELECT {month}_excluded_pans_{reporttype}.pan                   FROM monthly_{reporttype}.{month}_excluded_pans_{reporttype}))     ), cte3 AS ( SELECT {month}_repeated_pans_credit.pan FROM monthly_{reporttype}.{month}_repeated_pans_credit) SELECT cte2.pan,    cte2.max_freq,    cte2.max_len,    cte2.credit_amount,    cte2.cash_dep_ratio,    cte2.cash_wid_ratio   FROM cte2 WHERE NOT (cte2.pan IN ( SELECT cte3.pan FROM cte3)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_credit_excluded_pans_without_repeated_pan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Debit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Materialized Views for debit:\\n')\n",
    "accounts_ctr_debit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_accounts_debit TABLESPACE pg_default AS  SELECT {month}_{reporttype}_holder_totals_pan.facc,{month}_{reporttype}_holder_totals_pan.fiureid,{month}_{reporttype}_holder_totals_pan.accountnum, {month}_{reporttype}_holder_totals_pan.holdername, {month}_{reporttype}_holder_totals_pan.debit_amount,{month}_{reporttype}_holder_totals_pan.credit_amount,({month}_{reporttype}_holder_totals_pan.debit_amount / 10000000::numeric) AS debit_amount_in_cr, {month}_{reporttype}_holder_totals_pan.pan   FROM monthly_ctr.{month}_{reporttype}_holder_totals_pan   ORDER BY {month}_{reporttype}_holder_totals_pan.debit_amount DESC  LIMIT 5000 WITH DATA\")#. format(month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype,month,reporttype))\n",
    "print(f\"Materialized view created : {month}_{reporttype}_accounts_debit \")\n",
    "\n",
    "pan_debit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_pans_debit TABLESPACE pg_default AS   WITH cte1 AS (SELECT {month}_{reporttype}_holder_totals_pan.pan,{month}_{reporttype}_holder_totals_pan.holdername,count(1) AS cnt FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan GROUP BY {month}_{reporttype}_holder_totals_pan.holdername, {month}_{reporttype}_holder_totals_pan.pan), cte2 AS (SELECT cte1.pan,cte1.holdername,row_number() OVER (PARTITION BY cte1.pan ORDER BY cte1.cnt DESC) AS rnk FROM cte1), cte3 AS (SELECT {month}_{reporttype}_holder_totals_pan.pan,row_number() OVER (PARTITION BY {month}_{reporttype}_holder_totals_pan.pan ORDER BY (length({month}_{reporttype}_holder_totals_pan.holdername)) DESC) AS rr,{month}_{reporttype}_holder_totals_pan.holdername FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan)SELECT a.pan,c.holdername AS max_freq,b.holdername AS max_len,sum(a.debit_amount) AS debit_amount, CASE WHEN sum(a.cumulativecrturnover) <> 0::numeric THEN (sum(a.cumulativecashdpstturnover) / sum(a.cumulativecrturnover))*100  WHEN sum(a.cumulativecrturnover) = 0::numeric THEN '-1'::integer::numeric ELSE NULL::numeric END AS cash_dep_ratio, CASE WHEN sum(a.cumulativedrturnover) <> 0::numeric THEN (sum(a.cumulativecashwdturnover) / sum(a.cumulativedrturnover))*100  WHEN sum(a.cumulativedrturnover) = 0::numeric THEN '-1'::integer::numeric ELSE NULL::numeric END AS cash_wid_ratio, sum(a.debit_amount)/10000000 AS debit_amount_in_cr FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan a JOIN ( SELECT cte2.pan,cte2.holdername,cte2.rnk FROM cte2 WHERE cte2.rnk = 1) c ON c.pan = a.pan JOIN ( SELECT cte3.pan,cte3.holdername,cte3.rr FROM cte3 WHERE cte3.rr = 1) b ON b.pan = c.pan GROUP BY a.pan, c.holdername, b.holdername ORDER BY (sum(a.debit_amount)) DESC WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_pans_debit\")\n",
    "\n",
    "repeated_pans_debit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_repeated_pans_debit TABLESPACE pg_default AS WITH cte1 AS ( SELECT DISTINCT pan_month_of_report_{month}.pan,pan_month_of_report_{month}.report_type,count(DISTINCT pan_month_of_report_{month}.month) AS month FROM monthly_report.pan_month_of_report_{month} WHERE pan_month_of_report_{month}.report_type = 'debit_{reporttype}'::text GROUP BY pan_month_of_report_{month}.pan, pan_month_of_report_{month}.report_type HAVING count(DISTINCT pan_month_of_report_{month}.month) = {pan_month_of_report} ORDER BY (count(DISTINCT pan_month_of_report_{month}.month)) DESC), cte2 AS (SELECT {month}_pans_debit.pan,{month}_pans_debit.max_freq,{month}_pans_debit.max_len,{month}_pans_debit.debit_amount,{month}_pans_debit.cash_dep_ratio,{month}_pans_debit.cash_wid_ratio FROM monthly_{reporttype}.{month}_pans_debit) SELECT cte2.pan,cte2.max_freq,cte2.max_len,cte2.debit_amount,cte2.cash_dep_ratio,cte2.cash_wid_ratio FROM cte2 WHERE (cte2.pan IN ( SELECT DISTINCT cte1.pan FROM cte1)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_repeated_pans_debit\")\n",
    "\n",
    "debit_main_without_repeated_excluded_pan = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_debit_main_without_repeated_excluded_pan TABLESPACE pg_default AS WITH cte1 AS (SELECT {month}_excluded_pans_{reporttype}.pan, {month}_excluded_pans_{reporttype}.holdername FROM monthly_{reporttype}.{month}_excluded_pans_{reporttype}), cte2 AS (SELECT {month}_repeated_pans_debit.pan,{month}_repeated_pans_debit.max_freq FROM monthly_{reporttype}.{month}_repeated_pans_debit), cte3 AS (SELECT a.pan FROM cte1 a UNION SELECT b.pan FROM cte2 b) SELECT {month}_pans_debit.pan,{month}_pans_debit.max_freq,{month}_pans_debit.max_len,{month}_pans_debit.debit_amount,{month}_pans_debit.cash_dep_ratio,{month}_pans_debit.cash_wid_ratio FROM monthly_{reporttype}.{month}_pans_debit WHERE NOT ({month}_pans_debit.pan IN ( SELECT cte3.pan FROM cte3)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_debit_main_without_repeated_excluded_pan\")\n",
    "\n",
    "debit_excluded_pans_without_repeated_pan = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_debit_excluded_pans_without_repeated_pan TABLESPACE pg_default AS WITH cte1 AS ( SELECT {month}_pans_debit.pan,{month}_pans_debit.max_freq,{month}_pans_debit.max_len,{month}_pans_debit.debit_amount,{month}_pans_debit.cash_dep_ratio,{month}_pans_debit.cash_wid_ratio,{month}_pans_debit.debit_amount_in_cr FROM monthly_{reporttype}.{month}_pans_debit), cte2 AS ( \tSELECT cte1.pan, cte1.max_freq,            cte1.max_len,            cte1.debit_amount,            cte1.cash_dep_ratio,            cte1.cash_wid_ratio,            cte1.debit_amount_in_cr FROM cte1          WHERE (cte1.pan IN ( SELECT {month}_excluded_pans_{reporttype}.pan                   FROM monthly_{reporttype}.{month}_excluded_pans_{reporttype}))     ), cte3 AS ( SELECT {month}_repeated_pans_debit.pan FROM monthly_{reporttype}.{month}_repeated_pans_debit) SELECT cte2.pan,    cte2.max_freq,    cte2.max_len,    cte2.debit_amount,    cte2.cash_dep_ratio,    cte2.cash_wid_ratio   FROM cte2 WHERE NOT (cte2.pan IN ( SELECT cte3.pan FROM cte3)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_debit_excluded_pans_without_repeated_pan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Reconnect Postgres Engine**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Postgres database connected succesfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pengine = create_engine('postgresql+psycopg2://postgres:sal@123@localhost:5432/postgres')\n",
    "\n",
    "    conn = psycopg2.connect(database = 'postgres', user = 'postgres', password = 'sal@123',host = \"localhost\",port= 5432)\n",
    "    conn.autocommit = True\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "except:\n",
    "    print(\"Unable to create the Postgres DB connection\")\n",
    "\n",
    "db_conn=pengine.connect()\n",
    "print(\"\\n Postgres database connected succesfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **CTR - Schema Creation if doesn't exists**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report type is: ccr\n"
     ]
    }
   ],
   "source": [
    "reporttype = 'ccr'\n",
    "print('Report type is:',reporttype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [],
   "source": [
    "monthly_str_query = \"CREATE SCHEMA IF NOT EXISTS %s AUTHORIZATION %s;\"\n",
    "monthly_str = (AsIs(f'monthly_{reporttype}'),AsIs('postgres'))\n",
    "cursor.execute(monthly_str_query,monthly_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CHeck for the data path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SAL008\\\\Desktop\\\\Ravi\\\\Monthly Report\\\\Sep 2022\\\\Data\\\\CCR_SEP_NOV_22_DUMP'"
      ]
     },
     "execution_count": 238,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file_path = path +'\\\\'+month_report_format+ f'\\Data\\SAL_{reporttype.upper()}_{month[:3].upper()}2022'\n",
    "file_path = r\"C:\\Users\\SAL008\\Desktop\\Ravi\\Monthly Report\\Sep 2022\\Data\\CCR_SEP_NOV_22_DUMP\"\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataread Start time\n",
    "data_read_start_time= datetime.now()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **batch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id is- 2210010000\n",
      "Maximum batch id is- 2210319999\n"
     ]
    }
   ],
   "source": [
    "print('Minimum batch id is-', min_batchid)\n",
    "print('Maximum batch id is-', max_batchid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for oct22 BATCH file: 2210033507\n",
      "Maximum batch id for oct22 BATCH file: 2210297576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(212, 33)"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = pd.read_csv(file_path + '\\\\BATCH.csv')\n",
    "batch_ccr = batch[(batch['BATCHID'] >=min_batchid) & (batch['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {month} BATCH file:',batch_ccr['BATCHID'].min())\n",
    "print(f'Maximum batch id for {month} BATCH file:',batch_ccr['BATCHID'].max())\n",
    "batch_ccr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **CRFRPT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for oct22 REPORT file: 2210033507\n",
      "Maximum batch id for oct22 REPORT file: 2210297576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15566, 30)"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "report = pd.read_csv(file_path + '\\CRFRPT.csv')\n",
    "report_ccr = report[(report['BATCHID'] >=min_batchid) & (report['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {month} REPORT file:',report_ccr['BATCHID'].min())\n",
    "print(f'Maximum batch id for {month} REPORT file:',report_ccr['BATCHID'].max())\n",
    "report_ccr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **CRFBRC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for oct22 oct22 CRFBRC file: 2210033507\n",
      "Maximum batch id for oct22 CRFBRC file: 2210297576\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(15566, 20)"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arfbrc = pd.read_csv(file_path + '\\CRFBRC.csv')\n",
    "arfbrc_ccr = arfbrc[(arfbrc['BATCHID'] >=min_batchid) & (arfbrc['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {month} {month} CRFBRC file:',arfbrc_ccr['BATCHID'].min())\n",
    "print(f'Maximum batch id for {month} CRFBRC file:',arfbrc_ccr['BATCHID'].max())\n",
    "arfbrc_ccr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SAL008\\\\Desktop\\\\Ravi\\\\Monthly Report\\\\Oct 2022\\\\Data\\\\SAL_CCR_OCT2022'"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path = path +'\\\\'+month_report_format+ f'\\Data\\SAL_{reporttype.upper()}_{month[:3].upper()}2022'\n",
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ccr.to_csv(out_path + \"\\\\batch.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_ccr.to_csv(out_path + \"\\\\report.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "arfbrc_ccr.to_csv(out_path + \"\\\\arfbrc.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in reading the data : 74.98 minutes\n"
     ]
    }
   ],
   "source": [
    "data_read_end_time= datetime.now()\n",
    "time_taken_in_data_read = data_read_end_time -data_read_start_time\n",
    "print('Time taken in reading the data :',round((time_taken_in_data_read.total_seconds()/60),2),'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Lower the column headers to minimize thye error while importing the table into Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ccr\n",
    "#REs_Aug.columns=[col.lower() for col in REs_Aug.columns]\n",
    "batch_ccr.columns=[col.lower() for col in batch_ccr.columns]\n",
    "report_ccr.columns=[col.lower() for col in report_ccr.columns]\n",
    "arfbrc_ccr.columns=[col.lower() for col in arfbrc_ccr.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **CCR Insert Data into Created Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data insertion started at- 2023-01-20 15:13:50.102958\n"
     ]
    }
   ],
   "source": [
    "data_insertion_start_time= datetime.now()\n",
    "print('Data insertion started at-',data_insertion_start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch file inserted into oct22_batch_ccr with dimensions(212, 33)\n",
      "Report file inserted into oct22_report_ccr with dimensions (15566, 30)\n",
      "Branch file inserted into oct22_arfbrc_ccr with dimensions (15566, 20)\n"
     ]
    }
   ],
   "source": [
    "# REs_Aug.to_sql(f'{month}_re_list',pengine,schema='monthly_str',if_exists='replace',index=False)\n",
    "# print(f\"batch file inserted into {month}_re_list with dimensions{ REs_Aug.shape}\")\n",
    "\n",
    "batch_ccr.to_sql(f'{month}_batch_{reporttype}',pengine,schema='monthly_ccr',if_exists='replace',index=False)\n",
    "print(f\"batch file inserted into {month}_batch_{reporttype} with dimensions{batch_ccr.shape}\")\n",
    "\n",
    "report_ccr.to_sql(f'{month}_report_{reporttype}',pengine,schema='monthly_ccr',if_exists='replace',index=False)\n",
    "print(f\"Report file inserted into {month}_report_{reporttype} with dimensions {report_ccr.shape}\")\n",
    "\n",
    "arfbrc_ccr.to_sql(f'{month}_arfbrc_{reporttype}',pengine,schema='monthly_ccr',if_exists='replace',index=False)\n",
    "print(f\"Branch file inserted into {month}_arfbrc_{reporttype} with dimensions {arfbrc_ccr.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in reading the data : 0.1 minutes\n"
     ]
    }
   ],
   "source": [
    "# * Time Consumed in reading the Data \n",
    "data_insertion_end_time= datetime.now()\n",
    "time_taken_in_data_insertion = data_insertion_end_time - data_insertion_start_time\n",
    "print('Time taken in reading the data :',round((time_taken_in_data_insertion.total_seconds()/60),2),'minutes')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken in data reading and data insertion: 75.19 minutes\n"
     ]
    }
   ],
   "source": [
    "# * Total Time taken in reading and inserting the data \n",
    "total_time_taken_in_data_read_insertion = data_insertion_end_time -  data_read_start_time\n",
    "print('Time taken in data reading and data insertion:',round((total_time_taken_in_data_read_insertion.total_seconds()/60),2),'minutes')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report type is: ntr\n",
      "Month of analysis : oct22\n"
     ]
    }
   ],
   "source": [
    "reporttype ='ntr'\n",
    "print('Report type is:',reporttype)\n",
    "print('Month of analysis :',month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Check the timeframe *************\n",
      "Start BATCHID: 2210010000\n",
      "End BATCHID: 2210319999\n"
     ]
    }
   ],
   "source": [
    "print('****** Check the timeframe *************')\n",
    "print('Start BATCHID:',min_batchid)\n",
    "print('End BATCHID:',max_batchid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Resetting the Database Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Postgres database connected succesfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pengine = create_engine('postgresql+psycopg2://postgres:sal@123@localhost:5432/postgres')\n",
    "\n",
    "    conn = psycopg2.connect(database = 'postgres', user = 'postgres', password = 'sal@123',host = \"localhost\",port= 5432)\n",
    "    conn.autocommit = True\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "except:\n",
    "    print(\"Unable to create the Postgres DB connection\")\n",
    "\n",
    "db_conn=pengine.connect()\n",
    "print(\"\\n Postgres database connected succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #NTR\n",
    "monthly_ntr_query = \"CREATE SCHEMA IF NOT EXISTS %s AUTHORIZATION %s;\"\n",
    "monthly_ntr = (AsIs(f'monthly_{reporttype}'),AsIs('postgres'))\n",
    "cursor.execute(monthly_ntr_query,monthly_ntr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Insertion into NTR Schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SAL008\\\\Desktop\\\\Ravi\\\\Monthly Report\\\\Sep 2022\\\\Data\\\\NTR_SEP_NOV_22_DUMP'"
      ]
     },
     "execution_count": 259,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#file_path = path +'\\\\'+month_report_format+ f'\\Data\\SAL_{reporttype.upper()}_{month[:3].upper()}2022'\n",
    "file_path = r\"C:\\Users\\SAL008\\Desktop\\Ravi\\Monthly Report\\Sep 2022\\Data\\NTR_SEP_NOV_22_DUMP\"\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SAL008\\\\Desktop\\\\Ravi\\\\Monthly Report\\\\Oct 2022\\\\Data\\\\SAL_NTR_OCT2022'"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_path = path +'\\\\'+month_report_format+ f'\\Data\\SAL_{reporttype.upper()}_{month[:3].upper()}2022'\n",
    "out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for ntr BATCH file: 2210033457\n",
      "Maximum batch id for ntr BATCH file: 2210287510\n",
      "BATCH file for ntr has 409 rows and 33 columns\n"
     ]
    }
   ],
   "source": [
    "batch = pd.read_csv(file_path + '\\BATCH.csv')\n",
    "batch_ntr = batch[(batch['BATCHID'] >=min_batchid) & (batch['BATCHID'] <= max_batchid) & (batch['REPORTTYPE'] =='NTR')]\n",
    "print(f'Minimum batch id for {reporttype} BATCH file:',batch_ntr['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} BATCH file:',batch_ntr['BATCHID'].max())\n",
    "print(f'BATCH file for {reporttype} has {batch_ntr.shape[0]} rows and {batch_ntr.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for ntr report file: 2210033457\n",
      "Maximum batch id for ntr report file: 2210287510\n",
      "REPORT file for ntr has 70912 rows and 22 columns\n"
     ]
    }
   ],
   "source": [
    "report = pd.read_csv(file_path + '\\REPORT.csv')\n",
    "report_ntr = report[(report['BATCHID'] >=min_batchid) & (report['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} report file:',report_ntr['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} report file:',report_ntr['BATCHID'].max())\n",
    "print(f'REPORT file for {reporttype} has {report_ntr.shape[0]} rows and {report_ntr.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARFBRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for ntr arfbrc file: 2210033457\n",
      "Maximum batch id for ntr arfbrc file: 2210287510\n",
      "ARFBRC file for ntr has 95148 rows and 23 columns\n"
     ]
    }
   ],
   "source": [
    "arfbrc = pd.read_csv(file_path + '\\ARFBRC.csv')\n",
    "arfbrc_ntr = arfbrc[(arfbrc['BATCHID'] >=min_batchid) & (arfbrc['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} arfbrc file:',arfbrc_ntr['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} arfbrc file:',arfbrc_ntr['BATCHID'].max())\n",
    "print(f'ARFBRC file for {reporttype} has {arfbrc_ntr.shape[0]} rows and {arfbrc_ntr.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARFACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for ntr arfacc file: 2210033457\n",
      "Maximum batch id for ntr arfacc file: 2210287510\n",
      "ARFACC file for ntr has 95148 rows and 21 columns\n"
     ]
    }
   ],
   "source": [
    "arfacc = pd.read_csv(file_path + '\\ARFACC.csv')\n",
    "arfacc_ntr = arfacc[(arfacc['BATCHID'] >=min_batchid) & (arfacc['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} arfacc file:',arfacc_ntr['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} arfacc file:',arfacc_ntr['BATCHID'].max())\n",
    "print(f'ARFACC file for {reporttype} has {arfacc_ntr.shape[0]} rows and {arfacc_ntr.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARFINP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for ntr arfinp file: 2210033457\n",
      "Maximum batch id for ntr arfinp file: 2210287510\n",
      "ARFINP file for ntr has 222270 rows and 40 columns\n"
     ]
    }
   ],
   "source": [
    "arfinp = pd.read_csv(file_path + '\\ARFINP.csv')\n",
    "arfinp_ntr = arfinp[(arfinp['BATCHID'] >=min_batchid) & (arfinp['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} arfinp file:',arfinp_ntr['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} arfinp file:',arfinp_ntr['BATCHID'].max())\n",
    "print(f'ARFINP file for {reporttype} has {arfinp_ntr.shape[0]} rows and {arfinp_ntr.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARFLPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for ntr arflpe file: 2210033457\n",
      "Maximum batch id for ntr arflpe file: 2210287510\n",
      "ARFLPE file for ntr has 101401 rows and 36 columns\n"
     ]
    }
   ],
   "source": [
    "arflpe = pd.read_csv(file_path + '\\ARFLPE.csv')\n",
    "arflpe_ntr = arflpe[(arflpe['BATCHID'] >=min_batchid) & (arflpe['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} arflpe file:',arflpe_ntr['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} arflpe file:',arflpe_ntr['BATCHID'].max())\n",
    "print(f'ARFLPE file for {reporttype} has {arflpe_ntr.shape[0]} rows and {arflpe_ntr.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARFTRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for ntr arftrn file: 2210033457\n",
      "Maximum batch id for ntr arftrn file: 2210287510\n",
      "ARFTRN file for ntr has 687069 rows and 27 columns\n"
     ]
    }
   ],
   "source": [
    "arftrn = pd.read_csv(file_path + '\\ARFTRN.csv')\n",
    "arftrn_ntr = arftrn[(arftrn['BATCHID'] >=min_batchid) & (arftrn['BATCHID'] <= max_batchid )]\n",
    "print(f'Minimum batch id for {reporttype} arftrn file:',arftrn_ntr['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} arftrn file:',arftrn_ntr['BATCHID'].max())\n",
    "print(f'ARFTRN file for {reporttype} has {arftrn_ntr.shape[0]} rows and {arftrn_ntr.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Writing the files to csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_ntr.to_csv(out_path + \"\\\\batch.csv\")\n",
    "report_ntr.to_csv(out_path + \"\\\\report.csv\")\n",
    "arfbrc_ntr.to_csv(out_path + \"\\\\arfbrc.csv\")\n",
    "arfacc_ntr.to_csv(out_path + \"\\\\arfacc.csv\")\n",
    "arfinp_ntr.to_csv(out_path + \"\\\\arfinp.csv\")\n",
    "arflpe_ntr.to_csv(out_path + \"\\\\arflpe.csv\")\n",
    "arftrn_ntr.to_csv(out_path + \"\\\\arftrn.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Make the column headers to lowercase to avoid ambiguity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NTR\n",
    "batch_ntr.columns=[col.lower() for col in batch_ntr.columns]\n",
    "report_ntr.columns=[col.lower() for col in report_ntr.columns]\n",
    "arfbrc_ntr.columns=[col.lower() for col in arfbrc_ntr.columns]\n",
    "arfacc_ntr.columns=[col.lower() for col in arfacc_ntr.columns]\n",
    "arfinp_ntr.columns=[col.lower() for col in arfinp_ntr.columns]\n",
    "arflpe_ntr.columns=[col.lower() for col in arflpe_ntr.columns]\n",
    "arftrn_ntr.columns=[col.lower() for col in arftrn_ntr.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data Push to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch file inserted into oct22_batch_ntr with 409 rows\n",
      "Report file inserted into oct22_report_ntr with 70912 rows\n",
      "Branch file inserted into oct22_arfbrc_ntr with  95148 rows\n",
      "arfacc file inserted into oct22_arfacc_ntr with 222270 rows\n",
      "arfinp file inserted into oct22_arfinp_ntr with 101401 rows\n",
      "arflpe file inserted into oct22_arftrn_ntr with 687069 rows\n",
      "arflpe file inserted into oct22_arflpe_ntr with 95148 rows\n"
     ]
    }
   ],
   "source": [
    "# NTR Data\n",
    "batch_ntr.to_sql(f'{month}_batch_{reporttype}',pengine,schema='monthly_ntr',if_exists='replace',index=False)\n",
    "print(f\"Batch file inserted into {month}_batch_{reporttype} with {batch_ntr.shape[0]} rows\")\n",
    "\n",
    "report_ntr.to_sql(f'{month}_report_{reporttype}',pengine,schema='monthly_ntr',if_exists='replace',index=False)\n",
    "print(f\"Report file inserted into {month}_report_{reporttype} with {report_ntr.shape[0]} rows\")\n",
    "\n",
    "arfbrc_ntr.to_sql(f'{month}_arfbrc_{reporttype}',pengine,schema='monthly_ntr',if_exists='replace',index=False)\n",
    "print(f\"Branch file inserted into {month}_arfbrc_{reporttype} with  {arfbrc_ntr.shape[0]} rows\")\n",
    "\n",
    "arfinp_ntr.to_sql(f'{month}_arfinp_{reporttype}',pengine,schema='monthly_ntr',if_exists='replace',index=False)\n",
    "print(f\"arfacc file inserted into {month}_arfacc_{reporttype} with {arfinp_ntr.shape[0]} rows\")\n",
    "\n",
    "arflpe_ntr.to_sql(f'{month}_arflpe_{reporttype}',pengine,schema='monthly_ntr',if_exists='replace',index=False)\n",
    "print(f\"arfinp file inserted into {month}_arfinp_{reporttype} with {arflpe_ntr.shape[0]} rows\")\n",
    "\n",
    "arftrn_ntr.to_sql(f'{month}_arftrn_{reporttype}',pengine,schema='monthly_ntr',if_exists='replace',index=False)\n",
    "print(f\"arflpe file inserted into {month}_arftrn_{reporttype} with {arftrn_ntr.shape[0]} rows\")\n",
    "\n",
    "arfacc_ntr.to_sql(f'{month}_arfacc_{reporttype}',pengine,schema='monthly_ntr',if_exists='replace',index=False)\n",
    "print(f\"arflpe file inserted into {month}_arflpe_{reporttype} with {arfacc_ntr.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create and Insert data into arfinp_lpe table under monthly_ntr schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f'CREATE TABLE IF NOT EXISTS monthly_{reporttype}.{month}_arfinp_lpe_{reporttype}\\\n",
    "                    ( batchid bigint, rptsernum bigint, accountnum text COLLATE pg_catalog.\"default\", \\\n",
    "                    personname text COLLATE pg_catalog.\"default\",     pan text COLLATE pg_catalog.\"default\" )  \\\n",
    "                    TABLESPACE pg_default; ALTER TABLE IF EXISTS monthly_{reporttype}.{month}_arfinp_lpe_{reporttype}    \\\n",
    "                    OWNER to postgres;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f'insert into monthly_{reporttype}.{month}_arfinp_lpe_{reporttype} \\\n",
    "                    select distinct batchid, rptsernum , accountnum, personname, pan \\\n",
    "                    from monthly_{reporttype}.{month}_arfinp_{reporttype} \\\n",
    "                    union \\\n",
    "                    select distinct batchid, rptsernum , accountnum, personname, pan \\\n",
    "                    from monthly_{reporttype}.{month}_arflpe_{reporttype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a table for exclusion list of NTR PANS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **NTR resolved pan** - Copy all the Credit NTR pans from the appendix tab from last month submitted monthly report and paste them in monthly update data folder in the respective excluded_pans_{} csv file.* "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reporttype = 'ntr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_pans_NTR = pd.read_csv(path + '\\\\monthly update data\\\\excluded_pans_{}.csv'.format(reporttype))\n",
    "excluded_pans_NTR.to_sql(f'{month}_excluded_pans_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Materialized View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Resolved Pans\n",
    "resolved_pans = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_resolved_pans TABLESPACE pg_default AS  SELECT x.facc, x.fiureid,x.accountnum,x.pan FROM ( SELECT y.facc, y.fiureid, y.accountnum, y.pan, rank() OVER (PARTITION BY y.facc ORDER BY y.{reporttype}_count DESC) AS rr FROM ( SELECT DISTINCT z.fiureid || z.accountnum AS facc,  z.fiureid,z.accountnum,  z.pan,     count(DISTINCT z.batchid::character varying::text || z.rptsernum::character varying::text) AS {reporttype}_count FROM ( SELECT c.fiureid,  a.pan, a.batchid,  a.rptsernum, a.accountnum,a.personname,b.holdername,  similarity(a.personname, b.holdername) AS score  FROM monthly_{reporttype}.{month}_arfinp_lpe_{reporttype} a JOIN monthly_{reporttype}.{month}_arfacc_{reporttype} b ON a.batchid = b.batchid AND a.rptsernum = b.rptsernum AND a.accountnum = b.accountnum AND similarity(a.personname, b.holdername) >= 0.6::double precision JOIN monthly_{reporttype}.{month}_batch_{reporttype} c ON a.batchid = c.batchid\\\n",
    "                          WHERE a.pan <> ALL (ARRAY['XXXXX0000X'::text, 'XXXXX9999X'::text, 'ABCD1234F'::text, 'XXXXX1234X'::text, 'XXXXX'::text, 'X'::text, 'XXXXX1111X'::text])) z\\\n",
    "                  GROUP BY (z.fiureid || z.accountnum), z.fiureid, z.accountnum, z.pan\\\n",
    "                  ORDER BY (count(DISTINCT z.batchid::character varying::text || z.rptsernum::character varying::text)) DESC) y) x\\\n",
    "                  WHERE x.rr = 1\\\n",
    "                WITH DATA\")\n",
    "print(f\"Materialized view {month}_{reporttype}_resolved_pans created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Holder_total_account\n",
    "holder_total_pan = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan TABLESPACE pg_default AS SELECT z.facc, z.fiureid,z.accountnum,z.holdername,z.debit_amount,z.credit_amount,d.pan, z.cumulativecrturnover, z.cumulativedrturnover, z.cumulativecashdpstturnover, z.cumulativecashwdturnover  FROM ( SELECT c.fiureid || a.accountnum AS facc, c.fiureid, a.accountnum,  a.holdername, \t  sum(case when debitcredit = 'D' then amount else 0 end) AS debit_amount, \t  sum(case when debitcredit = 'C' then amount else 0 end) AS credit_amount,  \t  max(a.cumulativecrturnover) AS cumulativecrturnover,  \t  max(a.cumulativedrturnover) AS cumulativedrturnover, \t  max(a.cumulativecashdpstturnover) AS cumulativecashdpstturnover, \t  max(a.cumulativecashwdturnover) AS cumulativecashwdturnover \t  FROM monthly_{reporttype}.{month}_arfacc_{reporttype} a    \t  JOIN monthly_{reporttype}.{month}_arftrn_{reporttype} b ON a.batchid = b.batchid \t  AND a.rptsernum = b.rptsernum AND a.accountnum = b.accountnum \t  JOIN monthly_{reporttype}.{month}_batch_{reporttype} c ON a.batchid = c.batchid   \t  GROUP BY (c.fiureid || a.accountnum), c.fiureid, a.accountnum, a.holdername) z  \t  LEFT JOIN monthly_{reporttype}.{month}_{reporttype}_resolved_pans d ON d.facc = z.facc WITH DATA\") \n",
    "print(f\"Materialized view {month}_{reporttype}_holder_totals_pan created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Credit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Materialized Views for Credit:\\n')\n",
    "accounts_ntr_credit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_accounts_credit TABLESPACE pg_default AS  SELECT {month}_{reporttype}_holder_totals_pan.facc,{month}_{reporttype}_holder_totals_pan.fiureid,{month}_{reporttype}_holder_totals_pan.accountnum, {month}_{reporttype}_holder_totals_pan.holdername, {month}_{reporttype}_holder_totals_pan.credit_amount,({month}_{reporttype}_holder_totals_pan.credit_amount / 10000000::numeric) AS credit_amount_in_cr, {month}_{reporttype}_holder_totals_pan.pan   FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan   ORDER BY {month}_{reporttype}_holder_totals_pan.credit_amount DESC  LIMIT 5000 WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_accounts_credit \")\n",
    "\n",
    "pan_credit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_pans_credit TABLESPACE pg_default AS   WITH cte1 AS (SELECT {month}_{reporttype}_holder_totals_pan.pan,{month}_{reporttype}_holder_totals_pan.holdername,count(1) AS cnt FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan GROUP BY {month}_{reporttype}_holder_totals_pan.holdername, {month}_{reporttype}_holder_totals_pan.pan), cte2 AS (SELECT cte1.pan,cte1.holdername,row_number() OVER (PARTITION BY cte1.pan ORDER BY cte1.cnt DESC) AS rnk FROM cte1), cte3 AS (SELECT {month}_{reporttype}_holder_totals_pan.pan,row_number() OVER (PARTITION BY {month}_{reporttype}_holder_totals_pan.pan ORDER BY (length({month}_{reporttype}_holder_totals_pan.holdername)) DESC) AS rr,{month}_{reporttype}_holder_totals_pan.holdername FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan)SELECT a.pan,c.holdername AS max_freq,b.holdername AS max_len,sum(a.credit_amount) AS credit_amount, CASE WHEN sum(a.cumulativecrturnover) <> 0::numeric THEN (sum(a.cumulativecashdpstturnover) / sum(a.cumulativecrturnover) * 100::numeric) WHEN sum(a.cumulativecrturnover) = 0::numeric THEN '-1'::integer::numeric ELSE NULL::numeric END AS cash_dep_ratio, CASE WHEN sum(a.cumulativedrturnover) <> 0::numeric THEN (sum(a.cumulativecashwdturnover) / sum(a.cumulativedrturnover) * 100::numeric) WHEN sum(a.cumulativedrturnover) = 0::numeric THEN '-1'::integer::numeric ELSE NULL::numeric END AS cash_wid_ratio,sum(a.credit_amount) / 10000000::numeric AS credit_amount_in_cr FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan a JOIN ( SELECT cte2.pan,cte2.holdername,cte2.rnk FROM cte2 WHERE cte2.rnk = 1) c ON c.pan = a.pan JOIN ( SELECT cte3.pan,cte3.holdername,cte3.rr FROM cte3 WHERE cte3.rr = 1) b ON b.pan = c.pan GROUP BY a.pan, c.holdername, b.holdername ORDER BY (sum(a.credit_amount)) DESC WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_pans_credit\")\n",
    "\n",
    "repeated_pans_credit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_repeated_pans_credit TABLESPACE pg_default AS WITH cte1 AS ( SELECT DISTINCT pan_month_of_report_{month}.pan,pan_month_of_report_{month}.report_type,count(DISTINCT pan_month_of_report_{month}.month) AS month FROM monthly_report.pan_month_of_report_{month} WHERE pan_month_of_report_{month}.report_type = 'credit_{reporttype}'::text GROUP BY pan_month_of_report_{month}.pan, pan_month_of_report_{month}.report_type HAVING count(DISTINCT pan_month_of_report_{month}.month) = {pan_month_of_report} ORDER BY (count(DISTINCT pan_month_of_report_{month}.month)) DESC), cte2 AS (SELECT {month}_pans_credit.pan,{month}_pans_credit.max_freq,{month}_pans_credit.max_len,{month}_pans_credit.credit_amount,{month}_pans_credit.cash_dep_ratio,{month}_pans_credit.cash_wid_ratio FROM monthly_{reporttype}.{month}_pans_credit) SELECT cte2.pan,cte2.max_freq,cte2.max_len,cte2.credit_amount,cte2.cash_dep_ratio,cte2.cash_wid_ratio FROM cte2 WHERE (cte2.pan IN ( SELECT DISTINCT cte1.pan FROM cte1)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_repeated_pans_credit\")\n",
    "\n",
    "credit_main_without_repeated_excluded_pan = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_credit_main_without_repeated_excluded_pan TABLESPACE pg_default AS WITH cte1 AS (SELECT {month}_excluded_pans_{reporttype}.pan, {month}_excluded_pans_{reporttype}.holdername FROM monthly_{reporttype}.{month}_excluded_pans_{reporttype}), cte2 AS (SELECT {month}_repeated_pans_credit.pan,{month}_repeated_pans_credit.max_freq FROM monthly_{reporttype}.{month}_repeated_pans_credit), cte3 AS (SELECT a.pan FROM cte1 a UNION SELECT b.pan FROM cte2 b) SELECT {month}_pans_credit.pan,{month}_pans_credit.max_freq,{month}_pans_credit.max_len,{month}_pans_credit.credit_amount,{month}_pans_credit.cash_dep_ratio,{month}_pans_credit.cash_wid_ratio FROM monthly_{reporttype}.{month}_pans_credit WHERE NOT ({month}_pans_credit.pan IN ( SELECT cte3.pan FROM cte3)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_credit_main_without_repeated_excluded_pan\")\n",
    "\n",
    "credit_excluded_pans_without_repeated_pan = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_credit_excluded_pans_without_repeated_pan TABLESPACE pg_default AS WITH cte1 AS ( SELECT {month}_pans_credit.pan,{month}_pans_credit.max_freq,{month}_pans_credit.max_len,{month}_pans_credit.credit_amount,{month}_pans_credit.cash_dep_ratio,{month}_pans_credit.cash_wid_ratio,{month}_pans_credit.credit_amount_in_cr FROM monthly_{reporttype}.{month}_pans_credit), cte2 AS ( \tSELECT cte1.pan, cte1.max_freq,            cte1.max_len,            cte1.credit_amount,            cte1.cash_dep_ratio,            cte1.cash_wid_ratio,            cte1.credit_amount_in_cr FROM cte1          WHERE (cte1.pan IN ( SELECT {month}_excluded_pans_{reporttype}.pan                   FROM monthly_{reporttype}.{month}_excluded_pans_{reporttype}))     ), cte3 AS ( SELECT {month}_repeated_pans_credit.pan FROM monthly_{reporttype}.{month}_repeated_pans_credit) SELECT cte2.pan,    cte2.max_freq,    cte2.max_len,    cte2.credit_amount,    cte2.cash_dep_ratio,    cte2.cash_wid_ratio   FROM cte2 WHERE NOT (cte2.pan IN ( SELECT cte3.pan FROM cte3)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_credit_excluded_pans_without_repeated_pan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Debit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Materialized Views for Debit:\\n')\n",
    "accounts_ntr_debit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_accounts_debit TABLESPACE pg_default AS SELECT {month}_{reporttype}_holder_totals_pan.facc,{month}_{reporttype}_holder_totals_pan.fiureid, {month}_{reporttype}_holder_totals_pan.accountnum,{month}_{reporttype}_holder_totals_pan.holdername,{month}_{reporttype}_holder_totals_pan.debit_amount,{month}_{reporttype}_holder_totals_pan.debit_amount / 10000000::numeric::double precision AS debit_amount_in_cr,{month}_{reporttype}_holder_totals_pan.pan FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan  ORDER BY {month}_{reporttype}_holder_totals_pan.debit_amount DESC LIMIT 5000 WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_accounts_debit created\")\n",
    "\n",
    "pan_debit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_pans_debit TABLESPACE pg_default AS WITH cte1 AS (SELECT {month}_{reporttype}_holder_totals_pan.pan,{month}_{reporttype}_holder_totals_pan.holdername,count(1) AS cnt FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan GROUP BY {month}_{reporttype}_holder_totals_pan.holdername, {month}_{reporttype}_holder_totals_pan.pan ), cte2 AS ( SELECT cte1.pan, cte1.holdername, row_number() OVER (PARTITION BY cte1.pan ORDER BY cte1.cnt DESC) AS rnk FROM cte1 ), cte3 AS ( SELECT {month}_{reporttype}_holder_totals_pan.pan, row_number() OVER (PARTITION BY {month}_{reporttype}_holder_totals_pan.pan ORDER BY (length({month}_{reporttype}_holder_totals_pan.holdername)) DESC) AS rr, {month}_{reporttype}_holder_totals_pan.holdername FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan ) SELECT a.pan, c.holdername AS max_freq, b.holdername AS max_len, sum(a.debit_amount) AS debit_amount, CASE WHEN sum(a.cumulativecrturnover) <> 0::numeric THEN (sum(a.cumulativecashdpstturnover) / sum(a.cumulativecrturnover) * 100::numeric) WHEN sum(a.cumulativecrturnover) = 0::numeric THEN '-1'::integer::numeric ELSE NULL::numeric END AS cash_dep_ratio, CASE WHEN sum(a.cumulativedrturnover) <> 0::numeric THEN (sum(a.cumulativecashwdturnover) / sum(a.cumulativedrturnover) * 100::numeric) WHEN sum(a.cumulativedrturnover) = 0::numeric THEN '-1'::integer::numeric ELSE NULL::numeric END AS cash_wid_ratio, sum(a.debit_amount) / 10000000::numeric::double precision AS debit_amount_in_cr   FROM monthly_{reporttype}.{month}_{reporttype}_holder_totals_pan a JOIN ( SELECT cte2.pan, cte2.holdername, cte2.rnk FROM cte2 WHERE cte2.rnk = 1) c ON c.pan = a.pan JOIN ( SELECT cte3.pan, cte3.holdername, cte3.rr FROM cte3 WHERE cte3.rr = 1) b ON b.pan = c.pan  GROUP BY a.pan, c.holdername, b.holdername  ORDER BY (sum(a.debit_amount)) DESC WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_pans_debit\")\n",
    "\n",
    "#print('In the repeated pan code, change the pan_month_of_report.month to the last month analysis')\n",
    "repeated_pans_debit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_repeated_pans_debit TABLESPACE pg_default AS  WITH cte1 AS ( SELECT DISTINCT pan_month_of_report_{month}.pan,pan_month_of_report_{month}.report_type,count(DISTINCT pan_month_of_report_{month}.month) AS month FROM monthly_report.pan_month_of_report_{month} WHERE pan_month_of_report_{month}.report_type = 'debit_{reporttype}'::text   GROUP BY pan_month_of_report_{month}.pan, pan_month_of_report_{month}.report_type HAVING count(DISTINCT pan_month_of_report_{month}.month) = {pan_month_of_report} ORDER BY (count(DISTINCT pan_month_of_report_{month}.month)) DESC), cte2 AS (SELECT {month}_pans_debit.pan,{month}_pans_debit.max_freq,{month}_pans_debit.max_len,{month}_pans_debit.debit_amount,{month}_pans_debit.cash_dep_ratio,{month}_pans_debit.cash_wid_ratio  FROM monthly_{reporttype}.{month}_pans_debit) SELECT cte2.pan,cte2.max_freq,cte2.max_len,cte2.debit_amount,cte2.cash_dep_ratio,cte2.cash_wid_ratio FROM cte2 WHERE (cte2.pan IN ( SELECT DISTINCT cte1.pan FROM cte1)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_repeated_pans_debit\")\n",
    "\n",
    "debit_main_without_repeated_excluded_pan = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_debit_main_without_repeated_excluded_pan TABLESPACE pg_default AS WITH cte1 AS (SELECT {month}_excluded_pans_{reporttype}.pan, {month}_excluded_pans_{reporttype}.holdername FROM monthly_{reporttype}.{month}_excluded_pans_{reporttype}), cte2 AS (SELECT {month}_repeated_pans_debit.pan,{month}_repeated_pans_debit.max_freq FROM monthly_{reporttype}.{month}_repeated_pans_debit), cte3 AS (SELECT a.pan FROM cte1 a UNION SELECT b.pan FROM cte2 b) SELECT {month}_pans_debit.pan,{month}_pans_debit.max_freq,{month}_pans_debit.max_len,{month}_pans_debit.debit_amount,{month}_pans_debit.cash_dep_ratio,{month}_pans_debit.cash_wid_ratio FROM monthly_{reporttype}.{month}_pans_debit WHERE NOT ({month}_pans_debit.pan IN ( SELECT cte3.pan FROM cte3)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_debit_main_without_repeated_excluded_pan\")\n",
    "\n",
    "debit_excluded_pans_without_repeated_pan = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_debit_excluded_pans_without_repeated_pan TABLESPACE pg_default AS WITH cte1 AS ( SELECT {month}_pans_debit.pan,{month}_pans_debit.max_freq,{month}_pans_debit.max_len,{month}_pans_debit.debit_amount,{month}_pans_debit.cash_dep_ratio,{month}_pans_debit.cash_wid_ratio,{month}_pans_debit.debit_amount_in_cr FROM monthly_{reporttype}.{month}_pans_debit), cte2 AS ( \tSELECT cte1.pan, cte1.max_freq,            cte1.max_len,            cte1.debit_amount,            cte1.cash_dep_ratio,            cte1.cash_wid_ratio,            cte1.debit_amount_in_cr FROM cte1          WHERE (cte1.pan IN ( SELECT {month}_excluded_pans_{reporttype}.pan                   FROM monthly_{reporttype}.{month}_excluded_pans_{reporttype}))     ), cte3 AS ( SELECT {month}_repeated_pans_debit.pan FROM monthly_{reporttype}.{month}_repeated_pans_debit) SELECT cte2.pan,    cte2.max_freq,    cte2.max_len,    cte2.debit_amount,    cte2.cash_dep_ratio,    cte2.cash_wid_ratio   FROM cte2 WHERE NOT (cte2.pan IN ( SELECT cte3.pan FROM cte3)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_debit_excluded_pans_without_repeated_pan\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report type is: str\n",
      "Month of analysis : dec22\n"
     ]
    }
   ],
   "source": [
    "reporttype ='str'\n",
    "print('Report type is:',reporttype)\n",
    "print('Month of analysis :',month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SAL008\\\\Desktop\\\\Ravi\\\\Monthly Report\\\\Dec 2022\\\\Data\\\\SAL_STR_DEC2022'"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = path +'\\\\'+month_report_format+ f'\\Data\\SAL_{reporttype.upper()}_{month[:3].upper()}2022'\n",
    "file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Resetting the Database Connection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Postgres database connected succesfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    pengine = create_engine('postgresql+psycopg2://postgres:sal@123@localhost:5432/postgres')\n",
    "\n",
    "    conn = psycopg2.connect(database = 'postgres', user = 'postgres', password = 'sal@123',host = \"localhost\",port= 5432)\n",
    "    conn.autocommit = True\n",
    "\n",
    "    cursor = conn.cursor()\n",
    "except:\n",
    "    print(\"Unable to create the Postgres DB connection\")\n",
    "\n",
    "db_conn=pengine.connect()\n",
    "print(\"\\n Postgres database connected succesfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #EFT\n",
    "monthly_str_query = \"CREATE SCHEMA IF NOT EXISTS %s AUTHORIZATION %s;\"\n",
    "monthly_str = (AsIs(f'monthly_{reporttype}'),AsIs('postgres'))\n",
    "cursor.execute(monthly_str_query,monthly_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dec22_batch_str'"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{month}_batch_{reporttype}'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for str BATCH file: 2206018303\n",
      "Maximum batch id for str BATCH file: 2206301753\n",
      "BATCH file for str has 7173 rows and 33 columns\n",
      "Batch file inserted into jun22_batch_str with 7173 rows\n"
     ]
    }
   ],
   "source": [
    "batch = pd.read_csv(file_path + '\\BATCH.csv')\n",
    "batch_str = batch[(batch['BATCHID'] >=min_batchid) & (batch['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} BATCH file:',batch_str['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} BATCH file:',batch_str['BATCHID'].max())\n",
    "print(f'BATCH file for {reporttype} has {batch_str.shape[0]} rows and {batch_str.shape[1]} columns')\n",
    "\n",
    "#Loweing the column headers\n",
    "batch_str.columns=[col.lower() for col in batch_str.columns]\n",
    "# Data insertion to Postgres\n",
    "batch_str.to_sql(f'{month}_batch_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"Batch file inserted into {month}_batch_{reporttype} with {batch_str.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for str BATCH file: 2212017446\n",
      "Maximum batch id for str BATCH file: 2212315501\n",
      "BATCH file for str has 10124 rows and 33 columns\n"
     ]
    }
   ],
   "source": [
    "ARFbatch = pd.read_csv(file_path + '\\ARFBATCH.csv')\n",
    "batch_str_arf = ARFbatch[(ARFbatch['BATCHID'] >=min_batchid) & (ARFbatch['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} BATCH file:',batch_str_arf['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} BATCH file:',batch_str_arf['BATCHID'].max())\n",
    "print(f'BATCH file for {reporttype} has {batch_str_arf.shape[0]} rows and {batch_str_arf.shape[1]} columns')\n",
    "\n",
    "#Loweing the column headers\n",
    "batch_str_arf.columns=[col.lower() for col in batch_str_arf.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for str BATCH file: 2212017479\n",
      "Maximum batch id for str BATCH file: 2212315230\n",
      "BATCH file for str has 448 rows and 33 columns\n"
     ]
    }
   ],
   "source": [
    "TRFbatch = pd.read_csv(file_path + '\\TRFBATCH.csv')\n",
    "batch_str_trf = TRFbatch[(TRFbatch['BATCHID'] >=min_batchid) & (TRFbatch['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} BATCH file:',batch_str_trf['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} BATCH file:',batch_str_trf['BATCHID'].max())\n",
    "print(f'BATCH file for {reporttype} has {batch_str_trf.shape[0]} rows and {batch_str_trf.shape[1]} columns')\n",
    "\n",
    "#Loweing the column headers\n",
    "batch_str_trf.columns=[col.lower() for col in batch_str_trf.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_str=pd.concat([batch_str_arf,batch_str_trf])\n",
    "# print(f'Minimum batch id for {reporttype} BATCH file:',batch_str['BATCHID'].min())\n",
    "# print(f'Maximum batch id for {reporttype} BATCH file:',batch_str['BATCHID'].max())\n",
    "# print(f'BATCH file for {reporttype} has {batch_str.shape[0]} rows and {batch_str.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch file inserted into dec22_batch_str with 10572 rows\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Data insertion to Postgres\n",
    "batch_str.to_sql(f'{month}_batch_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"Batch file inserted into {month}_batch_{reporttype} with {batch_str.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for str report file: 2212017446\n",
      "Maximum batch id for str report file: 2212315501\n",
      "REPORT file for dec22 str has 31567 rows and 22 columns\n"
     ]
    }
   ],
   "source": [
    "ARFreport = pd.read_csv(file_path + '\\ARFREPORT.csv')\n",
    "report_str_arf = ARFreport[(ARFreport['BATCHID'] >=min_batchid) & (ARFreport['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} report file:',report_str_arf['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} report file:',report_str_arf['BATCHID'].max())\n",
    "print(f'REPORT file for {month} {reporttype} has {report_str_arf.shape[0]} rows and {report_str_arf.shape[1]} columns')\n",
    "\n",
    "#Loweing the column headers\n",
    "report_str_arf.columns=[col.lower() for col in report_str_arf.columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for str report file: 2212017479\n",
      "Maximum batch id for str report file: 2212315230\n",
      "REPORT file for dec22 str has 4042 rows and 22 columns\n"
     ]
    }
   ],
   "source": [
    "TRFreport = pd.read_csv(file_path + '\\TRFREPORT.csv')\n",
    "report_str_trf = TRFreport[(TRFreport['BATCHID'] >=min_batchid) & (TRFreport['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} report file:',report_str_trf['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} report file:',report_str_trf['BATCHID'].max())\n",
    "print(f'REPORT file for {month} {reporttype} has {report_str_trf.shape[0]} rows and {report_str_trf.shape[1]} columns')\n",
    "\n",
    "#Loweing the column headers\n",
    "report_str_trf.columns=[col.lower() for col in report_str.columns] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "report_str=pd.concat([report_str_arf,report_str_trf])\n",
    "# print(f'Minimum batch id for {reporttype} report file:',report_str['BATCHID'].min())\n",
    "# print(f'Maximum batch id for {reporttype} report file:',report_str['BATCHID'].max())\n",
    "# print(f'REPORT file for {month} {reporttype} has {report_str.shape[0]} rows and {report_str.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Report file inserted into dec22_report_str with 35609 rows\n"
     ]
    }
   ],
   "source": [
    "# Data insertion to Postgres\n",
    "report_str.to_sql(f'{month}_report_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"\\n Report file inserted into {month}_report_{reporttype} with {report_str.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for str report file: 2206018303\n",
      "Maximum batch id for str report file: 2206301753\n",
      "REPORT file for jun22 str has 30048 rows and 22 columns\n",
      "\n",
      " Report file inserted into jun22_report_str with 30048 rows\n"
     ]
    }
   ],
   "source": [
    "report = pd.read_csv(file_path + '\\REPORT.csv')\n",
    "report_str = report[(report['BATCHID'] >=min_batchid) & (report['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} report file:',report_str['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} report file:',report_str['BATCHID'].max())\n",
    "print(f'REPORT file for {month} {reporttype} has {report_str.shape[0]} rows and {report_str.shape[1]} columns')\n",
    "\n",
    "#Loweing the column headers\n",
    "report_str.columns=[col.lower() for col in report_str.columns]\n",
    "# Data insertion to Postgres\n",
    "report_str.to_sql(f'{month}_report_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"\\n Report file inserted into {month}_report_{reporttype} with {report_str.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ARFBRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for str arfbrc file: 2206018303\n",
      "Maximum batch id for str arfbrc file: 2206301753\n",
      "arfBRC file for jun22 str has 40292 rows and 23 columns\n",
      "\n",
      " Batch file inserted into jun22_arfbrc_str with 40292 rows\n"
     ]
    }
   ],
   "source": [
    "arfbrc = pd.read_csv(file_path + '\\ARFBRC.csv')\n",
    "arfbrc_str = arfbrc[(arfbrc['BATCHID'] >=min_batchid) & (arfbrc['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} arfbrc file:',arfbrc_str['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} arfbrc file:',arfbrc_str['BATCHID'].max())\n",
    "print(f'arfBRC file for {month} {reporttype} has {arfbrc_str.shape[0]} rows and {arfbrc_str.shape[1]} columns')\n",
    "\n",
    "#Loweing the column headers\n",
    "arfbrc_str.columns=[col.lower() for col in arfbrc_str.columns]\n",
    "# Data insertion to Postgres\n",
    "arfbrc_str.to_sql(f'{month}_arfbrc_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"\\n Batch file inserted into {month}_arfbrc_{reporttype} with {arfbrc_str.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARFLPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for str arflpe file: 2206018303\n",
      "Maximum batch id for str arflpe file: 2206301741\n",
      "arfLPE file for str has 11389 rows and 36 columns\n",
      "arfLPE file inserted into jun22_arflpe_str with 11389 rows\n"
     ]
    }
   ],
   "source": [
    "arflpe = pd.read_csv(file_path + '\\ARFLPE.csv')\n",
    "arflpe_str = arflpe[(arflpe['BATCHID'] >=min_batchid) & (arflpe['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} arflpe file:',arflpe_str['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} arflpe file:',arflpe_str['BATCHID'].max())\n",
    "print(f'arfLPE file for {reporttype} has {arflpe_str.shape[0]} rows and {arflpe_str.shape[1]} columns')\n",
    "\n",
    "#Lower the Columns\n",
    "arflpe_str.columns = [col.lower() for col in arflpe_str.columns]\n",
    "#Insert data to postgres\n",
    "arflpe_str.to_sql(f'{month}_arflpe_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"arfLPE file inserted into {month}_arflpe_{reporttype} with {arflpe_str.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARFINP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for str arfinp file: 2206018303\n",
      "Maximum batch id for str arfinp file: 2206301753\n",
      "arfinp file for str has 46207 rows and 40 columns\n",
      "arfinp file inserted into jun22_arfinp_str with 46207 rows\n"
     ]
    }
   ],
   "source": [
    "arfinp = pd.read_csv(file_path + '\\ARFINP.csv')\n",
    "arfinp_str = arfinp[(arfinp['BATCHID'] >=min_batchid) & (arfinp['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} arfinp file:',arfinp_str['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} arfinp file:',arfinp_str['BATCHID'].max())\n",
    "print(f'arfinp file for {reporttype} has {arfinp_str.shape[0]} rows and {arfinp_str.shape[1]} columns')\n",
    "\n",
    "#Lower the Columns\n",
    "arfinp_str.columns = [col.lower() for col in arfinp_str.columns]\n",
    "#Insert data to postgres\n",
    "arfinp_str.to_sql(f'{month}_arfinp_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"arfinp file inserted into {month}_arfinp_{reporttype} with {arfinp_str.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Insert data into TRNINP_LPE table under monthly_str schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f'CREATE TABLE IF NOT EXISTS monthly_{reporttype}.{month}_arfinp_lpe_{reporttype}\\\n",
    "                    ( batchid bigint, rptsernum bigint, \\\n",
    "                    personname text COLLATE pg_catalog.\"default\", pan text COLLATE pg_catalog.\"default\" )  \\\n",
    "                    TABLESPACE pg_default; ALTER TABLE IF EXISTS monthly_{reporttype}.{month}_trfinp_lpe_{reporttype}    \\\n",
    "                    OWNER to postgres;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arfinp file inserted into monthly_str.jun22_arfinp_lpe_str \n"
     ]
    }
   ],
   "source": [
    "cursor.execute(f'insert into monthly_{reporttype}.{month}_arfinp_lpe_{reporttype} \\\n",
    "                    select distinct batchid, rptsernum ,  personname, pan \\\n",
    "                    from monthly_{reporttype}.{month}_arfinp_{reporttype} \\\n",
    "                    union \\\n",
    "                    select distinct batchid, rptsernum ,  personname, pan \\\n",
    "                    from monthly_{reporttype}.{month}_arflpe_{reporttype}')\n",
    "\n",
    "print(f\"arfinp file inserted into monthly_{reporttype}.{month}_arfinp_lpe_{reporttype} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ARFACC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for str arfacc file: 2206018303\n",
      "Maximum batch id for str arfacc file: 2206301753\n",
      "arfacc file for str has 40291 rows and 21 columns\n",
      "arfacc file inserted into jun22_arfacc_str with 40291 rows\n"
     ]
    }
   ],
   "source": [
    "arfacc = pd.read_csv(file_path + '\\ARFACC.csv')\n",
    "arfacc_str = arfacc[(arfacc['BATCHID'] >=min_batchid) & (arfacc['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} arfacc file:',arfacc_str['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} arfacc file:',arfacc_str['BATCHID'].max())\n",
    "print(f'arfacc file for {reporttype} has {arfacc_str.shape[0]} rows and {arfacc_str.shape[1]} columns')\n",
    "\n",
    "#Lower the Columns\n",
    "arfacc_str.columns = [col.lower() for col in arfacc_str.columns]\n",
    "#Insert data to postgres\n",
    "arfacc_str.to_sql(f'{month}_arfacc_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"arfacc file inserted into {month}_arfacc_{reporttype} with {arfacc_str.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRFBRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for str arfbrc file: 2206018333\n",
      "Maximum batch id for str arfbrc file: 2206301726\n",
      "TRFBRC file for jun22 str has 16165 rows and 23 columns\n",
      "TRFBRC file inserted into jun22_trfbrc_str_str with 16165 rows\n"
     ]
    }
   ],
   "source": [
    "trfbrc = pd.read_csv(file_path + '\\TRFBRC.csv')\n",
    "trfbrc_str = trfbrc[(trfbrc['BATCHID'] >=min_batchid) & (trfbrc['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} arfbrc file:',trfbrc_str['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} arfbrc file:',trfbrc_str['BATCHID'].max())\n",
    "print(f'TRFBRC file for {month} {reporttype} has {trfbrc_str.shape[0]} rows and {trfbrc_str.shape[1]} columns')\n",
    "\n",
    "#Lower the Columns\n",
    "trfbrc_str.columns = [col.lower() for col in trfbrc_str.columns]\n",
    "#Insert data to postgres\n",
    "trfbrc_str.to_sql(f'{month}_trfbrc_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"TRFBRC file inserted into {month}_trfbrc_str_{reporttype} with {trfbrc_str.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRFTRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for str trflpe file: 2206018333\n",
      "Maximum batch id for str trflpe file: 2206301726\n",
      "TRFTRN file for str has 96276 rows and 55 columns\n",
      "TRFTRN file inserted into jun22_trftrn_str_str with 96276 rows\n"
     ]
    }
   ],
   "source": [
    "trftrn = pd.read_csv(file_path + '\\TRFTRN.csv')\n",
    "trftrn_str = trftrn[(trftrn['BATCHID'] >=min_batchid) & (trftrn['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} trflpe file:',trftrn_str['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} trflpe file:',trftrn_str['BATCHID'].max())\n",
    "print(f'TRFTRN file for {reporttype} has {trftrn_str.shape[0]} rows and {trftrn_str.shape[1]} columns')\n",
    "\n",
    "#Lower the Columns\n",
    "trftrn_str.columns = [col.lower() for col in trftrn_str.columns]\n",
    "#Insert data to postgres\n",
    "trftrn_str.to_sql(f'{month}_trftrn_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"TRFTRN file inserted into {month}_trftrn_str_{reporttype} with {trftrn_str.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysis month - sep22\n",
      "Analysis month in Camel Case - Sep22\n",
      "Analysis month in reporting format - Sep 2022\n"
     ]
    }
   ],
   "source": [
    "month = 'sep22'\n",
    "print('Analysis month -',month)\n",
    "month_cc = Camel_Case(month)\n",
    "print('Analysis month in Camel Case -',month_cc)\n",
    "#month_report_format = format(month_cc[0:3] +' '+str(datetime.now().year))\n",
    "month_report_format = format(month_cc[0:3] +' '+'20'+month[3:5])\n",
    "print('Analysis month in reporting format -',month_report_format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report type is: eft\n",
      "Month of analysis : sep22\n"
     ]
    }
   ],
   "source": [
    "reporttype ='eft'\n",
    "print('Report type is:',reporttype)\n",
    "print('Month of analysis :',month)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start BATCHID: 2209010000\n",
      "End BATCHID: 2209309999\n"
     ]
    }
   ],
   "source": [
    "min_batchid = 2209010000\n",
    "print('Start BATCHID:',min_batchid)\n",
    "\n",
    "max_batchid = 2209309999\n",
    "print('End BATCHID:',max_batchid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\SAL008\\\\Desktop\\\\Ravi\\\\Monthly Report\\\\Sep 2022\\\\Data\\\\SAL_EFT_SEP2022'"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = path +'\\\\'+month_report_format+ f'\\Data\\SAL_{reporttype.upper()}_{month[:3].upper()}2022'\n",
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pengine = create_engine('postgresql+psycopg2://postgres:sal@123@localhost:5432/postgres')\n",
    "\n",
    "conn = psycopg2.connect(database = 'postgres', user = 'postgres', password = 'sal@123',host = \"localhost\",port= 5432)\n",
    "conn.autocommit = True\n",
    "cursor = conn.cursor()\n",
    "db_conn=pengine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #EFT\n",
    "monthly_eft_query = \"CREATE SCHEMA IF NOT EXISTS %s AUTHORIZATION %s;\"\n",
    "monthly_eft = (AsIs(f'monthly_{reporttype}'),AsIs('postgres'))\n",
    "cursor.execute(monthly_eft_query,monthly_eft)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for eft BATCH file: 2209036210\n",
      "Maximum batch id for eft BATCH file: 2209292717\n",
      "BATCH file for eft has 1522 rows and 33 columns\n"
     ]
    }
   ],
   "source": [
    "batch = pd.read_csv(file_path + '\\BATCH.csv')\n",
    "batch_eft = batch[(batch['BATCHID'] >=min_batchid) & (batch['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} BATCH file:',batch_eft['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} BATCH file:',batch_eft['BATCHID'].max())\n",
    "print(f'BATCH file for {reporttype} has {batch_eft.shape[0]} rows and {batch_eft.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for eft report file: 2209036210\n",
      "Maximum batch id for eft report file: 2209292717\n",
      "REPORT file for sep22 eft has 1203400 rows and 22 columns\n"
     ]
    }
   ],
   "source": [
    "report = pd.read_csv(file_path + '\\REPORT.csv')\n",
    "report_eft = report[(report['BATCHID'] >=min_batchid) & (report['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} report file:',report_eft['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} report file:',report_eft['BATCHID'].max())\n",
    "print(f'REPORT file for {month} {reporttype} has {report_eft.shape[0]} rows and {report_eft.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRFBRC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum batch id for eft arfbrc file: 2209036210\n",
      "Maximum batch id for eft arfbrc file: 2209292717\n",
      "TRFBRC file for sep22 eft has 2703773 rows and 23 columns\n"
     ]
    }
   ],
   "source": [
    "trfbrc = pd.read_csv(file_path + '\\TRFBRC.csv')\n",
    "trfbrc_eft = trfbrc[(trfbrc['BATCHID'] >=min_batchid) & (trfbrc['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} arfbrc file:',trfbrc_eft['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} arfbrc file:',trfbrc_eft['BATCHID'].max())\n",
    "print(f'TRFBRC file for {month} {reporttype} has {trfbrc_eft.shape[0]} rows and {trfbrc_eft.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRFTRN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor.execute(f'  CREATE TABLE IF NOT EXISTS monthly_{reporttype}.{month}_trftrn_{reporttype} (     batchid bigint,     rptsernum bigint,     transactiondate text COLLATE pg_catalog.\"default\",     transactiontime text COLLATE pg_catalog.\"default\",     transactionrefnum text COLLATE pg_catalog.\"default\",     transactiontype text COLLATE pg_catalog.\"default\",     instrumenttype text COLLATE pg_catalog.\"default\",     transactioninstname text COLLATE pg_catalog.\"default\",     transactioninstrefnum text COLLATE pg_catalog.\"default\",     transactionstatecode text COLLATE pg_catalog.\"default\",     transactioncountrycode text COLLATE pg_catalog.\"default\",     pymtinstrumentnum text COLLATE pg_catalog.\"default\",     pymtinstrumentissueinstname text COLLATE pg_catalog.\"default\",     instrumentissueinstrefnum text COLLATE pg_catalog.\"default\",     instrumentcountrycode text COLLATE pg_catalog.\"default\",     amountrupees bigint,     amountforeigncurrency bigint,     currencyoftransaction text COLLATE pg_catalog.\"default\",     purposeoftransaction text COLLATE pg_catalog.\"default\",     purposecode text COLLATE pg_catalog.\"default\",     riskrating text COLLATE pg_catalog.\"default\",     customername text COLLATE pg_catalog.\"default\",     customerid text COLLATE pg_catalog.\"default\",     occupation text COLLATE pg_catalog.\"default\",     dob text COLLATE pg_catalog.\"default\",     gender text COLLATE pg_catalog.\"default\",     nationality text COLLATE pg_catalog.\"default\",     identificationtype text COLLATE pg_catalog.\"default\",     identificationnum text COLLATE pg_catalog.\"default\",     issuingauthority text COLLATE pg_catalog.\"default\",     placeofissue text COLLATE pg_catalog.\"default\",     pan text COLLATE pg_catalog.\"default\",     uin text COLLATE pg_catalog.\"default\",     address text COLLATE pg_catalog.\"default\",     city text COLLATE pg_catalog.\"default\",     statecode text COLLATE pg_catalog.\"default\",     pincode text COLLATE pg_catalog.\"default\",     countrycode text COLLATE pg_catalog.\"default\",     telephone text COLLATE pg_catalog.\"default\",     mobile text COLLATE pg_catalog.\"default\",     fax text COLLATE pg_catalog.\"default\",     email text COLLATE pg_catalog.\"default\",     accountnum text COLLATE pg_catalog.\"default\",     accountwithinstname text COLLATE pg_catalog.\"default\",     accountwithinstrefnum text COLLATE pg_catalog.\"default\",     relatedinstname text COLLATE pg_catalog.\"default\",     instrelationflag text COLLATE pg_catalog.\"default\",     relatedinstrefnum text COLLATE pg_catalog.\"default\",     remarks text COLLATE pg_catalog.\"default\",     deletedflag text COLLATE pg_catalog.\"default\",     creationdate text COLLATE pg_catalog.\"default\",     createdby text COLLATE pg_catalog.\"default\",     xmlindexid text COLLATE pg_catalog.\"default\",     reportxmlindexid text COLLATE pg_catalog.\"default\",     batchxmlindexid text COLLATE pg_catalog.\"default\",     country_in bigint,     tytype_in bigint )  TABLESPACE pg_default;  ALTER TABLE IF EXISTS monthly_{reporttype}.{month}_trftrn_{reporttype}     OWNER to postgres;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "trftrn = pd.read_csv(file_path + '\\TRFTRN.csv',chunksize = 10000,iterator = True, error_bad_lines=False,encoding='latin1',sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers:\n",
      "\n",
      "read_csv(filepath_or_buffer: Union[str, pathlib.Path, IO[~AnyStr]], sep=',', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal: str = '.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, dialect=None, error_bad_lines=True, warn_bad_lines=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None)\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "    \n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "    \n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handler (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator by Python's builtin sniffer\n",
      "        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
      "        different from ``'\\s+'`` will be interpreted as regular expressions and\n",
      "        will also force the use of the Python parsing engine. Note that regex\n",
      "        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, default ``None``\n",
      "        Alias for sep.\n",
      "    header : int, list of int, default 'infer'\n",
      "        Row number(s) to use as the column names, and the start of the\n",
      "        data.  Default behavior is to infer the column names: if no names\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a multi-index on the columns\n",
      "        e.g. [0,1,3]. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : array-like, optional\n",
      "        List of column names to use. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : int, str, sequence of int / str, or False, default ``None``\n",
      "      Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
      "      string name or column index. If a sequence of int / str is given, a\n",
      "      MultiIndex is used.\n",
      "    \n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g. when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : list-like or callable, optional\n",
      "        Return a subset of the columns. If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in `names` or\n",
      "        inferred from the document header row(s). For example, a valid list-like\n",
      "        `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a DataFrame from ``data`` with element order preserved use\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
      "        in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to True. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    squeeze : bool, default False\n",
      "        If the parsed data only contains one column then return a Series.\n",
      "    prefix : str, optional\n",
      "        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
      "    mangle_dupe_cols : bool, default True\n",
      "        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
      "        'X'...'X'. Passing in False will cause data to be overwritten if there\n",
      "        are duplicate names in the columns.\n",
      "    dtype : Type name or dict of column -> type, optional\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
      "        'c': 'Int64'}\n",
      "        Use `str` or `object` together with suitable `na_values` settings\n",
      "        to preserve and not interpret dtype.\n",
      "        If converters are specified, they will be applied INSTEAD\n",
      "        of dtype conversion.\n",
      "    engine : {'c', 'python'}, optional\n",
      "        Parser engine to use. The C engine is faster while the python engine is\n",
      "        currently more feature-complete.\n",
      "    converters : dict, optional\n",
      "        Dict of functions for converting values in certain columns. Keys can either\n",
      "        be integers or column labels.\n",
      "    true_values : list, optional\n",
      "        Values to consider as True.\n",
      "    false_values : list, optional\n",
      "        Values to consider as False.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : list-like, int or callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "        at the start of the file.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning True if the row should be skipped and False otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : scalar, str, list-like, or dict, optional\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values.  By default the following values are interpreted as\n",
      "        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "        '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n",
      "        'nan', 'null'.\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default NaN values when parsing the data.\n",
      "        Depending on whether `na_values` is passed in, the behavior is as follows:\n",
      "    \n",
      "        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
      "          is appended to the default NaN values used for parsing.\n",
      "        * If `keep_default_na` is True, and `na_values` are not specified, only\n",
      "          the default NaN values are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are specified, only\n",
      "          the NaN values specified `na_values` are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are not specified, no\n",
      "          strings will be parsed as NaN.\n",
      "    \n",
      "        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
      "        `na_values` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of na_values). In\n",
      "        data without any NAs, passing na_filter=False can improve the performance\n",
      "        of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of NA values placed in non-numeric columns.\n",
      "    skip_blank_lines : bool, default True\n",
      "        If True, skip over blank lines rather than interpreting as NaN values.\n",
      "    parse_dates : bool or list of int or names or list of lists or dict, default False\n",
      "        The behavior is as follows:\n",
      "    \n",
      "        * boolean. If True -> try parsing the index.\n",
      "        * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "          a single date column.\n",
      "        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
      "          result 'foo'\n",
      "    \n",
      "        If a column or index cannot be represented as an array of datetimes,\n",
      "        say because of an unparseable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an object data type. For\n",
      "        non-standard datetime parsing, use ``pd.to_datetime`` after\n",
      "        ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n",
      "        specify ``date_parser`` to be a partially-applied\n",
      "        :func:`pandas.to_datetime` with ``utc=True``. See\n",
      "        :ref:`io.csv.mixed_timezones` for more.\n",
      "    \n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
      "        format of the datetime strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "    keep_date_col : bool, default False\n",
      "        If True and `parse_dates` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : function, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. Pandas will try to call `date_parser` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by `parse_dates` into a single array\n",
      "        and pass that; and 3) call `date_parser` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by `parse_dates`) as\n",
      "        arguments.\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If True, use a cache of unique, converted dates to apply the datetime\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "    \n",
      "        .. versionadded:: 0.25.0\n",
      "    iterator : bool, default False\n",
      "        Return TextFileReader object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    chunksize : int, optional\n",
      "        Return TextFileReader object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and\n",
      "        `filepath_or_buffer` is path-like, then detect compression from the\n",
      "        following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n",
      "        decompression). If using 'zip', the ZIP file must contain only one data\n",
      "        file to be read in. Set to None for no decompression.\n",
      "    thousands : str, optional\n",
      "        Thousands separator.\n",
      "    decimal : str, default '.'\n",
      "        Character to recognize as decimal point (e.g. use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character to break file into lines. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        The character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the delimiter and it will be ignored.\n",
      "    quoting : int or csv.QUOTE_* instance, default 0\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "    doublequote : bool, default ``True``\n",
      "       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive quotechar elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        One-character string used to escape other characters.\n",
      "    comment : str, optional\n",
      "        Indicates remainder of line should not be parsed. If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter `header` but not by\n",
      "        `skiprows`. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
      "        treated as the header.\n",
      "    encoding : str, optional\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "        override values, a ParserWarning will be issued. See csv.Dialect\n",
      "        documentation for more details.\n",
      "    error_bad_lines : bool, default True\n",
      "        Lines with too many fields (e.g. a csv line with too many commas) will by\n",
      "        default cause an exception to be raised, and no DataFrame will be returned.\n",
      "        If False, then these \"bad lines\" will dropped from the DataFrame that is\n",
      "        returned.\n",
      "    warn_bad_lines : bool, default True\n",
      "        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
      "        \"bad line\" will be output.\n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
      "        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to True, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set False, or specify the type with the `dtype` parameter.\n",
      "        Note that the entire file is read into a single DataFrame regardless,\n",
      "        use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
      "        (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for `filepath_or_buffer`, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : str, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are `None` for the ordinary converter,\n",
      "        `high` for the high-precision converter, and `round_trip` for the\n",
      "        round-trip converter.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextParser\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "trftrn = pd.read_csv(file_path + '\\TRFTRN.csv',chunksize = 10000,iterator = True,encoding='latin', engine='python',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Skipping line 1247707: NULL byte detected. This byte cannot be processed in Python's native csv library at the moment, so please pass in engine='c' instead\n"
     ]
    }
   ],
   "source": [
    "trftrn = pd.read_csv(file_path + '\\TRFTRN.csv',encoding='latin', engine='python',error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1247705, 55)"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trftrn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trftrn = pd.read_csv(file_path + '\\TRFTRN.csv',quoting=csv.QUOTE_NONE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sep22_trftrn_eft'"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{month}_trftrn_{reporttype}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "c_list=list(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'batchid'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>batchid</th>\n",
       "      <th>rptsernum</th>\n",
       "      <th>transactiondate</th>\n",
       "      <th>transactiontime</th>\n",
       "      <th>transactionrefnum</th>\n",
       "      <th>transactiontype</th>\n",
       "      <th>instrumenttype</th>\n",
       "      <th>transactioninstname</th>\n",
       "      <th>transactioninstrefnum</th>\n",
       "      <th>transactionstatecode</th>\n",
       "      <th>transactioncountrycode</th>\n",
       "      <th>pymtinstrumentnum</th>\n",
       "      <th>pymtinstrumentissueinstname</th>\n",
       "      <th>instrumentissueinstrefnum</th>\n",
       "      <th>instrumentcountrycode</th>\n",
       "      <th>amountrupees</th>\n",
       "      <th>amountforeigncurrency</th>\n",
       "      <th>currencyoftransaction</th>\n",
       "      <th>purposeoftransaction</th>\n",
       "      <th>purposecode</th>\n",
       "      <th>riskrating</th>\n",
       "      <th>customername</th>\n",
       "      <th>customerid</th>\n",
       "      <th>occupation</th>\n",
       "      <th>dob</th>\n",
       "      <th>gender</th>\n",
       "      <th>nationality</th>\n",
       "      <th>identificationtype</th>\n",
       "      <th>identificationnum</th>\n",
       "      <th>issuingauthority</th>\n",
       "      <th>placeofissue</th>\n",
       "      <th>pan</th>\n",
       "      <th>uin</th>\n",
       "      <th>address</th>\n",
       "      <th>city</th>\n",
       "      <th>statecode</th>\n",
       "      <th>pincode</th>\n",
       "      <th>countrycode</th>\n",
       "      <th>telephone</th>\n",
       "      <th>mobile</th>\n",
       "      <th>fax</th>\n",
       "      <th>email</th>\n",
       "      <th>accountnum</th>\n",
       "      <th>accountwithinstname</th>\n",
       "      <th>accountwithinstrefnum</th>\n",
       "      <th>relatedinstname</th>\n",
       "      <th>instrelationflag</th>\n",
       "      <th>relatedinstrefnum</th>\n",
       "      <th>remarks</th>\n",
       "      <th>deletedflag</th>\n",
       "      <th>creationdate</th>\n",
       "      <th>createdby</th>\n",
       "      <th>xmlindexid</th>\n",
       "      <th>reportxmlindexid</th>\n",
       "      <th>batchxmlindexid</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1230000</th>\n",
       "      <td>2209143926</td>\n",
       "      <td>181210</td>\n",
       "      <td>2022-08-02</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S86055233</td>\n",
       "      <td>R</td>\n",
       "      <td>E</td>\n",
       "      <td>HSBC BANK</td>\n",
       "      <td>HSBC0110007</td>\n",
       "      <td>XX</td>\n",
       "      <td>IN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XX</td>\n",
       "      <td>901888</td>\n",
       "      <td>901888</td>\n",
       "      <td>INR</td>\n",
       "      <td>Compensation of employees</td>\n",
       "      <td>P1401</td>\n",
       "      <td>T3</td>\n",
       "      <td>VIJAYANT KUMAR</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>XX</td>\n",
       "      <td>Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>HSBC0110007 499151140006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>499151140006</td>\n",
       "      <td>HSBC BANK</td>\n",
       "      <td>HSBC0110007</td>\n",
       "      <td>ICICI Bank Ltd</td>\n",
       "      <td>F</td>\n",
       "      <td>ICICINBBNRI</td>\n",
       "      <td>SRS/IMC020822416/C+GST0/INREM/20220802164417</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-SEP-22</td>\n",
       "      <td>FINCOREBACKUP1</td>\n",
       "      <td>1</td>\n",
       "      <td>644</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230001</th>\n",
       "      <td>2209143927</td>\n",
       "      <td>182951</td>\n",
       "      <td>2022-08-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S8440793</td>\n",
       "      <td>R</td>\n",
       "      <td>E</td>\n",
       "      <td>ICICI BANK LTD,F.P.HOUSE NARIMAN POINT MUMBAI</td>\n",
       "      <td>0004</td>\n",
       "      <td>XX</td>\n",
       "      <td>IN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XX</td>\n",
       "      <td>648754</td>\n",
       "      <td>648754</td>\n",
       "      <td>INR</td>\n",
       "      <td>Compensation of employees</td>\n",
       "      <td>P1401</td>\n",
       "      <td>T3</td>\n",
       "      <td>SUNIL KUMAR SINGH</td>\n",
       "      <td>511078396.0</td>\n",
       "      <td>SALARIED - PRIVATE LIMITED</td>\n",
       "      <td>1980-08-11</td>\n",
       "      <td>M</td>\n",
       "      <td>XX</td>\n",
       "      <td>C</td>\n",
       "      <td>EPCPS7580J</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>EPCPS7580J</td>\n",
       "      <td>EPCPS7580J</td>\n",
       "      <td>VILL+POST BIHTA,P.S IMADPUR,DIST,BHOJPUR,ARA,BIHAR,NR ARA MACHINE,,,ARRAH,802209,BIHAR,INDIA</td>\n",
       "      <td>ARRAH</td>\n",
       "      <td>BR</td>\n",
       "      <td>802209.0</td>\n",
       "      <td>IN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>ROHIT.CAPTAIN.SINGH159@GMAIL.COM</td>\n",
       "      <td>041901075027</td>\n",
       "      <td>ICICI BANK LTD,F.P.HOUSE NARIMAN POINT MUMBAI</td>\n",
       "      <td>0004</td>\n",
       "      <td>ICICI BANK UK</td>\n",
       "      <td>F</td>\n",
       "      <td>000405075257</td>\n",
       "      <td>SRS/DUK040820221/C+GST0/INREM/20220808210724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-SEP-22</td>\n",
       "      <td>FINCOREBACKUP1</td>\n",
       "      <td>1</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230002</th>\n",
       "      <td>2209143927</td>\n",
       "      <td>182951</td>\n",
       "      <td>2022-08-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S8440793</td>\n",
       "      <td>P</td>\n",
       "      <td>E</td>\n",
       "      <td>ICICI BANK UK</td>\n",
       "      <td>000405075257</td>\n",
       "      <td>XX</td>\n",
       "      <td>GB</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XX</td>\n",
       "      <td>648754</td>\n",
       "      <td>648754</td>\n",
       "      <td>INR</td>\n",
       "      <td>Compensation of employees</td>\n",
       "      <td>P1401</td>\n",
       "      <td>T3</td>\n",
       "      <td>ATLANTAS SHIP MANAGEMENT LTD</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>XX</td>\n",
       "      <td>Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>PORTLAND HOUSE 69-71 WEMBLEY HILL ROAD, WEMBLEY, MIDDLESEX, ENGLAND, HA9 8BU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ASM</td>\n",
       "      <td>ICICI BANK UK</td>\n",
       "      <td>000405075257</td>\n",
       "      <td>ICICI BANK UK</td>\n",
       "      <td>E</td>\n",
       "      <td>000405075257</td>\n",
       "      <td>SRS/DUK040820221/C+GST0/INREM/20220808210724</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-SEP-22</td>\n",
       "      <td>FINCOREBACKUP1</td>\n",
       "      <td>2</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230003</th>\n",
       "      <td>2209143927</td>\n",
       "      <td>182952</td>\n",
       "      <td>2022-08-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S8441284</td>\n",
       "      <td>R</td>\n",
       "      <td>E</td>\n",
       "      <td>ICICI BANK LTD,F.P.HOUSE NARIMAN POINT MUMBAI</td>\n",
       "      <td>0004</td>\n",
       "      <td>XX</td>\n",
       "      <td>IN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XX</td>\n",
       "      <td>1236426</td>\n",
       "      <td>1236426</td>\n",
       "      <td>INR</td>\n",
       "      <td>Compensation of employees</td>\n",
       "      <td>P1401</td>\n",
       "      <td>T3</td>\n",
       "      <td>NAMBIDASAN BALAKRISHNAN</td>\n",
       "      <td>76225875.0</td>\n",
       "      <td>SALARIED - PRIVATE LIMITED</td>\n",
       "      <td>1964-01-28</td>\n",
       "      <td>M</td>\n",
       "      <td>XX</td>\n",
       "      <td>C</td>\n",
       "      <td>ACXPN7444C</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ACXPN7444C</td>\n",
       "      <td>ACXPN7444C</td>\n",
       "      <td>NO 89,KRISHNA ESTATE,KANATHUR,REDDY KUPPAM,,,,KANCHIPURAM,603112,TAMIL NADU,INDIA</td>\n",
       "      <td>KANCHIPURAM</td>\n",
       "      <td>TN</td>\n",
       "      <td>603112.0</td>\n",
       "      <td>IN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>BEENAADAAS@GMAIL.COM</td>\n",
       "      <td>613401202507</td>\n",
       "      <td>ICICI BANK LTD,F.P.HOUSE NARIMAN POINT MUMBAI</td>\n",
       "      <td>0004</td>\n",
       "      <td>ICICI BANK UK</td>\n",
       "      <td>F</td>\n",
       "      <td>000405075257</td>\n",
       "      <td>SRS/CRM040820222/C+GST0/INREM/20220808210725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-SEP-22</td>\n",
       "      <td>FINCOREBACKUP1</td>\n",
       "      <td>1</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1230004</th>\n",
       "      <td>2209143927</td>\n",
       "      <td>182952</td>\n",
       "      <td>2022-08-08</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S8441284</td>\n",
       "      <td>P</td>\n",
       "      <td>E</td>\n",
       "      <td>ICICI BANK UK</td>\n",
       "      <td>000405075257</td>\n",
       "      <td>XX</td>\n",
       "      <td>GB</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XX</td>\n",
       "      <td>1236426</td>\n",
       "      <td>1236426</td>\n",
       "      <td>INR</td>\n",
       "      <td>Compensation of employees</td>\n",
       "      <td>P1401</td>\n",
       "      <td>T3</td>\n",
       "      <td>ATLANTAS SHIP MANAGEMENT LTD</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>X</td>\n",
       "      <td>XX</td>\n",
       "      <td>Z</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>PORTLAND HOUSE 69-71 WEMBLEY HILL ROAD, WEMBLEY, MIDDLESEX, ENGLAND, HA9 8BU</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XX</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>ASM</td>\n",
       "      <td>ICICI BANK UK</td>\n",
       "      <td>000405075257</td>\n",
       "      <td>ICICI BANK UK</td>\n",
       "      <td>E</td>\n",
       "      <td>000405075257</td>\n",
       "      <td>SRS/CRM040820222/C+GST0/INREM/20220808210725</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-SEP-22</td>\n",
       "      <td>FINCOREBACKUP1</td>\n",
       "      <td>2</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            batchid  rptsernum transactiondate  transactiontime  \\\n",
       "1230000  2209143926     181210      2022-08-02              NaN   \n",
       "1230001  2209143927     182951      2022-08-08              NaN   \n",
       "1230002  2209143927     182951      2022-08-08              NaN   \n",
       "1230003  2209143927     182952      2022-08-08              NaN   \n",
       "1230004  2209143927     182952      2022-08-08              NaN   \n",
       "\n",
       "        transactionrefnum transactiontype instrumenttype  \\\n",
       "1230000         S86055233               R              E   \n",
       "1230001          S8440793               R              E   \n",
       "1230002          S8440793               P              E   \n",
       "1230003          S8441284               R              E   \n",
       "1230004          S8441284               P              E   \n",
       "\n",
       "                                   transactioninstname transactioninstrefnum  \\\n",
       "1230000                                      HSBC BANK           HSBC0110007   \n",
       "1230001  ICICI BANK LTD,F.P.HOUSE NARIMAN POINT MUMBAI                  0004   \n",
       "1230002                                  ICICI BANK UK          000405075257   \n",
       "1230003  ICICI BANK LTD,F.P.HOUSE NARIMAN POINT MUMBAI                  0004   \n",
       "1230004                                  ICICI BANK UK          000405075257   \n",
       "\n",
       "        transactionstatecode transactioncountrycode pymtinstrumentnum  \\\n",
       "1230000                   XX                     IN                     \n",
       "1230001                   XX                     IN                     \n",
       "1230002                   XX                     GB                     \n",
       "1230003                   XX                     IN                     \n",
       "1230004                   XX                     GB                     \n",
       "\n",
       "        pymtinstrumentissueinstname instrumentissueinstrefnum  \\\n",
       "1230000                         NaN                       NaN   \n",
       "1230001                         NaN                       NaN   \n",
       "1230002                         NaN                       NaN   \n",
       "1230003                         NaN                       NaN   \n",
       "1230004                         NaN                       NaN   \n",
       "\n",
       "        instrumentcountrycode  amountrupees  amountforeigncurrency  \\\n",
       "1230000                    XX        901888                 901888   \n",
       "1230001                    XX        648754                 648754   \n",
       "1230002                    XX        648754                 648754   \n",
       "1230003                    XX       1236426                1236426   \n",
       "1230004                    XX       1236426                1236426   \n",
       "\n",
       "        currencyoftransaction       purposeoftransaction purposecode  \\\n",
       "1230000                   INR  Compensation of employees       P1401   \n",
       "1230001                   INR  Compensation of employees       P1401   \n",
       "1230002                   INR  Compensation of employees       P1401   \n",
       "1230003                   INR  Compensation of employees       P1401   \n",
       "1230004                   INR  Compensation of employees       P1401   \n",
       "\n",
       "        riskrating                  customername   customerid  \\\n",
       "1230000         T3                VIJAYANT KUMAR          NaN   \n",
       "1230001         T3             SUNIL KUMAR SINGH  511078396.0   \n",
       "1230002         T3  ATLANTAS SHIP MANAGEMENT LTD          NaN   \n",
       "1230003         T3       NAMBIDASAN BALAKRISHNAN   76225875.0   \n",
       "1230004         T3  ATLANTAS SHIP MANAGEMENT LTD          NaN   \n",
       "\n",
       "                         occupation         dob gender nationality  \\\n",
       "1230000                                     NaN      X          XX   \n",
       "1230001  SALARIED - PRIVATE LIMITED  1980-08-11      M          XX   \n",
       "1230002                                     NaN      X          XX   \n",
       "1230003  SALARIED - PRIVATE LIMITED  1964-01-28      M          XX   \n",
       "1230004                                     NaN      X          XX   \n",
       "\n",
       "        identificationtype identificationnum issuingauthority placeofissue  \\\n",
       "1230000                  Z               NaN                                 \n",
       "1230001                  C        EPCPS7580J                                 \n",
       "1230002                  Z               NaN                                 \n",
       "1230003                  C        ACXPN7444C                                 \n",
       "1230004                  Z               NaN                                 \n",
       "\n",
       "                pan         uin  \\\n",
       "1230000                           \n",
       "1230001  EPCPS7580J  EPCPS7580J   \n",
       "1230002                           \n",
       "1230003  ACXPN7444C  ACXPN7444C   \n",
       "1230004                           \n",
       "\n",
       "                                                                                              address  \\\n",
       "1230000                                                                      HSBC0110007 499151140006   \n",
       "1230001  VILL+POST BIHTA,P.S IMADPUR,DIST,BHOJPUR,ARA,BIHAR,NR ARA MACHINE,,,ARRAH,802209,BIHAR,INDIA   \n",
       "1230002                  PORTLAND HOUSE 69-71 WEMBLEY HILL ROAD, WEMBLEY, MIDDLESEX, ENGLAND, HA9 8BU   \n",
       "1230003             NO 89,KRISHNA ESTATE,KANATHUR,REDDY KUPPAM,,,,KANCHIPURAM,603112,TAMIL NADU,INDIA   \n",
       "1230004                  PORTLAND HOUSE 69-71 WEMBLEY HILL ROAD, WEMBLEY, MIDDLESEX, ENGLAND, HA9 8BU   \n",
       "\n",
       "                city statecode   pincode countrycode  telephone  mobile fax  \\\n",
       "1230000          NaN        XX       NaN          XX        NaN     NaN       \n",
       "1230001        ARRAH        BR  802209.0          IN        NaN     NaN       \n",
       "1230002          NaN        XX       NaN          XX        NaN     NaN       \n",
       "1230003  KANCHIPURAM        TN  603112.0          IN        NaN     NaN       \n",
       "1230004          NaN        XX       NaN          XX        NaN     NaN       \n",
       "\n",
       "                                    email    accountnum  \\\n",
       "1230000                                    499151140006   \n",
       "1230001  ROHIT.CAPTAIN.SINGH159@GMAIL.COM  041901075027   \n",
       "1230002                                             ASM   \n",
       "1230003              BEENAADAAS@GMAIL.COM  613401202507   \n",
       "1230004                                             ASM   \n",
       "\n",
       "                                   accountwithinstname accountwithinstrefnum  \\\n",
       "1230000                                      HSBC BANK           HSBC0110007   \n",
       "1230001  ICICI BANK LTD,F.P.HOUSE NARIMAN POINT MUMBAI                  0004   \n",
       "1230002                                  ICICI BANK UK          000405075257   \n",
       "1230003  ICICI BANK LTD,F.P.HOUSE NARIMAN POINT MUMBAI                  0004   \n",
       "1230004                                  ICICI BANK UK          000405075257   \n",
       "\n",
       "        relatedinstname instrelationflag relatedinstrefnum  \\\n",
       "1230000  ICICI Bank Ltd                F       ICICINBBNRI   \n",
       "1230001   ICICI BANK UK                F      000405075257   \n",
       "1230002   ICICI BANK UK                E      000405075257   \n",
       "1230003   ICICI BANK UK                F      000405075257   \n",
       "1230004   ICICI BANK UK                E      000405075257   \n",
       "\n",
       "                                              remarks  deletedflag  \\\n",
       "1230000  SRS/IMC020822416/C+GST0/INREM/20220802164417          NaN   \n",
       "1230001  SRS/DUK040820221/C+GST0/INREM/20220808210724          NaN   \n",
       "1230002  SRS/DUK040820221/C+GST0/INREM/20220808210724          NaN   \n",
       "1230003  SRS/CRM040820222/C+GST0/INREM/20220808210725          NaN   \n",
       "1230004  SRS/CRM040820222/C+GST0/INREM/20220808210725          NaN   \n",
       "\n",
       "        creationdate       createdby  xmlindexid  reportxmlindexid  \\\n",
       "1230000    16-SEP-22  FINCOREBACKUP1           1               644   \n",
       "1230001    16-SEP-22  FINCOREBACKUP1           1                99   \n",
       "1230002    16-SEP-22  FINCOREBACKUP1           2                99   \n",
       "1230003    16-SEP-22  FINCOREBACKUP1           1               100   \n",
       "1230004    16-SEP-22  FINCOREBACKUP1           2               100   \n",
       "\n",
       "         batchxmlindexid  \n",
       "1230000                1  \n",
       "1230001                1  \n",
       "1230002                1  \n",
       "1230003                1  \n",
       "1230004                1  "
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sep22_trftrn_eft'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f'{month}_trftrn_{reporttype}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function read_csv in module pandas.io.parsers:\n",
      "\n",
      "read_csv(filepath_or_buffer: Union[str, pathlib.Path, IO[~AnyStr]], sep=',', delimiter=None, header='infer', names=None, index_col=None, usecols=None, squeeze=False, prefix=None, mangle_dupe_cols=True, dtype=None, engine=None, converters=None, true_values=None, false_values=None, skipinitialspace=False, skiprows=None, skipfooter=0, nrows=None, na_values=None, keep_default_na=True, na_filter=True, verbose=False, skip_blank_lines=True, parse_dates=False, infer_datetime_format=False, keep_date_col=False, date_parser=None, dayfirst=False, cache_dates=True, iterator=False, chunksize=None, compression='infer', thousands=None, decimal: str = '.', lineterminator=None, quotechar='\"', quoting=0, doublequote=True, escapechar=None, comment=None, encoding=None, dialect=None, error_bad_lines=True, warn_bad_lines=True, delim_whitespace=False, low_memory=True, memory_map=False, float_precision=None)\n",
      "    Read a comma-separated values (csv) file into DataFrame.\n",
      "    \n",
      "    Also supports optionally iterating or breaking of the file\n",
      "    into chunks.\n",
      "    \n",
      "    Additional help can be found in the online docs for\n",
      "    `IO Tools <https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html>`_.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    filepath_or_buffer : str, path object or file-like object\n",
      "        Any valid string path is acceptable. The string could be a URL. Valid\n",
      "        URL schemes include http, ftp, s3, gs, and file. For file URLs, a host is\n",
      "        expected. A local file could be: file://localhost/path/to/table.csv.\n",
      "    \n",
      "        If you want to pass in a path object, pandas accepts any ``os.PathLike``.\n",
      "    \n",
      "        By file-like object, we refer to objects with a ``read()`` method, such as\n",
      "        a file handler (e.g. via builtin ``open`` function) or ``StringIO``.\n",
      "    sep : str, default ','\n",
      "        Delimiter to use. If sep is None, the C engine cannot automatically detect\n",
      "        the separator, but the Python parsing engine can, meaning the latter will\n",
      "        be used and automatically detect the separator by Python's builtin sniffer\n",
      "        tool, ``csv.Sniffer``. In addition, separators longer than 1 character and\n",
      "        different from ``'\\s+'`` will be interpreted as regular expressions and\n",
      "        will also force the use of the Python parsing engine. Note that regex\n",
      "        delimiters are prone to ignoring quoted data. Regex example: ``'\\r\\t'``.\n",
      "    delimiter : str, default ``None``\n",
      "        Alias for sep.\n",
      "    header : int, list of int, default 'infer'\n",
      "        Row number(s) to use as the column names, and the start of the\n",
      "        data.  Default behavior is to infer the column names: if no names\n",
      "        are passed the behavior is identical to ``header=0`` and column\n",
      "        names are inferred from the first line of the file, if column\n",
      "        names are passed explicitly then the behavior is identical to\n",
      "        ``header=None``. Explicitly pass ``header=0`` to be able to\n",
      "        replace existing names. The header can be a list of integers that\n",
      "        specify row locations for a multi-index on the columns\n",
      "        e.g. [0,1,3]. Intervening rows that are not specified will be\n",
      "        skipped (e.g. 2 in this example is skipped). Note that this\n",
      "        parameter ignores commented lines and empty lines if\n",
      "        ``skip_blank_lines=True``, so ``header=0`` denotes the first line of\n",
      "        data rather than the first line of the file.\n",
      "    names : array-like, optional\n",
      "        List of column names to use. If the file contains a header row,\n",
      "        then you should explicitly pass ``header=0`` to override the column names.\n",
      "        Duplicates in this list are not allowed.\n",
      "    index_col : int, str, sequence of int / str, or False, default ``None``\n",
      "      Column(s) to use as the row labels of the ``DataFrame``, either given as\n",
      "      string name or column index. If a sequence of int / str is given, a\n",
      "      MultiIndex is used.\n",
      "    \n",
      "      Note: ``index_col=False`` can be used to force pandas to *not* use the first\n",
      "      column as the index, e.g. when you have a malformed file with delimiters at\n",
      "      the end of each line.\n",
      "    usecols : list-like or callable, optional\n",
      "        Return a subset of the columns. If list-like, all elements must either\n",
      "        be positional (i.e. integer indices into the document columns) or strings\n",
      "        that correspond to column names provided either by the user in `names` or\n",
      "        inferred from the document header row(s). For example, a valid list-like\n",
      "        `usecols` parameter would be ``[0, 1, 2]`` or ``['foo', 'bar', 'baz']``.\n",
      "        Element order is ignored, so ``usecols=[0, 1]`` is the same as ``[1, 0]``.\n",
      "        To instantiate a DataFrame from ``data`` with element order preserved use\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['foo', 'bar']]`` for columns\n",
      "        in ``['foo', 'bar']`` order or\n",
      "        ``pd.read_csv(data, usecols=['foo', 'bar'])[['bar', 'foo']]``\n",
      "        for ``['bar', 'foo']`` order.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the column\n",
      "        names, returning names where the callable function evaluates to True. An\n",
      "        example of a valid callable argument would be ``lambda x: x.upper() in\n",
      "        ['AAA', 'BBB', 'DDD']``. Using this parameter results in much faster\n",
      "        parsing time and lower memory usage.\n",
      "    squeeze : bool, default False\n",
      "        If the parsed data only contains one column then return a Series.\n",
      "    prefix : str, optional\n",
      "        Prefix to add to column numbers when no header, e.g. 'X' for X0, X1, ...\n",
      "    mangle_dupe_cols : bool, default True\n",
      "        Duplicate columns will be specified as 'X', 'X.1', ...'X.N', rather than\n",
      "        'X'...'X'. Passing in False will cause data to be overwritten if there\n",
      "        are duplicate names in the columns.\n",
      "    dtype : Type name or dict of column -> type, optional\n",
      "        Data type for data or columns. E.g. {'a': np.float64, 'b': np.int32,\n",
      "        'c': 'Int64'}\n",
      "        Use `str` or `object` together with suitable `na_values` settings\n",
      "        to preserve and not interpret dtype.\n",
      "        If converters are specified, they will be applied INSTEAD\n",
      "        of dtype conversion.\n",
      "    engine : {'c', 'python'}, optional\n",
      "        Parser engine to use. The C engine is faster while the python engine is\n",
      "        currently more feature-complete.\n",
      "    converters : dict, optional\n",
      "        Dict of functions for converting values in certain columns. Keys can either\n",
      "        be integers or column labels.\n",
      "    true_values : list, optional\n",
      "        Values to consider as True.\n",
      "    false_values : list, optional\n",
      "        Values to consider as False.\n",
      "    skipinitialspace : bool, default False\n",
      "        Skip spaces after delimiter.\n",
      "    skiprows : list-like, int or callable, optional\n",
      "        Line numbers to skip (0-indexed) or number of lines to skip (int)\n",
      "        at the start of the file.\n",
      "    \n",
      "        If callable, the callable function will be evaluated against the row\n",
      "        indices, returning True if the row should be skipped and False otherwise.\n",
      "        An example of a valid callable argument would be ``lambda x: x in [0, 2]``.\n",
      "    skipfooter : int, default 0\n",
      "        Number of lines at bottom of file to skip (Unsupported with engine='c').\n",
      "    nrows : int, optional\n",
      "        Number of rows of file to read. Useful for reading pieces of large files.\n",
      "    na_values : scalar, str, list-like, or dict, optional\n",
      "        Additional strings to recognize as NA/NaN. If dict passed, specific\n",
      "        per-column NA values.  By default the following values are interpreted as\n",
      "        NaN: '', '#N/A', '#N/A N/A', '#NA', '-1.#IND', '-1.#QNAN', '-NaN', '-nan',\n",
      "        '1.#IND', '1.#QNAN', '<NA>', 'N/A', 'NA', 'NULL', 'NaN', 'n/a',\n",
      "        'nan', 'null'.\n",
      "    keep_default_na : bool, default True\n",
      "        Whether or not to include the default NaN values when parsing the data.\n",
      "        Depending on whether `na_values` is passed in, the behavior is as follows:\n",
      "    \n",
      "        * If `keep_default_na` is True, and `na_values` are specified, `na_values`\n",
      "          is appended to the default NaN values used for parsing.\n",
      "        * If `keep_default_na` is True, and `na_values` are not specified, only\n",
      "          the default NaN values are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are specified, only\n",
      "          the NaN values specified `na_values` are used for parsing.\n",
      "        * If `keep_default_na` is False, and `na_values` are not specified, no\n",
      "          strings will be parsed as NaN.\n",
      "    \n",
      "        Note that if `na_filter` is passed in as False, the `keep_default_na` and\n",
      "        `na_values` parameters will be ignored.\n",
      "    na_filter : bool, default True\n",
      "        Detect missing value markers (empty strings and the value of na_values). In\n",
      "        data without any NAs, passing na_filter=False can improve the performance\n",
      "        of reading a large file.\n",
      "    verbose : bool, default False\n",
      "        Indicate number of NA values placed in non-numeric columns.\n",
      "    skip_blank_lines : bool, default True\n",
      "        If True, skip over blank lines rather than interpreting as NaN values.\n",
      "    parse_dates : bool or list of int or names or list of lists or dict, default False\n",
      "        The behavior is as follows:\n",
      "    \n",
      "        * boolean. If True -> try parsing the index.\n",
      "        * list of int or names. e.g. If [1, 2, 3] -> try parsing columns 1, 2, 3\n",
      "          each as a separate date column.\n",
      "        * list of lists. e.g.  If [[1, 3]] -> combine columns 1 and 3 and parse as\n",
      "          a single date column.\n",
      "        * dict, e.g. {'foo' : [1, 3]} -> parse columns 1, 3 as date and call\n",
      "          result 'foo'\n",
      "    \n",
      "        If a column or index cannot be represented as an array of datetimes,\n",
      "        say because of an unparseable value or a mixture of timezones, the column\n",
      "        or index will be returned unaltered as an object data type. For\n",
      "        non-standard datetime parsing, use ``pd.to_datetime`` after\n",
      "        ``pd.read_csv``. To parse an index or column with a mixture of timezones,\n",
      "        specify ``date_parser`` to be a partially-applied\n",
      "        :func:`pandas.to_datetime` with ``utc=True``. See\n",
      "        :ref:`io.csv.mixed_timezones` for more.\n",
      "    \n",
      "        Note: A fast-path exists for iso8601-formatted dates.\n",
      "    infer_datetime_format : bool, default False\n",
      "        If True and `parse_dates` is enabled, pandas will attempt to infer the\n",
      "        format of the datetime strings in the columns, and if it can be inferred,\n",
      "        switch to a faster method of parsing them. In some cases this can increase\n",
      "        the parsing speed by 5-10x.\n",
      "    keep_date_col : bool, default False\n",
      "        If True and `parse_dates` specifies combining multiple columns then\n",
      "        keep the original columns.\n",
      "    date_parser : function, optional\n",
      "        Function to use for converting a sequence of string columns to an array of\n",
      "        datetime instances. The default uses ``dateutil.parser.parser`` to do the\n",
      "        conversion. Pandas will try to call `date_parser` in three different ways,\n",
      "        advancing to the next if an exception occurs: 1) Pass one or more arrays\n",
      "        (as defined by `parse_dates`) as arguments; 2) concatenate (row-wise) the\n",
      "        string values from the columns defined by `parse_dates` into a single array\n",
      "        and pass that; and 3) call `date_parser` once for each row using one or\n",
      "        more strings (corresponding to the columns defined by `parse_dates`) as\n",
      "        arguments.\n",
      "    dayfirst : bool, default False\n",
      "        DD/MM format dates, international and European format.\n",
      "    cache_dates : bool, default True\n",
      "        If True, use a cache of unique, converted dates to apply the datetime\n",
      "        conversion. May produce significant speed-up when parsing duplicate\n",
      "        date strings, especially ones with timezone offsets.\n",
      "    \n",
      "        .. versionadded:: 0.25.0\n",
      "    iterator : bool, default False\n",
      "        Return TextFileReader object for iteration or getting chunks with\n",
      "        ``get_chunk()``.\n",
      "    chunksize : int, optional\n",
      "        Return TextFileReader object for iteration.\n",
      "        See the `IO Tools docs\n",
      "        <https://pandas.pydata.org/pandas-docs/stable/io.html#io-chunking>`_\n",
      "        for more information on ``iterator`` and ``chunksize``.\n",
      "    compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer'\n",
      "        For on-the-fly decompression of on-disk data. If 'infer' and\n",
      "        `filepath_or_buffer` is path-like, then detect compression from the\n",
      "        following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no\n",
      "        decompression). If using 'zip', the ZIP file must contain only one data\n",
      "        file to be read in. Set to None for no decompression.\n",
      "    thousands : str, optional\n",
      "        Thousands separator.\n",
      "    decimal : str, default '.'\n",
      "        Character to recognize as decimal point (e.g. use ',' for European data).\n",
      "    lineterminator : str (length 1), optional\n",
      "        Character to break file into lines. Only valid with C parser.\n",
      "    quotechar : str (length 1), optional\n",
      "        The character used to denote the start and end of a quoted item. Quoted\n",
      "        items can include the delimiter and it will be ignored.\n",
      "    quoting : int or csv.QUOTE_* instance, default 0\n",
      "        Control field quoting behavior per ``csv.QUOTE_*`` constants. Use one of\n",
      "        QUOTE_MINIMAL (0), QUOTE_ALL (1), QUOTE_NONNUMERIC (2) or QUOTE_NONE (3).\n",
      "    doublequote : bool, default ``True``\n",
      "       When quotechar is specified and quoting is not ``QUOTE_NONE``, indicate\n",
      "       whether or not to interpret two consecutive quotechar elements INSIDE a\n",
      "       field as a single ``quotechar`` element.\n",
      "    escapechar : str (length 1), optional\n",
      "        One-character string used to escape other characters.\n",
      "    comment : str, optional\n",
      "        Indicates remainder of line should not be parsed. If found at the beginning\n",
      "        of a line, the line will be ignored altogether. This parameter must be a\n",
      "        single character. Like empty lines (as long as ``skip_blank_lines=True``),\n",
      "        fully commented lines are ignored by the parameter `header` but not by\n",
      "        `skiprows`. For example, if ``comment='#'``, parsing\n",
      "        ``#empty\\na,b,c\\n1,2,3`` with ``header=0`` will result in 'a,b,c' being\n",
      "        treated as the header.\n",
      "    encoding : str, optional\n",
      "        Encoding to use for UTF when reading/writing (ex. 'utf-8'). `List of Python\n",
      "        standard encodings\n",
      "        <https://docs.python.org/3/library/codecs.html#standard-encodings>`_ .\n",
      "    dialect : str or csv.Dialect, optional\n",
      "        If provided, this parameter will override values (default or not) for the\n",
      "        following parameters: `delimiter`, `doublequote`, `escapechar`,\n",
      "        `skipinitialspace`, `quotechar`, and `quoting`. If it is necessary to\n",
      "        override values, a ParserWarning will be issued. See csv.Dialect\n",
      "        documentation for more details.\n",
      "    error_bad_lines : bool, default True\n",
      "        Lines with too many fields (e.g. a csv line with too many commas) will by\n",
      "        default cause an exception to be raised, and no DataFrame will be returned.\n",
      "        If False, then these \"bad lines\" will dropped from the DataFrame that is\n",
      "        returned.\n",
      "    warn_bad_lines : bool, default True\n",
      "        If error_bad_lines is False, and warn_bad_lines is True, a warning for each\n",
      "        \"bad line\" will be output.\n",
      "    delim_whitespace : bool, default False\n",
      "        Specifies whether or not whitespace (e.g. ``' '`` or ``'    '``) will be\n",
      "        used as the sep. Equivalent to setting ``sep='\\s+'``. If this option\n",
      "        is set to True, nothing should be passed in for the ``delimiter``\n",
      "        parameter.\n",
      "    low_memory : bool, default True\n",
      "        Internally process the file in chunks, resulting in lower memory use\n",
      "        while parsing, but possibly mixed type inference.  To ensure no mixed\n",
      "        types either set False, or specify the type with the `dtype` parameter.\n",
      "        Note that the entire file is read into a single DataFrame regardless,\n",
      "        use the `chunksize` or `iterator` parameter to return the data in chunks.\n",
      "        (Only valid with C parser).\n",
      "    memory_map : bool, default False\n",
      "        If a filepath is provided for `filepath_or_buffer`, map the file object\n",
      "        directly onto memory and access the data directly from there. Using this\n",
      "        option can improve performance because there is no longer any I/O overhead.\n",
      "    float_precision : str, optional\n",
      "        Specifies which converter the C engine should use for floating-point\n",
      "        values. The options are `None` for the ordinary converter,\n",
      "        `high` for the high-precision converter, and `round_trip` for the\n",
      "        round-trip converter.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    DataFrame or TextParser\n",
      "        A comma-separated values (csv) file is returned as two-dimensional\n",
      "        data structure with labeled axes.\n",
      "    \n",
      "    See Also\n",
      "    --------\n",
      "    DataFrame.to_csv : Write DataFrame to a comma-separated values (csv) file.\n",
      "    read_csv : Read a comma-separated values (csv) file into DataFrame.\n",
      "    read_fwf : Read a table of fixed-width formatted lines into DataFrame.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> pd.read_csv('data.csv')  # doctest: +SKIP\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pd.read_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['BATCHID', 'RPTSERNUM', 'TRANSACTIONDATE', 'TRANSACTIONTIME',\n",
       "       'TRANSACTIONREFNUM', 'TRANSACTIONTYPE', 'INSTRUMENTTYPE',\n",
       "       'TRANSACTIONINSTNAME', 'TRANSACTIONINSTREFNUM', 'TRANSACTIONSTATECODE',\n",
       "       'TRANSACTIONCOUNTRYCODE', 'PYMTINSTRUMENTNUM',\n",
       "       'PYMTINSTRUMENTISSUEINSTNAME', 'INSTRUMENTISSUEINSTREFNUM',\n",
       "       'INSTRUMENTCOUNTRYCODE', 'AMOUNTRUPEES', 'AMOUNTFOREIGNCURRENCY',\n",
       "       'CURRENCYOFTRANSACTION', 'PURPOSEOFTRANSACTION', 'PURPOSECODE',\n",
       "       'RISKRATING', 'CUSTOMERNAME', 'CUSTOMERID', 'OCCUPATION', 'DOB',\n",
       "       'GENDER', 'NATIONALITY', 'IDENTIFICATIONTYPE', 'IDENTIFICATIONNUM',\n",
       "       'ISSUINGAUTHORITY', 'PLACEOFISSUE', 'PAN', 'UIN', 'ADDRESS', 'CITY',\n",
       "       'STATECODE', 'PINCODE', 'COUNTRYCODE', 'TELEPHONE', 'MOBILE', 'FAX',\n",
       "       'EMAIL', 'ACCOUNTNUM', 'ACCOUNTWITHINSTNAME', 'ACCOUNTWITHINSTREFNUM',\n",
       "       'RELATEDINSTNAME', 'INSTRELATIONFLAG', 'RELATEDINSTREFNUM', 'REMARKS',\n",
       "       'DELETEDFLAG', 'CREATIONDATE', 'CREATEDBY', 'XMLINDEXID',\n",
       "       'REPORTXMLINDEXID', 'BATCHXMLINDEXID'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-208-05de6d020123>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrftrn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{month}_trftrn_{reporttype}'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'monthly_eft'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mif_exists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'append'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   2603\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msql\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2605\u001b[1;33m         sql.to_sql(\n\u001b[0m\u001b[0;32m   2606\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2607\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m    587\u001b[0m         )\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m     pandas_sql.to_sql(\n\u001b[0m\u001b[0;32m    590\u001b[0m         \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   1396\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1398\u001b[1;33m             \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunksize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1399\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSQLAlchemyError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1400\u001b[0m             \u001b[1;31m# GH34431\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36minsert\u001b[1;34m(self, chunksize, method)\u001b[0m\n\u001b[0;32m    828\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    829\u001b[0m                 \u001b[0mchunk_iter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mstart_i\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mend_i\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0marr\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 830\u001b[1;33m                 \u001b[0mexec_insert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    831\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    832\u001b[0m     def _query_iterator(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36m_execute_insert\u001b[1;34m(self, conn, keys, data_iter)\u001b[0m\n\u001b[0;32m    745\u001b[0m         \"\"\"\n\u001b[0;32m    746\u001b[0m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mdict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mrow\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 747\u001b[1;33m         \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    748\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    749\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute_insert_multi\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, object_, *multiparams, **params)\u001b[0m\n\u001b[0;32m   1009\u001b[0m             )\n\u001b[0;32m   1010\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\elements.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[1;34m(self, connection, multiparams, params)\u001b[0m\n\u001b[0;32m    296\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute_on_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msupports_execution\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute_clauseelement\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mObjectNotExecutableError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_clauseelement\u001b[1;34m(self, elem, multiparams, params)\u001b[0m\n\u001b[0;32m   1122\u001b[0m             )\n\u001b[0;32m   1123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1124\u001b[1;33m         ret = self._execute_context(\n\u001b[0m\u001b[0;32m   1125\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1126\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_ctx_cls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_compiled\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1316\u001b[1;33m             self._handle_dbapi_exception(\n\u001b[0m\u001b[0;32m   1317\u001b[0m                 \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[0;32m   1512\u001b[0m                 )\n\u001b[0;32m   1513\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1514\u001b[1;33m                 \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1515\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1516\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;31m# credit to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[0;32m   1254\u001b[0m                             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m                     self.dialect.do_executemany(\n\u001b[0m\u001b[0;32m   1257\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m                     )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\dialects\\postgresql\\psycopg2.py\u001b[0m in \u001b[0;36mdo_executemany\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    896\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo_executemany\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    897\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutemany_mode\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mEXECUTEMANY_DEFAULT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 898\u001b[1;33m             \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecutemany\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    899\u001b[0m             \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\encodings\\utf_8.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(input, errors)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mencode\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf_8_encode\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'strict'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcodecs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutf_8_decode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trftrn.to_sql(f'{month}_trftrn_{reporttype}',pengine,schema='monthly_eft',if_exists='append',index=False)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParserError",
     "evalue": "Error tokenizing data. C error: EOF inside string starting at row 1247706",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mParserError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-212-5f5cc1c757c2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdf_btrn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile_path\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'\\TRFTRN.csv'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'latin'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0merror_bad_lines\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCHID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    457\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 458\u001b[1;33m         \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    459\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    460\u001b[0m         \u001b[0mparser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   2153\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2154\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2155\u001b[1;33m             \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2156\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2157\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.read\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_low_memory\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._read_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._tokenize_rows\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.raise_parser_error\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mParserError\u001b[0m: Error tokenizing data. C error: EOF inside string starting at row 1247706"
     ]
    }
   ],
   "source": [
    "df_btrn=pd.read_csv(file_path + '\\TRFTRN.csv',encoding='latin',error_bad_lines=False,usecols=['BATCHID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2209144691"
      ]
     },
     "execution_count": 211,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(df_btrn['BATCHID'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>BATCHID</th>\n",
       "      <th>RPTSERNUM</th>\n",
       "      <th>TRANSACTIONDATE</th>\n",
       "      <th>TRANSACTIONTIME</th>\n",
       "      <th>TRANSACTIONREFNUM</th>\n",
       "      <th>TRANSACTIONTYPE</th>\n",
       "      <th>INSTRUMENTTYPE</th>\n",
       "      <th>TRANSACTIONINSTNAME</th>\n",
       "      <th>TRANSACTIONINSTREFNUM</th>\n",
       "      <th>TRANSACTIONSTATECODE</th>\n",
       "      <th>TRANSACTIONCOUNTRYCODE</th>\n",
       "      <th>PYMTINSTRUMENTNUM</th>\n",
       "      <th>PYMTINSTRUMENTISSUEINSTNAME</th>\n",
       "      <th>INSTRUMENTISSUEINSTREFNUM</th>\n",
       "      <th>INSTRUMENTCOUNTRYCODE</th>\n",
       "      <th>AMOUNTRUPEES</th>\n",
       "      <th>AMOUNTFOREIGNCURRENCY</th>\n",
       "      <th>CURRENCYOFTRANSACTION</th>\n",
       "      <th>PURPOSEOFTRANSACTION</th>\n",
       "      <th>PURPOSECODE</th>\n",
       "      <th>RISKRATING</th>\n",
       "      <th>CUSTOMERNAME</th>\n",
       "      <th>CUSTOMERID</th>\n",
       "      <th>OCCUPATION</th>\n",
       "      <th>DOB</th>\n",
       "      <th>GENDER</th>\n",
       "      <th>NATIONALITY</th>\n",
       "      <th>IDENTIFICATIONTYPE</th>\n",
       "      <th>IDENTIFICATIONNUM</th>\n",
       "      <th>ISSUINGAUTHORITY</th>\n",
       "      <th>PLACEOFISSUE</th>\n",
       "      <th>PAN</th>\n",
       "      <th>UIN</th>\n",
       "      <th>ADDRESS</th>\n",
       "      <th>CITY</th>\n",
       "      <th>STATECODE</th>\n",
       "      <th>PINCODE</th>\n",
       "      <th>COUNTRYCODE</th>\n",
       "      <th>TELEPHONE</th>\n",
       "      <th>MOBILE</th>\n",
       "      <th>FAX</th>\n",
       "      <th>EMAIL</th>\n",
       "      <th>ACCOUNTNUM</th>\n",
       "      <th>ACCOUNTWITHINSTNAME</th>\n",
       "      <th>ACCOUNTWITHINSTREFNUM</th>\n",
       "      <th>RELATEDINSTNAME</th>\n",
       "      <th>INSTRELATIONFLAG</th>\n",
       "      <th>RELATEDINSTREFNUM</th>\n",
       "      <th>REMARKS</th>\n",
       "      <th>DELETEDFLAG</th>\n",
       "      <th>CREATIONDATE</th>\n",
       "      <th>CREATEDBY</th>\n",
       "      <th>XMLINDEXID</th>\n",
       "      <th>REPORTXMLINDEXID</th>\n",
       "      <th>BATCHXMLINDEXID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2209144484</td>\n",
       "      <td>21001</td>\n",
       "      <td>2022-08-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T54SWSW715328</td>\n",
       "      <td>R</td>\n",
       "      <td>E</td>\n",
       "      <td>JP Morgan Chase Bank NA London</td>\n",
       "      <td>CHASGB2L</td>\n",
       "      <td>XX</td>\n",
       "      <td>GB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GB</td>\n",
       "      <td>32806254</td>\n",
       "      <td>411068</td>\n",
       "      <td>USD</td>\n",
       "      <td>Repatriation of Foreign Portfolio Investment made by overseas Investors in India  in equity shares</td>\n",
       "      <td>S0009</td>\n",
       "      <td>T3</td>\n",
       "      <td>JPM ASIA GROWTH FUND</td>\n",
       "      <td>6480000771</td>\n",
       "      <td>Open-End Investment Funds</td>\n",
       "      <td>2001-07-09</td>\n",
       "      <td>X</td>\n",
       "      <td>XX</td>\n",
       "      <td>C</td>\n",
       "      <td>AABCJ3846M</td>\n",
       "      <td>Income Tax Dept</td>\n",
       "      <td>India</td>\n",
       "      <td>AABCJ3846M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60 VICTORIA EMBANKMENT LONDONEC4Y 0JP GREAT BRITAIN AND NORTHERN IRELAND</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>XX</td>\n",
       "      <td>EC4Y0JP</td>\n",
       "      <td>GB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TDS.ACCOUNT.OPENING@RBS.CO.UK</td>\n",
       "      <td>22072</td>\n",
       "      <td>JP Morgan Chase Bank NA London</td>\n",
       "      <td>CHASGB2L</td>\n",
       "      <td>JPMorgan Chase Bank N.A New York Branch</td>\n",
       "      <td>F</td>\n",
       "      <td>CHASUS33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-SEP-22</td>\n",
       "      <td>FINCOREBACKUP1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2209144484</td>\n",
       "      <td>21001</td>\n",
       "      <td>2022-08-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T54SWSW715328</td>\n",
       "      <td>P</td>\n",
       "      <td>E</td>\n",
       "      <td>JP MORGAN CHASE BANK, N.A.</td>\n",
       "      <td>CHASINBX</td>\n",
       "      <td>MH</td>\n",
       "      <td>IN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IN</td>\n",
       "      <td>32806254</td>\n",
       "      <td>411068</td>\n",
       "      <td>USD</td>\n",
       "      <td>Repatriation of Foreign Portfolio Investment made by overseas Investors in India  in equity shares</td>\n",
       "      <td>S0009</td>\n",
       "      <td>T3</td>\n",
       "      <td>JPM ASIA GROWTH FUND</td>\n",
       "      <td>6480000771</td>\n",
       "      <td>Open-End Investment Funds</td>\n",
       "      <td>2001-07-09</td>\n",
       "      <td>X</td>\n",
       "      <td>XX</td>\n",
       "      <td>C</td>\n",
       "      <td>AABCJ3846M</td>\n",
       "      <td>Income Tax Dept</td>\n",
       "      <td>India</td>\n",
       "      <td>AABCJ3846M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60 VICTORIA EMBANKMENT LONDONEC4Y 0JP GREAT BRITAIN AND NORTHERN IRELAND</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>XX</td>\n",
       "      <td>EC4Y0JP</td>\n",
       "      <td>GB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TDS.ACCOUNT.OPENING@RBS.CO.UK</td>\n",
       "      <td>5385009823</td>\n",
       "      <td>JP MORGAN CHASE BANK, N.A.</td>\n",
       "      <td>CHASINBX</td>\n",
       "      <td>JPMorgan Chase Bank N.A New York Branch</td>\n",
       "      <td>E</td>\n",
       "      <td>CHASUS33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-SEP-22</td>\n",
       "      <td>FINCOREBACKUP1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2209144484</td>\n",
       "      <td>21002</td>\n",
       "      <td>2022-08-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T54SWSW715329</td>\n",
       "      <td>P</td>\n",
       "      <td>E</td>\n",
       "      <td>JP MORGAN CHASE BANK, N.A.</td>\n",
       "      <td>CHASINBX</td>\n",
       "      <td>MH</td>\n",
       "      <td>IN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IN</td>\n",
       "      <td>12378450</td>\n",
       "      <td>155104</td>\n",
       "      <td>USD</td>\n",
       "      <td>Repatriation of Foreign Portfolio Investment made by overseas Investors in India  in equity shares</td>\n",
       "      <td>S0009</td>\n",
       "      <td>T3</td>\n",
       "      <td>JPM ASIA GROWTH FUND</td>\n",
       "      <td>6480000771</td>\n",
       "      <td>Open-End Investment Funds</td>\n",
       "      <td>2001-07-09</td>\n",
       "      <td>X</td>\n",
       "      <td>XX</td>\n",
       "      <td>C</td>\n",
       "      <td>AABCJ3846M</td>\n",
       "      <td>Income Tax Dept</td>\n",
       "      <td>India</td>\n",
       "      <td>AABCJ3846M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60 VICTORIA EMBANKMENT LONDONEC4Y 0JP GREAT BRITAIN AND NORTHERN IRELAND</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>XX</td>\n",
       "      <td>EC4Y0JP</td>\n",
       "      <td>GB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TDS.ACCOUNT.OPENING@RBS.CO.UK</td>\n",
       "      <td>5385009823</td>\n",
       "      <td>JP MORGAN CHASE BANK, N.A.</td>\n",
       "      <td>CHASINBX</td>\n",
       "      <td>JPMorgan Chase Bank N.A New York Branch</td>\n",
       "      <td>E</td>\n",
       "      <td>CHASUS33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-SEP-22</td>\n",
       "      <td>FINCOREBACKUP1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2209144484</td>\n",
       "      <td>21002</td>\n",
       "      <td>2022-08-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T54SWSW715329</td>\n",
       "      <td>R</td>\n",
       "      <td>E</td>\n",
       "      <td>JP Morgan Chase Bank NA London</td>\n",
       "      <td>CHASGB2L</td>\n",
       "      <td>XX</td>\n",
       "      <td>GB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>GB</td>\n",
       "      <td>12378450</td>\n",
       "      <td>155104</td>\n",
       "      <td>USD</td>\n",
       "      <td>Repatriation of Foreign Portfolio Investment made by overseas Investors in India  in equity shares</td>\n",
       "      <td>S0009</td>\n",
       "      <td>T3</td>\n",
       "      <td>JPM ASIA GROWTH FUND</td>\n",
       "      <td>6480000771</td>\n",
       "      <td>Open-End Investment Funds</td>\n",
       "      <td>2001-07-09</td>\n",
       "      <td>X</td>\n",
       "      <td>XX</td>\n",
       "      <td>C</td>\n",
       "      <td>AABCJ3846M</td>\n",
       "      <td>Income Tax Dept</td>\n",
       "      <td>India</td>\n",
       "      <td>AABCJ3846M</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60 VICTORIA EMBANKMENT LONDONEC4Y 0JP GREAT BRITAIN AND NORTHERN IRELAND</td>\n",
       "      <td>LONDON</td>\n",
       "      <td>XX</td>\n",
       "      <td>EC4Y0JP</td>\n",
       "      <td>GB</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>TDS.ACCOUNT.OPENING@RBS.CO.UK</td>\n",
       "      <td>22072</td>\n",
       "      <td>JP Morgan Chase Bank NA London</td>\n",
       "      <td>CHASGB2L</td>\n",
       "      <td>JPMorgan Chase Bank N.A New York Branch</td>\n",
       "      <td>F</td>\n",
       "      <td>CHASUS33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-SEP-22</td>\n",
       "      <td>FINCOREBACKUP1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2209144484</td>\n",
       "      <td>21003</td>\n",
       "      <td>2022-08-23</td>\n",
       "      <td>NaN</td>\n",
       "      <td>T54SWSW715330</td>\n",
       "      <td>P</td>\n",
       "      <td>E</td>\n",
       "      <td>JP MORGAN CHASE BANK, N.A.</td>\n",
       "      <td>CHASINBX</td>\n",
       "      <td>MH</td>\n",
       "      <td>IN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IN</td>\n",
       "      <td>1934761</td>\n",
       "      <td>24240</td>\n",
       "      <td>USD</td>\n",
       "      <td>Repatriation of Foreign Portfolio Investment made by overseas Investors in India  in equity shares</td>\n",
       "      <td>S0009</td>\n",
       "      <td>T3</td>\n",
       "      <td>VANGUARD EMERGING MKTS STK INDEX FD A SR OF VANGUARD INTL EQ INDEX FDS</td>\n",
       "      <td>6480003220</td>\n",
       "      <td>Open-End Investment Funds</td>\n",
       "      <td>1998-01-23</td>\n",
       "      <td>X</td>\n",
       "      <td>XX</td>\n",
       "      <td>C</td>\n",
       "      <td>AAATY0918K</td>\n",
       "      <td>Income Tax Dept</td>\n",
       "      <td>India</td>\n",
       "      <td>AAATY0918K</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100 VANGUARD BLVD MALVERN PA 193552331UNITED STATES</td>\n",
       "      <td>Malvern</td>\n",
       "      <td>XX</td>\n",
       "      <td>PA19355</td>\n",
       "      <td>US</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>thomas_j_higgins@vanguard.com</td>\n",
       "      <td>5385202758</td>\n",
       "      <td>JP MORGAN CHASE BANK, N.A.</td>\n",
       "      <td>CHASINBX</td>\n",
       "      <td>JPMorgan Chase Bank N.A New York Branch</td>\n",
       "      <td>E</td>\n",
       "      <td>CHASUS33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16-SEP-22</td>\n",
       "      <td>FINCOREBACKUP1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      BATCHID  RPTSERNUM TRANSACTIONDATE  TRANSACTIONTIME TRANSACTIONREFNUM  \\\n",
       "0  2209144484      21001      2022-08-23              NaN     T54SWSW715328   \n",
       "1  2209144484      21001      2022-08-23              NaN     T54SWSW715328   \n",
       "2  2209144484      21002      2022-08-23              NaN     T54SWSW715329   \n",
       "3  2209144484      21002      2022-08-23              NaN     T54SWSW715329   \n",
       "4  2209144484      21003      2022-08-23              NaN     T54SWSW715330   \n",
       "\n",
       "  TRANSACTIONTYPE INSTRUMENTTYPE             TRANSACTIONINSTNAME  \\\n",
       "0               R              E  JP Morgan Chase Bank NA London   \n",
       "1               P              E      JP MORGAN CHASE BANK, N.A.   \n",
       "2               P              E      JP MORGAN CHASE BANK, N.A.   \n",
       "3               R              E  JP Morgan Chase Bank NA London   \n",
       "4               P              E      JP MORGAN CHASE BANK, N.A.   \n",
       "\n",
       "  TRANSACTIONINSTREFNUM TRANSACTIONSTATECODE TRANSACTIONCOUNTRYCODE  \\\n",
       "0              CHASGB2L                   XX                     GB   \n",
       "1              CHASINBX                   MH                     IN   \n",
       "2              CHASINBX                   MH                     IN   \n",
       "3              CHASGB2L                   XX                     GB   \n",
       "4              CHASINBX                   MH                     IN   \n",
       "\n",
       "  PYMTINSTRUMENTNUM PYMTINSTRUMENTISSUEINSTNAME INSTRUMENTISSUEINSTREFNUM  \\\n",
       "0               NaN                         NaN                       NaN   \n",
       "1               NaN                         NaN                       NaN   \n",
       "2               NaN                         NaN                       NaN   \n",
       "3               NaN                         NaN                       NaN   \n",
       "4               NaN                         NaN                       NaN   \n",
       "\n",
       "  INSTRUMENTCOUNTRYCODE  AMOUNTRUPEES  AMOUNTFOREIGNCURRENCY  \\\n",
       "0                    GB      32806254                 411068   \n",
       "1                    IN      32806254                 411068   \n",
       "2                    IN      12378450                 155104   \n",
       "3                    GB      12378450                 155104   \n",
       "4                    IN       1934761                  24240   \n",
       "\n",
       "  CURRENCYOFTRANSACTION  \\\n",
       "0                   USD   \n",
       "1                   USD   \n",
       "2                   USD   \n",
       "3                   USD   \n",
       "4                   USD   \n",
       "\n",
       "                                                                                 PURPOSEOFTRANSACTION  \\\n",
       "0  Repatriation of Foreign Portfolio Investment made by overseas Investors in India  in equity shares   \n",
       "1  Repatriation of Foreign Portfolio Investment made by overseas Investors in India  in equity shares   \n",
       "2  Repatriation of Foreign Portfolio Investment made by overseas Investors in India  in equity shares   \n",
       "3  Repatriation of Foreign Portfolio Investment made by overseas Investors in India  in equity shares   \n",
       "4  Repatriation of Foreign Portfolio Investment made by overseas Investors in India  in equity shares   \n",
       "\n",
       "  PURPOSECODE RISKRATING  \\\n",
       "0       S0009         T3   \n",
       "1       S0009         T3   \n",
       "2       S0009         T3   \n",
       "3       S0009         T3   \n",
       "4       S0009         T3   \n",
       "\n",
       "                                                             CUSTOMERNAME  \\\n",
       "0                                                    JPM ASIA GROWTH FUND   \n",
       "1                                                    JPM ASIA GROWTH FUND   \n",
       "2                                                    JPM ASIA GROWTH FUND   \n",
       "3                                                    JPM ASIA GROWTH FUND   \n",
       "4  VANGUARD EMERGING MKTS STK INDEX FD A SR OF VANGUARD INTL EQ INDEX FDS   \n",
       "\n",
       "   CUSTOMERID                 OCCUPATION         DOB GENDER NATIONALITY  \\\n",
       "0  6480000771  Open-End Investment Funds  2001-07-09      X          XX   \n",
       "1  6480000771  Open-End Investment Funds  2001-07-09      X          XX   \n",
       "2  6480000771  Open-End Investment Funds  2001-07-09      X          XX   \n",
       "3  6480000771  Open-End Investment Funds  2001-07-09      X          XX   \n",
       "4  6480003220  Open-End Investment Funds  1998-01-23      X          XX   \n",
       "\n",
       "  IDENTIFICATIONTYPE IDENTIFICATIONNUM ISSUINGAUTHORITY PLACEOFISSUE  \\\n",
       "0                  C        AABCJ3846M  Income Tax Dept        India   \n",
       "1                  C        AABCJ3846M  Income Tax Dept        India   \n",
       "2                  C        AABCJ3846M  Income Tax Dept        India   \n",
       "3                  C        AABCJ3846M  Income Tax Dept        India   \n",
       "4                  C        AAATY0918K  Income Tax Dept        India   \n",
       "\n",
       "          PAN  UIN  \\\n",
       "0  AABCJ3846M  NaN   \n",
       "1  AABCJ3846M  NaN   \n",
       "2  AABCJ3846M  NaN   \n",
       "3  AABCJ3846M  NaN   \n",
       "4  AAATY0918K  NaN   \n",
       "\n",
       "                                                                    ADDRESS  \\\n",
       "0  60 VICTORIA EMBANKMENT LONDONEC4Y 0JP GREAT BRITAIN AND NORTHERN IRELAND   \n",
       "1  60 VICTORIA EMBANKMENT LONDONEC4Y 0JP GREAT BRITAIN AND NORTHERN IRELAND   \n",
       "2  60 VICTORIA EMBANKMENT LONDONEC4Y 0JP GREAT BRITAIN AND NORTHERN IRELAND   \n",
       "3  60 VICTORIA EMBANKMENT LONDONEC4Y 0JP GREAT BRITAIN AND NORTHERN IRELAND   \n",
       "4                       100 VANGUARD BLVD MALVERN PA 193552331UNITED STATES   \n",
       "\n",
       "      CITY STATECODE  PINCODE COUNTRYCODE TELEPHONE MOBILE  FAX  \\\n",
       "0   LONDON        XX  EC4Y0JP          GB       NaN    NaN  NaN   \n",
       "1   LONDON        XX  EC4Y0JP          GB       NaN    NaN  NaN   \n",
       "2   LONDON        XX  EC4Y0JP          GB       NaN    NaN  NaN   \n",
       "3   LONDON        XX  EC4Y0JP          GB       NaN    NaN  NaN   \n",
       "4  Malvern        XX  PA19355          US       NaN    NaN  NaN   \n",
       "\n",
       "                           EMAIL  ACCOUNTNUM             ACCOUNTWITHINSTNAME  \\\n",
       "0  TDS.ACCOUNT.OPENING@RBS.CO.UK       22072  JP Morgan Chase Bank NA London   \n",
       "1  TDS.ACCOUNT.OPENING@RBS.CO.UK  5385009823      JP MORGAN CHASE BANK, N.A.   \n",
       "2  TDS.ACCOUNT.OPENING@RBS.CO.UK  5385009823      JP MORGAN CHASE BANK, N.A.   \n",
       "3  TDS.ACCOUNT.OPENING@RBS.CO.UK       22072  JP Morgan Chase Bank NA London   \n",
       "4  thomas_j_higgins@vanguard.com  5385202758      JP MORGAN CHASE BANK, N.A.   \n",
       "\n",
       "  ACCOUNTWITHINSTREFNUM                          RELATEDINSTNAME  \\\n",
       "0              CHASGB2L  JPMorgan Chase Bank N.A New York Branch   \n",
       "1              CHASINBX  JPMorgan Chase Bank N.A New York Branch   \n",
       "2              CHASINBX  JPMorgan Chase Bank N.A New York Branch   \n",
       "3              CHASGB2L  JPMorgan Chase Bank N.A New York Branch   \n",
       "4              CHASINBX  JPMorgan Chase Bank N.A New York Branch   \n",
       "\n",
       "  INSTRELATIONFLAG RELATEDINSTREFNUM REMARKS  DELETEDFLAG CREATIONDATE  \\\n",
       "0                F          CHASUS33     NaN          NaN    16-SEP-22   \n",
       "1                E          CHASUS33     NaN          NaN    16-SEP-22   \n",
       "2                E          CHASUS33     NaN          NaN    16-SEP-22   \n",
       "3                F          CHASUS33     NaN          NaN    16-SEP-22   \n",
       "4                E          CHASUS33     NaN          NaN    16-SEP-22   \n",
       "\n",
       "        CREATEDBY  XMLINDEXID  REPORTXMLINDEXID  BATCHXMLINDEXID  \n",
       "0  FINCOREBACKUP1           1                 1                1  \n",
       "1  FINCOREBACKUP1           2                 1                1  \n",
       "2  FINCOREBACKUP1           1                 2                1  \n",
       "3  FINCOREBACKUP1           2                 2                1  \n",
       "4  FINCOREBACKUP1           1                 3                1  "
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trftrn.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "trftrn.columns=[col.lower() for col in trftrn.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "Error",
     "evalue": "line contains NUL",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mError\u001b[0m                                     Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-203-a1aa533289bd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# If you are using the chunksize concept, use this query to insert the data into PostgresDB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mdf\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrftrn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[1;31m#print(df.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1169\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1170\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1171\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_chunk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1172\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mget_chunk\u001b[1;34m(self, size)\u001b[0m\n\u001b[0;32m   1228\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1229\u001b[0m             \u001b[0msize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnrows\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_currow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1230\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1231\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1232\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, nrows)\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1195\u001b[0m         \u001b[0mnrows\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_validate_integer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"nrows\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1196\u001b[1;33m         \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1198\u001b[0m         \u001b[1;31m# May alter columns / col_dict\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m   2554\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrows\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2555\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2556\u001b[1;33m             \u001b[0mcontent\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_lines\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2557\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2558\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_chunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_get_lines\u001b[1;34m(self, rows)\u001b[0m\n\u001b[0;32m   3295\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mrows\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3296\u001b[0m                         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3297\u001b[1;33m                             \u001b[0mnew_rows\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3298\u001b[0m                         \u001b[0mlines\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_rows\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3299\u001b[0m                     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mError\u001b[0m: line contains NUL"
     ]
    }
   ],
   "source": [
    "# If you are using the chunksize concept, use this query to insert the data into PostgresDB\n",
    "\n",
    "for df in trftrn:\n",
    "    #print(df.shape)\n",
    "    #\n",
    "    #df.columns=c_list\n",
    "    df = df[(df['BATCHID'] >=min_batchid) & (df['BATCHID'] <= max_batchid)]\n",
    "    #print(df.columns)\n",
    "    #df=df[list(df.columns)[0:55]]\n",
    "    #reader=csv.reader(x.replace('\\0','')for x in trftrn )\n",
    "    #print(df.head())\n",
    "    #print(df.columns)\n",
    "    #print(len(df.columns))\n",
    "   \n",
    "    df.columns=[col.lower() for col in df.columns]\n",
    "    df.to_sql(f'{month}_trftrn_{reporttype}',pengine,schema='monthly_eft',if_exists='append',index=False)    \n",
    "    #break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextFileReader' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-35b1ae37d9f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"TRFTRN EFT file inserted into {month}_trftrn_{reporttype} with {trftrn.shape[0]} rows\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextFileReader' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "print(f\"TRFTRN EFT file inserted into {month}_trftrn_{reporttype} with {trftrn.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextFileReader' object has no attribute 'info'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-77-727aa585d8b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrftrn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextFileReader' object has no attribute 'info'"
     ]
    }
   ],
   "source": [
    "trftrn.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trftrn = pd.read_csv(file_path + '\\TRFTRN.csv')\n",
    "# trftrn_eft = trftrn[(trftrn['BATCHID'] >=min_batchid) & (trftrn['BATCHID'] <= max_batchid)]\n",
    "# print(f'Minimum batch id for {reporttype} trflpe file:',trftrn_eft['BATCHID'].min())\n",
    "# print(f'Maximum batch id for {reporttype} trflpe file:',trftrn_eft['BATCHID'].max())\n",
    "# print(f'TRFLPE file for {reporttype} has {trftrn_eft.shape[0]} rows and {trftrn_eft.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Reducing uppercase to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_eft.columns=[col.lower() for col in batch_eft.columns]\n",
    "report_eft.columns=[col.lower() for col in report_eft.columns]\n",
    "trfbrc_eft.columns = [col.lower() for col in trfbrc_eft.columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Insert Data to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "InternalError",
     "evalue": "(psycopg2.errors.DependentObjectsStillExist) cannot drop table monthly_eft.sep22_batch_eft because other objects depend on it\nDETAIL:  materialized view monthly_report.combined_batch_sep22 depends on table monthly_eft.sep22_batch_eft\nmaterialized view monthly_report.batches_without_trn_sep22 depends on table monthly_eft.sep22_batch_eft\nHINT:  Use DROP ... CASCADE to drop the dependent objects too.\n\n[SQL: \nDROP TABLE monthly_eft.sep22_batch_eft]\n(Background on this error at: http://sqlalche.me/e/13/2j85)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDependentObjectsStillExist\u001b[0m                Traceback (most recent call last)",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[0;32m   1275\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1276\u001b[1;33m                     self.dialect.do_execute(\n\u001b[0m\u001b[0;32m   1277\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mDependentObjectsStillExist\u001b[0m: cannot drop table monthly_eft.sep22_batch_eft because other objects depend on it\nDETAIL:  materialized view monthly_report.combined_batch_sep22 depends on table monthly_eft.sep22_batch_eft\nmaterialized view monthly_report.batches_without_trn_sep22 depends on table monthly_eft.sep22_batch_eft\nHINT:  Use DROP ... CASCADE to drop the dependent objects too.\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-41-5f84bdfa1e05>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbatch_eft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{month}_batch_{reporttype}'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'monthly_eft'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mif_exists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Batch file inserted into {month}_batch_{reporttype} with {batch_eft.shape[0]} rows\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mreport_eft\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_sql\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'{month}_report_{reporttype}'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mpengine\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34mf'monthly_{reporttype}'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mif_exists\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'replace'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\" REPORT file inserted into {month}_batch_{reporttype} with {report_eft.shape[0]} rows\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(self, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   2603\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mio\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msql\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2604\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2605\u001b[1;33m         sql.to_sql(\n\u001b[0m\u001b[0;32m   2606\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2607\u001b[0m             \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(frame, name, con, schema, if_exists, index, index_label, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m    587\u001b[0m         )\n\u001b[0;32m    588\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m     pandas_sql.to_sql(\n\u001b[0m\u001b[0;32m    590\u001b[0m         \u001b[0mframe\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m         \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mto_sql\u001b[1;34m(self, frame, name, if_exists, index, index_label, schema, chunksize, dtype, method)\u001b[0m\n\u001b[0;32m   1391\u001b[0m             \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1392\u001b[0m         )\n\u001b[1;32m-> 1393\u001b[1;33m         \u001b[0mtable\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1395\u001b[0m         \u001b[1;32mfrom\u001b[0m \u001b[0msqlalchemy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mexc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    723\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Table '{self.name}' already exists.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    724\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mif_exists\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"replace\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 725\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpd_sql\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    726\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute_create\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    727\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mif_exists\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"append\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\io\\sql.py\u001b[0m in \u001b[0;36mdrop_table\u001b[1;34m(self, table_name, schema)\u001b[0m\n\u001b[0;32m   1452\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhas_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1453\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreflect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0monly\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1454\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1455\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclear\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1456\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\schema.py\u001b[0m in \u001b[0;36mdrop\u001b[1;34m(self, bind, checkfirst)\u001b[0m\n\u001b[0;32m    937\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mbind\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    938\u001b[0m             \u001b[0mbind\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_bind_or_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 939\u001b[1;33m         \u001b[0mbind\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_visitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mddl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSchemaDropper\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheckfirst\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheckfirst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    940\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m     def tometadata(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_run_visitor\u001b[1;34m(self, visitorcallable, element, connection, **kwargs)\u001b[0m\n\u001b[0;32m   2093\u001b[0m     ):\n\u001b[0;32m   2094\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_optional_conn_ctx_manager\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mconn\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2095\u001b[1;33m             \u001b[0mconn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_visitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvisitorcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2096\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2097\u001b[0m     \u001b[1;32mclass\u001b[0m \u001b[0m_trans_ctx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_run_visitor\u001b[1;34m(self, visitorcallable, element, **kwargs)\u001b[0m\n\u001b[0;32m   1654\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1655\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_visitor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvisitorcallable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0melement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1656\u001b[1;33m         \u001b[0mvisitorcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdialect\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraverse_single\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0melement\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1657\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1658\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\visitors.py\u001b[0m in \u001b[0;36mtraverse_single\u001b[1;34m(self, obj, **kw)\u001b[0m\n\u001b[0;32m    143\u001b[0m             \u001b[0mmeth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"visit_%s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__visit_name__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    144\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 145\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    146\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    147\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0miterate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\ddl.py\u001b[0m in \u001b[0;36mvisit_table\u001b[1;34m(self, table, drop_ok, _is_metadata_operation)\u001b[0m\n\u001b[0;32m   1007\u001b[0m         )\n\u001b[0;32m   1008\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1009\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDropTable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1011\u001b[0m         \u001b[1;31m# traverse client side defaults which may refer to server-side\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36mexecute\u001b[1;34m(self, object_, *multiparams, **params)\u001b[0m\n\u001b[0;32m   1009\u001b[0m             )\n\u001b[0;32m   1010\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1011\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mmeth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1012\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1013\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\sql\\ddl.py\u001b[0m in \u001b[0;36m_execute_on_connection\u001b[1;34m(self, connection, multiparams, params)\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_execute_on_connection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mconnection\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_execute_ddl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmultiparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbind\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_ddl\u001b[1;34m(self, ddl, multiparams, params)\u001b[0m\n\u001b[0;32m   1066\u001b[0m             \u001b[1;32melse\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1067\u001b[0m         )\n\u001b[1;32m-> 1068\u001b[1;33m         ret = self._execute_context(\n\u001b[0m\u001b[0;32m   1069\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1070\u001b[0m             \u001b[0mdialect\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecution_ctx_cls\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_init_ddl\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[0;32m   1314\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1315\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1316\u001b[1;33m             self._handle_dbapi_exception(\n\u001b[0m\u001b[0;32m   1317\u001b[0m                 \u001b[0me\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1318\u001b[0m             )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context)\u001b[0m\n\u001b[0;32m   1508\u001b[0m                 \u001b[0mutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnewraise\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1509\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mshould_wrap\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1510\u001b[1;33m                 util.raise_(\n\u001b[0m\u001b[0;32m   1511\u001b[0m                     \u001b[0msqlalchemy_exception\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwith_traceback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mexc_info\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1512\u001b[0m                 )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\util\\compat.py\u001b[0m in \u001b[0;36mraise_\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m    180\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[1;31m# credit to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\base.py\u001b[0m in \u001b[0;36m_execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, *args)\u001b[0m\n\u001b[0;32m   1274\u001b[0m                             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1275\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mevt_handled\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1276\u001b[1;33m                     self.dialect.do_execute(\n\u001b[0m\u001b[0;32m   1277\u001b[0m                         \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m                     )\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sqlalchemy\\engine\\default.py\u001b[0m in \u001b[0;36mdo_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    591\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    592\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo_execute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 593\u001b[1;33m         \u001b[0mcursor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdo_execute_no_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcursor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatement\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: (psycopg2.errors.DependentObjectsStillExist) cannot drop table monthly_eft.sep22_batch_eft because other objects depend on it\nDETAIL:  materialized view monthly_report.combined_batch_sep22 depends on table monthly_eft.sep22_batch_eft\nmaterialized view monthly_report.batches_without_trn_sep22 depends on table monthly_eft.sep22_batch_eft\nHINT:  Use DROP ... CASCADE to drop the dependent objects too.\n\n[SQL: \nDROP TABLE monthly_eft.sep22_batch_eft]\n(Background on this error at: http://sqlalche.me/e/13/2j85)"
     ]
    }
   ],
   "source": [
    "batch_eft.to_sql(f'{month}_batch_{reporttype}',pengine,schema='monthly_eft',if_exists='replace',index=False)\n",
    "print(f\"Batch file inserted into {month}_batch_{reporttype} with {batch_eft.shape[0]} rows\")\n",
    "\n",
    "report_eft.to_sql(f'{month}_report_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\" REPORT file inserted into {month}_batch_{reporttype} with {report_eft.shape[0]} rows\")\n",
    "\n",
    "trfbrc_eft.to_sql(f'{month}_trfbrc_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"TRFBRC file inserted into {month}_batch_{reporttype} with {trfbrc_eft.shape[0]} rows\")\n",
    "\n",
    "# trftrn_eft.to_sql(f'{month}_trflpe_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "# print(f\"TRFLPE file inserted into {month}_batch_{reporttype} with {trftrn_eft.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TRFLPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'path_eft' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-1541f5f5517d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrflpe\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_eft\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'TRFLPE.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mtrflpe_eft\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrflpe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrflpe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCHID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m>=\u001b[0m\u001b[0mmin_batchid\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m&\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrflpe\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCHID'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mmax_batchid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Minimum batch id for {reporttype} trflpe file:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrflpe_eft\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCHID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'Maximum batch id for {reporttype} trflpe file:'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mtrflpe_eft\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'BATCHID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf'TRFLPE file for {reporttype} has {trflpe_eft.shape[0]} rows and {trflpe_eft.shape[1]} columns'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'path_eft' is not defined"
     ]
    }
   ],
   "source": [
    "trflpe = pd.read_csv(path_eft + 'TRFLPE.csv')\n",
    "trflpe_eft = trflpe[(trflpe['BATCHID'] >=min_batchid) & (trflpe['BATCHID'] <= max_batchid)]\n",
    "print(f'Minimum batch id for {reporttype} trflpe file:',trflpe_eft['BATCHID'].min())\n",
    "print(f'Maximum batch id for {reporttype} trflpe file:',trflpe_eft['BATCHID'].max())\n",
    "print(f'TRFLPE file for {reporttype} has {trflpe_eft.shape[0]} rows and {trflpe_eft.shape[1]} columns')\n",
    "\n",
    "trflpe_eft.columns = [col.lower() for col in trflpe.columns]\n",
    "\n",
    "trflpe_eft.to_sql(f'{month}_trflpe_{reporttype}',pengine,schema=f'monthly_{reporttype}',if_exists='replace',index=False)\n",
    "print(f\"TRFLPE file inserted into {month}_batch_{reporttype} with {trflpe_eft.shape[0]} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRFINP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cursor.execute(f'CREATE TABLE IF NOT EXISTS monthly_{reporttype}.{month}_trfinp_{reporttype} (     batchid bigint,     rptsernum bigint,     personname text COLLATE pg_catalog.\"default\",     customerid text,     relationflag text COLLATE pg_catalog.\"default\",     commaddress text COLLATE pg_catalog.\"default\",     commcity text COLLATE pg_catalog.\"default\",     commstatecode text COLLATE pg_catalog.\"default\",     commpincode text,     commcountrycode text COLLATE pg_catalog.\"default\",     secaddress text COLLATE pg_catalog.\"default\",     seccity text COLLATE pg_catalog.\"default\",     secstatecode text COLLATE pg_catalog.\"default\",     secpincode text COLLATE pg_catalog.\"default\",     seccountrycode text COLLATE pg_catalog.\"default\",     telephone text,     mobile text,     fax text COLLATE pg_catalog.\"default\",     email text COLLATE pg_catalog.\"default\",     pan text COLLATE pg_catalog.\"default\",     uin text,     gender text COLLATE pg_catalog.\"default\",     dob text COLLATE pg_catalog.\"default\",     identificationtype text COLLATE pg_catalog.\"default\",     identificationnum text COLLATE pg_catalog.\"default\",     issuingauthority text COLLATE pg_catalog.\"default\",     placeofissue text,     nationality text COLLATE pg_catalog.\"default\",     placeofwork text COLLATE pg_catalog.\"default\",     fatherorspouse text,     occupation text COLLATE pg_catalog.\"default\",     deletedflag text COLLATE pg_catalog.\"default\",     creationdate text COLLATE pg_catalog.\"default\",     createdby text COLLATE pg_catalog.\"default\",     xmlindexid text COLLATE pg_catalog.\"default\",     reportxmlindexid text,     batchxmlindexid text )  TABLESPACE pg_default;  ALTER TABLE IF EXISTS monthly_{reporttype}.{month}_trfinp_{reporttype}     OWNER to postgres;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trfinp_eft = pd.read_csv(file_path + '\\TRFINP.csv',chunksize= 10000,iterator=True)\n",
    "\n",
    "# sh = []\n",
    "# i=0\n",
    "# for df in trfinp_eft:\n",
    "#     df.columns=[col.lower() for col in df.columns]    \n",
    "#     sh.append(df.shape)\n",
    "#     df.to_sql(f'{month}_trfinp_{reporttype}',pengine,schema='monthly_eft',if_exists='append',index=False)\n",
    "#     i = i+1\n",
    "# print(f'Transaction data inserted into monthly_{reporttype} schema, {month}_trfinp_{reporttype} table ')\n",
    "# print((i-1)*10000+sh[-1][0])\n",
    "\n",
    "# print(f'Minimum batch id for {reporttype} trfinp file:',trfinp_eft['BATCHID'].min())\n",
    "# print(f'Maximum batch id for {reporttype} trfinp file:',trfinp_eft['BATCHID'].max())\n",
    "# print(f'TRFINP file for {reporttype} has {trfinp_eft.shape[0]} rows and {trfinp_eft.shape[1]} columns')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create and Insert data into TRNINP_LPE table under monthly_eft schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor.execute(f'CREATE TABLE IF NOT EXISTS monthly_{reporttype}.{month}_trfinp_lpe_{reporttype}\\\n",
    "#                     ( batchid bigint, rptsernum bigint, \\\n",
    "#                     personname text COLLATE pg_catalog.\"default\", pan text COLLATE pg_catalog.\"default\" )  \\\n",
    "#                     TABLESPACE pg_default; ALTER TABLE IF EXISTS monthly_{reporttype}.{month}_trfinp_lpe_{reporttype}    \\\n",
    "#                     OWNER to postgres;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cursor.execute(f'insert into monthly_{reporttype}.{month}_trfinp_lpe_{reporttype} \\\n",
    "#                     select distinct batchid, rptsernum ,  personname, pan \\\n",
    "#                     from monthly_{reporttype}.{month}_trfinp_{reporttype} \\\n",
    "#                     union \\\n",
    "#                     select distinct batchid, rptsernum ,  personname, pan \\\n",
    "#                     from monthly_{reporttype}.{month}_trflpe_{reporttype}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### TRFTRN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Create table with the predefined schema "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "cursor.execute(f'CREATE TABLE IF NOT EXISTS monthly_{reporttype}.{month}_trftrn_{reporttype} (     batchid bigint,     rptsernum bigint,     transactiondate text COLLATE pg_catalog.\"default\",     transactiontime text COLLATE pg_catalog.\"default\",     transactionrefnum text COLLATE pg_catalog.\"default\",     transactiontype text COLLATE pg_catalog.\"default\",     instrumenttype text COLLATE pg_catalog.\"default\",     transactioninstname text COLLATE pg_catalog.\"default\",     transactioninstrefnum text COLLATE pg_catalog.\"default\",     transactionstatecode text COLLATE pg_catalog.\"default\",     transactioncountrycode text COLLATE pg_catalog.\"default\",     pymtinstrumentnum text COLLATE pg_catalog.\"default\",     pymtinstrumentissueinstname text COLLATE pg_catalog.\"default\",     instrumentissueinstrefnum text COLLATE pg_catalog.\"default\",     instrumentcountrycode text COLLATE pg_catalog.\"default\",     amountrupees bigint,     amountforeigncurrency bigint,     currencyoftransaction text COLLATE pg_catalog.\"default\",     purposeoftransaction text COLLATE pg_catalog.\"default\",     purposecode text COLLATE pg_catalog.\"default\",     riskrating text COLLATE pg_catalog.\"default\",     customername text COLLATE pg_catalog.\"default\",     customerid text COLLATE pg_catalog.\"default\",     occupation text COLLATE pg_catalog.\"default\",     dob text COLLATE pg_catalog.\"default\",     gender text COLLATE pg_catalog.\"default\",     nationality text COLLATE pg_catalog.\"default\",     identificationtype text COLLATE pg_catalog.\"default\",     identificationnum text COLLATE pg_catalog.\"default\",     issuingauthority text COLLATE pg_catalog.\"default\",     placeofissue text COLLATE pg_catalog.\"default\",     pan text COLLATE pg_catalog.\"default\",     uin text COLLATE pg_catalog.\"default\",     address text COLLATE pg_catalog.\"default\",     city text COLLATE pg_catalog.\"default\",     statecode text COLLATE pg_catalog.\"default\",     pincode text COLLATE pg_catalog.\"default\",     countrycode text COLLATE pg_catalog.\"default\",     telephone text COLLATE pg_catalog.\"default\",     mobile text COLLATE pg_catalog.\"default\",     fax text COLLATE pg_catalog.\"default\",     email text COLLATE pg_catalog.\"default\",     accountnum text COLLATE pg_catalog.\"default\",     accountwithinstname text COLLATE pg_catalog.\"default\",     accountwithinstrefnum text COLLATE pg_catalog.\"default\",     relatedinstname text COLLATE pg_catalog.\"default\",     instrelationflag text COLLATE pg_catalog.\"default\",     relatedinstrefnum text COLLATE pg_catalog.\"default\",     remarks text COLLATE pg_catalog.\"default\",     deletedflag text COLLATE pg_catalog.\"default\",     creationdate text COLLATE pg_catalog.\"default\",     createdby text COLLATE pg_catalog.\"default\",     xmlindexid text COLLATE pg_catalog.\"default\",     reportxmlindexid text COLLATE pg_catalog.\"default\",     batchxmlindexid text COLLATE pg_catalog.\"default\",     country_in bigint,     tytype_in bigint )  TABLESPACE pg_default;  ALTER TABLE IF EXISTS monthly_{reporttype}.{month}_trftrn_{reporttype}     OWNER to postgres;')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "trftrn_eft = pd.read_csv(file_path + '\\TRFTRN.csv',chunksize= 10000,iterator=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transaction data inserted into monthly_eft schema, sep22_trftrn_eft table \n",
      "7705\n"
     ]
    }
   ],
   "source": [
    "sh = []\n",
    "i=0\n",
    "for df in trftrn_eft:\n",
    "    #print(df.shape)\n",
    "    df.columns=[col.lower() for col in df.columns]\n",
    "    df=df[list(df.columns)[0:55]]\n",
    "    df['country_in'] = 0\n",
    "    df['tytype_in'] = 0\n",
    "    sh.append(df.shape)\n",
    "    df.to_sql(f'{month}_trftrn_{reporttype}',pengine,schema='monthly_eft',if_exists='append',index=False)\n",
    "    i = i+1\n",
    "print(f'Transaction data inserted into monthly_{reporttype} schema, {month}_trftrn_{reporttype} table ')\n",
    "print((i-1)*10000+sh[-1][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "country_in = db_conn.execute(f'update monthly_{reporttype}.{month}_trftrn_{reporttype} \\\n",
    "                    set country_in=1 where  cast(batchid as text)|| cast(rptsernum as text)|| cast(transactionrefnum as text) \\\n",
    "                    in ( select cast(batchid as text)|| cast(rptsernum as text)|| cast(transactionrefnum as text) \\\n",
    "                    from monthly_{reporttype}.{month}_trftrn_{reporttype} \\\n",
    "                    group by cast(batchid as text)|| cast(rptsernum as text)|| cast(transactionrefnum as text) \\\n",
    "                    having count(distinct transactioncountrycode)>1 )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "tytype_in = db_conn.execute(f'update monthly_{reporttype}.{month}_trftrn_{reporttype} \\\n",
    "                    set tytype_in=1 where  cast(batchid as text)|| cast(rptsernum as text)|| cast(transactionrefnum as text)  \\\n",
    "                    in( select cast(batchid as text)|| cast(rptsernum as text)|| cast(transactionrefnum as text) \\\n",
    "                    from monthly_{reporttype}.{month}_trftrn_{reporttype} \\\n",
    "                    group by cast(batchid as text)|| cast(rptsernum as text)|| cast(transactionrefnum as text) \\\n",
    "                    having count(distinct transactiontype)>1 )')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#excluded_pans_path = path + '\\\\'+month_report_format + '\\\\monthly update data\\\\'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "excluded_pans_eft = pd.read_csv(path + '\\\\monthly update data\\\\excluded_pans_{}.csv'.format(reporttype))\n",
    "excluded_pans_eft.to_sql(f'{month}_excluded_pans_{reporttype}',pengine,schema='monthly_eft',if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for resolved PANS\n"
     ]
    }
   ],
   "source": [
    "print('Check for resolved PANS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Country Master data push"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country_master = pd.read_csv(f\"C:\\\\Users\\\\SAL008\\\\Desktop\\\\Ravi\\Monthly Report\\\\Supporting_docs\\\\country_master.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#country_master.to_sql(f'country_master',pengine,schema='monthly_eft',if_exists='replace',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create Materialized Views for EFT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Credit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eft_credit_main = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_credit TABLESPACE pg_default AS  WITH cte1 AS ( SELECT {month}_trftrn_{reporttype}.pan, {month}_trftrn_{reporttype}.customername, count(1) AS cnt  FROM monthly_{reporttype}.{month}_trftrn_{reporttype} GROUP BY {month}_trftrn_{reporttype}.pan, {month}_trftrn_{reporttype}.customername ),  cte2 AS ( SELECT cte1.pan, cte1.customername, row_number() OVER (PARTITION BY cte1.pan  ORDER BY cte1.cnt DESC) AS rr FROM cte1 ), cte3 AS ( SELECT {month}_trftrn_{reporttype}.pan, {month}_trftrn_{reporttype}.batchid, {month}_trftrn_{reporttype}.rptsernum, {month}_trftrn_{reporttype}.transactionrefnum             FROM monthly_{reporttype}.{month}_trftrn_{reporttype}  WHERE {month}_trftrn_{reporttype}.transactioncountrycode = 'IN'::text  AND {month}_trftrn_{reporttype}.transactiontype = 'R'::text   AND {month}_trftrn_{reporttype}.country_in = 1  AND {month}_trftrn_{reporttype}.tytype_in = 1 ),  cte4 AS ( SELECT string_agg(DISTINCT f.country_name, ','::text) AS countries, string_agg(DISTINCT e.purposecode, ','::text) AS pcodes, c3.pan  FROM cte3 c3 \t\t  JOIN ( SELECT {month}_trftrn_{reporttype}.batchid, {month}_trftrn_{reporttype}.rptsernum, {month}_trftrn_{reporttype}.transactiondate, {month}_trftrn_{reporttype}.transactionrefnum, {month}_trftrn_{reporttype}.transactiontype,  \t\t\t\t{month}_trftrn_{reporttype}.instrumenttype, {month}_trftrn_{reporttype}.transactioninstname, {month}_trftrn_{reporttype}.transactioninstrefnum, {month}_trftrn_{reporttype}.transactionstatecode, \t\t\t\t{month}_trftrn_{reporttype}.transactioncountrycode, {month}_trftrn_{reporttype}.amountrupees, {month}_trftrn_{reporttype}.amountforeigncurrency, {month}_trftrn_{reporttype}.currencyoftransaction, \t\t\t\t{month}_trftrn_{reporttype}.purposeoftransaction, {month}_trftrn_{reporttype}.purposecode, {month}_trftrn_{reporttype}.riskrating, {month}_trftrn_{reporttype}.customername, {month}_trftrn_{reporttype}.customerid,   \t\t\t\t{month}_trftrn_{reporttype}.occupation, {month}_trftrn_{reporttype}.pan, {month}_trftrn_{reporttype}.accountnum, {month}_trftrn_{reporttype}.country_in,        \t\t\t\t{month}_trftrn_{reporttype}.tytype_in \t\t\t\tFROM monthly_{reporttype}.{month}_trftrn_{reporttype} \t\t\t\tWHERE {month}_trftrn_{reporttype}.transactioncountrycode <> 'IN'::text) e \t\t  ON c3.batchid = e.batchid AND c3.rptsernum = e.rptsernum AND c3.transactionrefnum = e.transactionrefnum              \t\t  JOIN ( SELECT country_master.code, country_master.country_name \t\t\t\tFROM monthly_report.country_master) f ON f.code = e.transactioncountrycode \t\t  GROUP BY c3.pan ),  cte5 AS ( SELECT trn.pan, sum(trn.amountrupees) AS amount, count(DISTINCT trn.accountnum) AS total_accounts, count(DISTINCT trn.batchid::text || trn.rptsernum::text) AS total_{reporttype}s  FROM monthly_{reporttype}.{month}_trftrn_{reporttype} trn  WHERE trn.transactioncountrycode = 'IN'::text \t\t\t\t   AND trn.country_in = 1 \t\t\t\t   AND trn.tytype_in = 1 \t\t\t\t   AND trn.transactiontype = 'R'::text \t\t\t\t   AND trn.pan IS NOT NULL \t\t\t\t   AND TRIM(BOTH FROM trn.pan) <> ''::text           \t\t\t\t   GROUP BY trn.pan          \t\t\t\t   ORDER BY (sum(trn.amountrupees)) DESC ),  cte6 AS ( SELECT c4.pan, c2.customername, c4.pcodes, c4.countries \t\t  FROM cte4 c4 \t\t  JOIN ( SELECT cte2.pan, cte2.customername, cte2.rr FROM cte2 WHERE cte2.rr = 1) c2 ON c4.pan = c2.pan ) \t\t  SELECT c5.pan, c6.customername, c5.amount, c5.total_{reporttype}s, c5.total_accounts, c6.pcodes, c6.countries   \t\t  FROM cte5 c5 JOIN cte6 c6 ON c5.pan = c6.pan   ORDER BY c5.amount DESC \t\t  WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_credit\")\n",
    "\n",
    "eft_credit_repeated_pans = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_repeated_pans_credit TABLESPACE pg_default AS  WITH cte1 AS (          SELECT DISTINCT pan_month_of_report_{month}.pan,             pan_month_of_report_{month}.report_type,             count(DISTINCT pan_month_of_report_{month}.month) AS month            FROM monthly_report.pan_month_of_report_{month}           WHERE pan_month_of_report_{month}.report_type = 'credit_{reporttype}'::text           GROUP BY pan_month_of_report_{month}.pan, pan_month_of_report_{month}.report_type          HAVING count(DISTINCT pan_month_of_report_{month}.month) = {pan_month_of_report}           ORDER BY (count(DISTINCT pan_month_of_report_{month}.month)) DESC         ), cte2 AS (          SELECT {month}_{reporttype}_credit.pan,             {month}_{reporttype}_credit.customername,             {month}_{reporttype}_credit.amount,             {month}_{reporttype}_credit.total_{reporttype}s,             {month}_{reporttype}_credit.total_accounts,             {month}_{reporttype}_credit.pcodes,             {month}_{reporttype}_credit.countries            FROM monthly_{reporttype}.{month}_{reporttype}_credit                  )  SELECT cte2.pan,     cte2.customername,     cte2.amount,     cte2.total_{reporttype}s,     cte2.total_accounts,     cte2.pcodes,     cte2.countries    FROM cte2   WHERE (cte2.pan IN ( SELECT DISTINCT cte1.pan            FROM cte1)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_repeated_pans_credit\")\n",
    "\n",
    "eft_without_repeated_pans_credit = db_conn.execute(f\" CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_excluded_pans_without_repeated_pans_credit TABLESPACE pg_default AS  WITH cte1 AS (          SELECT {month}_{reporttype}_credit.pan,             {month}_{reporttype}_credit.customername,             {month}_{reporttype}_credit.amount,             {month}_{reporttype}_credit.total_{reporttype}s,             {month}_{reporttype}_credit.total_accounts,             {month}_{reporttype}_credit.pcodes,             {month}_{reporttype}_credit.countries            FROM monthly_{reporttype}.{month}_{reporttype}_credit          LIMIT 1000         ), cte2 AS (          SELECT cte1.pan,             cte1.customername,             cte1.amount,             cte1.total_{reporttype}s,             cte1.total_accounts,             cte1.pcodes,             cte1.countries            FROM cte1           WHERE (cte1.pan IN ( SELECT excluded_pans_{reporttype}.pan                    FROM monthly_{reporttype}.excluded_pans_{reporttype}))         ), cte3 AS (          SELECT {month}_{reporttype}_repeated_pans_credit.pan,             {month}_{reporttype}_repeated_pans_credit.customername            FROM monthly_{reporttype}.{month}_{reporttype}_repeated_pans_credit         )  SELECT cte2.pan,     cte2.customername,     cte2.amount,     cte2.total_{reporttype}s,     cte2.total_accounts,     cte2.pcodes,     cte2.countries    FROM cte2   WHERE NOT (cte2.pan IN ( SELECT cte3.pan FROM cte3)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_excluded_pans_without_repeated_pans_credit\")\n",
    "\n",
    "eft_without_excluded_repeat_pans_credit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_without_excluded_repeat_pans_credit TABLESPACE pg_default AS  WITH cte1 AS (          SELECT {month}_{reporttype}_credit.pan,             {month}_{reporttype}_credit.customername,             {month}_{reporttype}_credit.amount,             {month}_{reporttype}_credit.total_{reporttype}s,             {month}_{reporttype}_credit.total_accounts,             {month}_{reporttype}_credit.pcodes,             {month}_{reporttype}_credit.countries            FROM monthly_{reporttype}.{month}_{reporttype}_credit          LIMIT 1000         ), cte2 AS (          SELECT excluded_pans_{reporttype}.pan,             excluded_pans_{reporttype}.holdername            FROM monthly_{reporttype}.excluded_pans_{reporttype}         ), cte3 AS (          SELECT {month}_{reporttype}_repeated_pans_credit.pan,             {month}_{reporttype}_repeated_pans_credit.customername            FROM monthly_{reporttype}.{month}_{reporttype}_repeated_pans_credit         ), cte4 AS (          SELECT a.pan            FROM cte2 a         UNION          SELECT b.pan            FROM cte3 b         )  SELECT cte1.pan,     cte1.customername,     cte1.amount,     cte1.total_{reporttype}s,     cte1.total_accounts,     cte1.pcodes,     cte1.countries    FROM cte1   WHERE NOT (cte1.pan IN ( SELECT cte4.pan FROM cte4)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_without_excluded_repeat_pans_credit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Debit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "eft_debit_main = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_debit TABLESPACE pg_default AS  WITH cte1 AS ( SELECT {month}_trftrn_{reporttype}.pan, {month}_trftrn_{reporttype}.customername, count(1) AS cnt  FROM monthly_{reporttype}.{month}_trftrn_{reporttype} GROUP BY {month}_trftrn_{reporttype}.pan, {month}_trftrn_{reporttype}.customername ),  cte2 AS ( SELECT cte1.pan, cte1.customername, row_number() OVER (PARTITION BY cte1.pan  ORDER BY cte1.cnt DESC) AS rr FROM cte1 ), cte3 AS ( SELECT {month}_trftrn_{reporttype}.pan, {month}_trftrn_{reporttype}.batchid, {month}_trftrn_{reporttype}.rptsernum, {month}_trftrn_{reporttype}.transactionrefnum             FROM monthly_{reporttype}.{month}_trftrn_{reporttype}  WHERE {month}_trftrn_{reporttype}.transactioncountrycode = 'IN'::text  AND {month}_trftrn_{reporttype}.transactiontype = 'P'::text   AND {month}_trftrn_{reporttype}.country_in = 1  AND {month}_trftrn_{reporttype}.tytype_in = 1 ),  cte4 AS ( SELECT string_agg(DISTINCT f.country_name, ','::text) AS countries, string_agg(DISTINCT e.purposecode, ','::text) AS pcodes, c3.pan  FROM cte3 c3 \t\t  JOIN ( SELECT {month}_trftrn_{reporttype}.batchid, {month}_trftrn_{reporttype}.rptsernum, {month}_trftrn_{reporttype}.transactiondate, {month}_trftrn_{reporttype}.transactionrefnum, {month}_trftrn_{reporttype}.transactiontype,  \t\t\t\t{month}_trftrn_{reporttype}.instrumenttype, {month}_trftrn_{reporttype}.transactioninstname, {month}_trftrn_{reporttype}.transactioninstrefnum, {month}_trftrn_{reporttype}.transactionstatecode, \t\t\t\t{month}_trftrn_{reporttype}.transactioncountrycode, {month}_trftrn_{reporttype}.amountrupees, {month}_trftrn_{reporttype}.amountforeigncurrency, {month}_trftrn_{reporttype}.currencyoftransaction, \t\t\t\t{month}_trftrn_{reporttype}.purposeoftransaction, {month}_trftrn_{reporttype}.purposecode, {month}_trftrn_{reporttype}.riskrating, {month}_trftrn_{reporttype}.customername, {month}_trftrn_{reporttype}.customerid,   \t\t\t\t{month}_trftrn_{reporttype}.occupation, {month}_trftrn_{reporttype}.pan, {month}_trftrn_{reporttype}.accountnum, {month}_trftrn_{reporttype}.country_in,        \t\t\t\t{month}_trftrn_{reporttype}.tytype_in \t\t\t\tFROM monthly_{reporttype}.{month}_trftrn_{reporttype} \t\t\t\tWHERE {month}_trftrn_{reporttype}.transactioncountrycode <> 'IN'::text) e \t\t  ON c3.batchid = e.batchid AND c3.rptsernum = e.rptsernum AND c3.transactionrefnum = e.transactionrefnum              \t\t  JOIN ( SELECT country_master.code, country_master.country_name \t\t\t\tFROM monthly_report.country_master) f ON f.code = e.transactioncountrycode \t\t  GROUP BY c3.pan ),  cte5 AS ( SELECT trn.pan, sum(trn.amountrupees) AS amount, count(DISTINCT trn.accountnum) AS total_accounts, count(DISTINCT trn.batchid::text || trn.rptsernum::text) AS total_{reporttype}s  FROM monthly_{reporttype}.{month}_trftrn_{reporttype} trn  WHERE trn.transactioncountrycode = 'IN'::text \t\t\t\t   AND trn.country_in = 1 \t\t\t\t   AND trn.tytype_in = 1 \t\t\t\t   AND trn.transactiontype = 'P'::text \t\t\t\t   AND trn.pan IS NOT NULL \t\t\t\t   AND TRIM(BOTH FROM trn.pan) <> ''::text           \t\t\t\t   GROUP BY trn.pan          \t\t\t\t   ORDER BY (sum(trn.amountrupees)) DESC ),  cte6 AS ( SELECT c4.pan, c2.customername, c4.pcodes, c4.countries \t\t  FROM cte4 c4 \t\t  JOIN ( SELECT cte2.pan, cte2.customername, cte2.rr FROM cte2 WHERE cte2.rr = 1) c2 ON c4.pan = c2.pan ) \t\t  SELECT c5.pan, c6.customername, c5.amount, c5.total_{reporttype}s, c5.total_accounts, c6.pcodes, c6.countries   \t\t  FROM cte5 c5 JOIN cte6 c6 ON c5.pan = c6.pan   ORDER BY c5.amount DESC \t\t  WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_debit\")\n",
    "\n",
    "eft_debit_repeated_pans = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_repeated_pans_debit TABLESPACE pg_default AS  WITH cte1 AS (          SELECT DISTINCT pan_month_of_report_{month}.pan,             pan_month_of_report_{month}.report_type,             count(DISTINCT pan_month_of_report_{month}.month) AS month            FROM monthly_report.pan_month_of_report_{month}           WHERE pan_month_of_report_{month}.report_type = 'debit_{reporttype}'::text           GROUP BY pan_month_of_report_{month}.pan, pan_month_of_report_{month}.report_type          HAVING count(DISTINCT pan_month_of_report_{month}.month) = {pan_month_of_report}           ORDER BY (count(DISTINCT pan_month_of_report_{month}.month)) DESC         ), cte2 AS (          SELECT {month}_{reporttype}_debit.pan,             {month}_{reporttype}_debit.customername,             {month}_{reporttype}_debit.amount,             {month}_{reporttype}_debit.total_{reporttype}s,             {month}_{reporttype}_debit.total_accounts,             {month}_{reporttype}_debit.pcodes,             {month}_{reporttype}_debit.countries            FROM monthly_{reporttype}.{month}_{reporttype}_debit                  )  SELECT cte2.pan,     cte2.customername,     cte2.amount,     cte2.total_{reporttype}s,     cte2.total_accounts,     cte2.pcodes,     cte2.countries    FROM cte2   WHERE (cte2.pan IN ( SELECT DISTINCT cte1.pan            FROM cte1)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_repeated_pans_debit\")\n",
    "\n",
    "eft_without_repeated_pans_debit = db_conn.execute(f\" CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_excluded_pans_without_repeated_pans_debit TABLESPACE pg_default AS  WITH cte1 AS (          SELECT {month}_{reporttype}_debit.pan,             {month}_{reporttype}_debit.customername,             {month}_{reporttype}_debit.amount,             {month}_{reporttype}_debit.total_{reporttype}s,             {month}_{reporttype}_debit.total_accounts,             {month}_{reporttype}_debit.pcodes,             {month}_{reporttype}_debit.countries            FROM monthly_{reporttype}.{month}_{reporttype}_debit          LIMIT 1000         ), cte2 AS (          SELECT cte1.pan,             cte1.customername,             cte1.amount,             cte1.total_{reporttype}s,             cte1.total_accounts,             cte1.pcodes,             cte1.countries            FROM cte1           WHERE (cte1.pan IN ( SELECT excluded_pans_{reporttype}.pan                    FROM monthly_{reporttype}.excluded_pans_{reporttype}))         ), cte3 AS (          SELECT {month}_{reporttype}_repeated_pans_debit.pan,             {month}_{reporttype}_repeated_pans_debit.customername            FROM monthly_{reporttype}.{month}_{reporttype}_repeated_pans_debit         )  SELECT cte2.pan,     cte2.customername,     cte2.amount,     cte2.total_{reporttype}s,     cte2.total_accounts,     cte2.pcodes,     cte2.countries    FROM cte2   WHERE NOT (cte2.pan IN ( SELECT cte3.pan FROM cte3)) WITH DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_excluded_pans_without_repeated_pans_debit\")\n",
    "\n",
    "eft_without_excluded_repeat_pans_debit = db_conn.execute(f\"CREATE MATERIALIZED VIEW IF NOT EXISTS monthly_{reporttype}.{month}_{reporttype}_without_excluded_repeat_pans_debit TABLESPACE pg_default AS  WITH cte1 AS (          SELECT {month}_{reporttype}_debit.pan,             {month}_{reporttype}_debit.customername,             {month}_{reporttype}_debit.amount,             {month}_{reporttype}_debit.total_{reporttype}s,             {month}_{reporttype}_debit.total_accounts,             {month}_{reporttype}_debit.pcodes,             {month}_{reporttype}_debit.countries            FROM monthly_{reporttype}.{month}_{reporttype}_debit          LIMIT 1000         ), cte2 AS (          SELECT excluded_pans_{reporttype}.pan,             excluded_pans_{reporttype}.holdername            FROM monthly_{reporttype}.excluded_pans_{reporttype}         ), cte3 AS (          SELECT {month}_{reporttype}_repeated_pans_debit.pan,             {month}_{reporttype}_repeated_pans_debit.customername            FROM monthly_{reporttype}.{month}_{reporttype}_repeated_pans_debit         ), cte4 AS (          SELECT a.pan            FROM cte2 a         UNION          SELECT b.pan            FROM cte3 b         )  SELECT cte1.pan,     cte1.customername,     cte1.amount,     cte1.total_{reporttype}s,     cte1.total_accounts,     cte1.pcodes,     cte1.countries    FROM cte1   WHERE NOT (cte1.pan IN ( SELECT cte4.pan FROM cte4))  WITH  DATA\")\n",
    "print(f\"Materialized view created : {month}_{reporttype}_without_excluded_repeat_pans_debit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total Materialized Views created:\\n\")\n",
    "\n",
    "print(f\"{month}_{reporttype}_arfinp_lpe_{reporttype} created,\\n{month}_{reporttype}_resolved_pans created,\\n{month}_{reporttype}_holder_totals_pan created\")\n",
    "print(\"\\nMaterialized Views for Credit:\")\n",
    "print(f\"{month}_{reporttype}_pans_credit,\\\n",
    "      \\n{month}_{reporttype}_repeated_pans_credit,\\\n",
    "      \\n{month}_{reporttype}_credit_main_without_repeated_excluded_pan,\\\n",
    "      \\n{month}_{reporttype}_credit_excluded_pans_without_repeated_pan,\\\n",
    "      \\n{month}_{reporttype}_accounts_credit\")\n",
    "\n",
    "print(\"\\nMaterialized Views for Debit:\")\n",
    "\n",
    "print(f\"{month}_{reporttype}_pans_debit,\\\n",
    "        \\n{month}_{reporttype}_repeated_pans_debit,\\\n",
    "        \\n{month}_{reporttype}_debit_main_without_repeated_excluded_pan\\\n",
    "        \\n{month}_{reporttype}_debit_excluded_pans_without_repeated_pan\\\n",
    "        \\n{month}_{reporttype}_accounts_debit \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ---------------------------------------------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
